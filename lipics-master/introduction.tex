Deep neural networks have demonstrated remarkable capabilities, achieving \newline human-like or even superior performance across a wide range of tasks. However, their robustness is often compromised by their susceptibility to input perturbations \cite{szegedy}. This vulnerability has catalyzed the verification community to develop various methodologies, each presenting a unique balance between completeness and computational efficiency \cite{katz2019marabou,Reluplex,deeppoly}. This surge in innovation has also led to the inception of competitions such as VNNComp \cite{VNNcomp}, which aim to systematically evaluate the performance of neural network verification tools. While the verification engines are generic, the benchmarks usually focus on local robustness, i.e. given a DNN, an image and a small neighbourhood around this image, is it the case that all the images in the neighbourhood are classified in the same way. While some quite large DNNs (e.g. ResNet with tens of thousands of neurons) can be verified very efficiently (tens of seconds per input) \cite{crown}, with all inputs either certified robust or an attack on robustness is found; some smaller DNNs (with hundreds of neurons, only using the simpler ReLU activation function) cannot be analysed fully, with $12-20\%$ of {\em undecided} inputs where neither of the decisions can be reached (\cite{crown} and Table \ref{tab:example}). Using complete verifiers, while theoretically would close the gap, fail to answer fully due to (very long) time-outs. Actually, the DNNs easy to verify are trained specifically (using DiffAI \cite{DiffAI} or PGD \cite{PGD}), while the DNNs harder to verify are {\em verification-agnostic} \cite{SDPFI}, trained in a {\em natural} way.

In this work, we concentrate on {\em verification-agnostic} DNNs, given the prevalence of techniques for {\em easier} DNNs, and also the decrease in accuracy of specifically trained DNNs (e.g. the large easy-to-verify ResNet network has $29\%$ accuracy \cite{prima}!). Our analysis focuses on DNNs employing the standard ReLU activation function, though our findings potentially extend to more sophisticated architectures. Our investigation delves into the core abstraction mechanisms integral to several prominent algorithms, such as Eran-DeepPoly \cite{deeppoly}, Linear Programming \cite{MILP}, PRIMA \cite{prima}, 
MN-BaB \cite{ferrari2022complete} and various implementations of ($\alpha$)($\beta$)-CROWN \cite{crown,xu2020fast}\footnote{\cite{VNNcomp} reports that $\alpha,\beta$-Crown \cite{crown,xu2020fast} often surpasses in accuracy other competing techniques, even complete ones due to (even long) time-outs;  It maintains completeness for smaller DNNs \cite{xu2020fast}, and showcases impressive efficiency for larger networks, benefiting from branch-and-bound based methodology \cite{cutting,BaB}.}. 
	The high-level approach followed by all these techniques is to compute lower or/and upper bounds for the values of neurons (abstraction on values) for inputs in the considered input region, and then finally conclude based on the bounds of neurons in the output layer. These tools are thus sound but not necessarily complete, i.e., when these tools certify a DNN to be robust for a particular image $I$, then the corresponding DNN is indeed robust but it may happen that the tool is unable to certify a DNN to be robust even though it actually is, because of bounds inaccuracies. 
	%To dig further into their incompleteness, we first remark that 
	Notice that complete methods exist \cite{Reluplex,katz2019marabou,SDPFI}, but they time-out on hard instances. In practice, the most accurate today's results are obtained using ($\alpha$)($\beta$)-CROWN \cite{crown}.



%As a starting point, we sought to understand properties of DNNs that make them hard to verify.
Our key finding is a new notion of {\em compensations}, that explains why bounds are inaccurate. Formally, a {\em compensating pair of paths} $(\pi,\pi')$ between neurons $a$ and $b)$ is such that $w < 0 < w'$ for $w,w'$ the products of weights seen along $\pi$ and $\pi'$ respectively. Ignoring the (ReLU) activation functions, the weight of $b$ is loaded with 
$(w+w') weight(a)$ by $\pi$ and $\pi'$. As $w,w'$ have opposite signs, they will compensate (partly) each other. The compensation is only partial due to the ReLU activation seen along the way of $\pi$ which can "clip" a part of $w \cdot weight(a)$, and similarly for $\pi'$. However, it is very hard to evaluate by how much without explicitly considering both phases of the ReLUs, which the efficient tools try to avoid because it is very time-consuming (combinatioral explosion as the problem is NP-hard \cite{Reluplex}).
%; for instance, what is a differentiating  feature between DNNs trained in natural way (i.e., without explicit concern for robustness) versus the DNNs trained to be robust. 
Our three main contributions address the challenges of inaccuracies when verifying DNNs:
\begin{enumerate}
	\item  Our first main contribution establishes, through Theorem \ref{th1}, the foundational understanding that the compensating pairs are the 	%primary 
	source of {\em all} inaccuracies, 
	 as in their absence, even the simplest Box abstraction (and most abstractions under consideration) is fully accurate. This is confirmed experimentally by the fact that 
	 DNNs easier to verified have less compensation strenght than verification agnostic DNNs. 
	This revelation, while theoretically pivotal, has limited practical applicability as most networks inherently possess compensating pairs. Nonetheless, this insight lays the groundwork for our innovative approach, {\em relaxing} the usual but inefficient Mixed Integer Linear Program (MILP) formulation encoding exactly DNNs \cite{MILP}. To compute bounds for $z$, we encode as binary variables only ReLU nodes on {\em compensating paths with target $z$}; other nodes, in particular those on compensating paths to neurons before $z$ are encoded using the much faster linear relaxation. To take into account indirectly the compensating paths to previous nodes, we inductively compute bounds for every neuron layer by layer, and use these bounds of previous neurons in the MILP encoding for $z$. This is more efficient, as the number of binary nodes is strongly reduced. However, the question remains how accurate is it?
	
\item  Our second main contribution, delineated in Theorem \ref{th2}, answers this question, showing that this abstraction is fully accurate, in that the bounds computed in such a way for neuron $z$ correspond exactly to the minimal and maximal values reachable by inputs of the DNN. Again, the prevalence of compensating paths in actual DNNs renders this algorithm computationally intensive because the number of binary variables is still high.


\item Our third main contribution is to experimentally confirm that focusing on compensations {\em does} improve accuracy: we introduce {\CMP} (Algorithm \ref{algo1}), building on our theoretical insights. This method prioritizes ReLU nodes based on the weights of their associated compensating paths and differentiates their treatment in the MILP model, employing binary constraints for heavier nodes and linear constraints for lighter ones. Our empirical evaluation (refer to Table \ref{tab:example}) reveals {\CMP} achieves a beneficial balance between accuracy and completeness compared to prevailing methods. It reduces the proportion of undecided inputs from 17.6\% (as seen with $\beta$-Crown \cite{crown}) to 10.4\% on the DNN architecture of $8 \times 200$. Moreover, it verifies over 20\% more images than $\alpha,\beta$-CROWN \cite{xu2020fast} (or PRIMA \cite{prima}, MN-BaB \cite{ferrari2022complete}) on the larger $6 \times 500$ DNN.


%We verify experimentally that the algorithm offers interesting trade-offs, by testing on local robustness for DNNs trained "naturally" (and thus difficult to verify).

%KSM: I think this is a distraction in intro, so I suggest moving to later part
% Overall, the worst case complexity of algorithm \ref{algo1} is lower than $O(N 2^K LP(N))$, where $N$ is the number of nodes of the DNN, $K$ the number of ReLU nodes selected as binary variable, and $LP(N)$ is the (polynomial time) complexity of solving a linear program representing a DNN with $N$ nodes. This complexity is an upper bound, as e.g. Gurobi is fairly efficient and never need to consider all of the $2^K$ ReLU configurations to compute the bounds. Keeping $K$ reasonably low thus provides an efficient algorithm. 
%By design, it will never run into a complexity wall (unlike the full MILP encoding), although it can take a while on large networks because of the linear factor $N$ in the number of nodes.


\end{enumerate}

These theoretical and experimental results both point to compensation being a promising notion, opening up many opportunities, as discussed in Section \ref{Discussion}. 





%   
% 
%
%In this context, application of DNNs in safety critical applications is cautiously envisioned. For that to happen at a large scale, hard guarantees should be provided \cite{certification}, through e.g. incremental verification \cite{incremental}, so that to avoid dramatic consequences. It is the reason for the development of (hard) verification tools since 2016, with now many tools with different trade-offs from exact computation but slow (e.g. Marabou \cite{katz2019marabou}/Reluplex\cite{Reluplex}), up to very efficient but also incomplete (e.g. ERAN-DeepPoly \cite{deeppoly}). To benchmark these tools, a competition has been run since 2019, namely VNNcomp \cite{VNNcomp}. The current overall better performing verifier is $\alpha$-$\beta$-Crown \cite{crown}, a fairly sophisticatedly engineered tool based mainly on "branch and bound" (BaB) \cite{BaB}, and which can scale all the way from complete on smaller DNNs \cite{xu2020fast} up to very efficient on larger DNNs, constantly upgraded, e.g. \cite{cutting}. 
%
%While the verification engines are generic, the benchmarks usually focus on local robustness, i.e. given a DNN, an image and a small neighbourhood around this image, 
%is it the case that all the images in the neighbourhood are classified in the same way.
%While some quite large DNNs (e.g. ResNet with tens of thousands of neurons) can be verified very efficiently (tens of seconds per input) \cite{crown}, with all inputs either certified robust or an attack on robustness is found; some smaller DNNs (with hundreds of neurons, only using the simpler ReLU activation function) cannot be analysed fully, with $12-20\%$ of inputs where neither of the decisions can be reached (\cite{crown} and Table \ref{tab:example}). Actually, DNNs which are trained to be robust (using DiffAI \cite{DiffAI} or PGD \cite{PGD}) are easier to verify, while the DNNs trained in a "natural" way are harder to verify.
%
%
%In this paper, we focus on DNNs trained in a "natural" way,
%%uncovering what makes the DNNs trained in a natural way so hard to verify (
%because for "easier" DNNs, adequate methods already exist. 
%To do so, we analyse the abstraction mechanisms at the heart of several efficient algorithms, namely Eran-DeepPoly \cite{deeppoly}, the Linear Programming approximation \cite{MILP}, PRIMA \cite{prima}, and different versions of ($\alpha$)($\beta$)-CROWN \cite{crown}. All these algorithms compute lower or/and upper bounds for the values of neurons (abstraction on values) for inputs in the considered input region, and conclude based on such bounds. For instance, if for all image $I'$ in the neighbourhood of image $I$, we have $weight_{I'}(n'-n) < 0$ for $n$ the output neuron corresponding to the expected class, then we know that the DNN is robust in the neighbourhood of image $I$. We restrict the formal study to DNNs using only the standard ReLU activation function, although nothing specific prevents the results to be extended to more general architectures. We uncover that {\em compensations} 
%(see next paragraph) is the phenomenon creating inaccuracies. We verified experimentally that a DNN trained in a natural way has heavier compensating pairs than DNNs trained in a robust way.
%
%Formally, a compensating pair is a pair of paths $(\pi,\pi')$ between a pair of neurons $(a,b)$, such that we have $w < 0 < w'$, for $w,w'$ the products of weight seen along $\pi$ and $\pi'$. Ignoring the (ReLU) activation functions, the weight of $b$ is loaded with $w \cdot weight(a)$ by $\pi$, while it is loaded with $w' \cdot weight(a)$ by $\pi'$. That is, it is loaded by $(w+w') weight(a)$. As $w,w'$ have opposite sign, they will compensate (partly) each other. The compensation is only partial due to the ReLU activation seen along the way of $\pi$ which can "clip" a part of $w \cdot weight(a)$, and similarly for $\pi'$. However, it is very hard to evaluate by how much without explicitly considering both phases of the ReLUs, which all the efficient tools try to avoid because it is very expansive (could be exponential in the number of such ReLU nodes opened).

%Our first main contribution is to formally show, in Theorem \ref{th1}, that compensation is the sole reason for the inaccuracies as (most) efficient algorithms will compute exact bounds for all neurons if there is no compensating pair of paths at all.
%While this theorem is theoretically interesting, it is not usable in practice as (almost) all networks have some compensating pairs. However, this notion of compensating pairs opens a first interesting idea concerning an exact abstraction of the network using a Mixed Integer Linear Program \cite{MILP}, where the weight of each neuron is a linear variable, and ReLU node may be associated with binary variables (exact encoding) or linear variables (overapproximation). While LP tools can scale to thousands of linear variables, MILP encoding can only be solved for a limited number of binary variables. This suggests that a simpler encoding could be used for those ReLUs that are not on compensating pairs, as their precise outcome may not be necessary.

%Our second main contribution is to show formally in Theorem \ref{th2}, that 
%encoding all ReLU nodes on a pair of compensating paths with a binary variable,
%and using linear relaxation for the other ReLU nodes, will lead to exact bounds for (most) of the algorithms considered. This theorem allows to restrict the number of integer variables, and thus to obtain encodings that are faster to solve. Practically, however, (almost) all ReLU nodes are on some compensating path, and using this exact restricted MILP encoding will be too time consuming.

%Our third main contribution is more practical, proposing Algorithm \ref{algo1} based on this knowledge that compensating pair of paths are the reason for inaccuracy. The idea is thus to use this information to rank the ReLU nodes in terms of importance, and only keep the most important ones as binary variables, and use linear relaxation for the least important ones.
%%More precisely, the algorithm will, as DeepPoly, consider layers one by one and neurons $b$ %on this layer one by one, selecting the heaviest pairs of compensating paths ending in $b$
%%and associating these nodes with a binary variable. Then an MILP tool such as Gurobi is used %to compute the lower and upper bound for node $b$. 
%Overall, the worst case complexity of algorithm \ref{algo1} is lower than $O(N 2^K LP(N))$, where $N$ is the number of nodes of the DNN, $K$ the number of ReLU nodes selected as binary variable, and $LP(N)$ is the (polynomial time) complexity of solving a linear program representing a DNN with $N$ nodes. This complexity is an upper bound, as e.g. Gurobi is fairly efficient and never need to consider all of the $2^K$ ReLU configurations to compute the bounds. Keeping $K$ reasonably low thus provides an efficient algorithm. 
%By design, it will never run into a complexity wall (unlike the full MILP encoding), although it can take a while on large networks because of the linear factor $N$ in the number of nodes. An additional interesting point is that it is extremely easy to parallelize, as all the nodes in the same layer can be run in parallel. We verify experimentally that the algorithm offers interesting trade-offs, by testing on local robustness for DNNs trained "naturally" (and thus difficult to verify).


%KSM: I suggest we move this to experimental evaluation
%This paper does not focus on producing the most efficient tool, and we did not spend engineering efforts to optimize it. The focus is instead on the novel notion of compensation, the associated methodology and its evaluation. For instance, our implementation is fully in Python, with uncompetitive runtime for our DeepPoly implementation ($\approx 100$ slower than in CROWN). Still, evaluation of the methodology versus even the most efficient tools reveals a lot of potential for the notion of compensation, opening up several opportunities for applying it in different contexts of DNN verification (see Section \ref{Discussion}). 

