\subsection{Related Work} 


We now compare {\CMP} with major verification tools for DNNs to clarify our methodology and its distinction from the existing state-of-the-art. When contrasted with the exact encoding of a DNN using MILP \cite{MILP}, we selectively target a limited number of ReLU nodes based on compensating strength. This approach scales to larger DNNs while preserving good accuracy. In comparison to MIPplanet \cite{MIPplanet}, which opts for a different selection of ReLU nodes, our strategy iteratively computes bounds for all neurons. This significantly reduces the number of ReLU nodes represented exactly for a given neuron through numerous rapid computations, proving more efficient than a single, larger computation. Furthermore, in contrast to the linear relaxation of MILP encoding \cite{MILP}, {\CMP} is inherently more precise, albeit with a slower runtime. 

When examining ERAN-DeepPoly \cite{deeppoly}, which efficiently computes bounds on values, we establish that LP encoding is unequivocally more accurate. This is a novel insight, to our knowledge. DeepPoly abstracts the weight of every node using two functions: an upper function and a lower function. While the upper function is fixed, the lower function offers two choices. In Proposition \ref{LP}, we demonstrate that the LP relaxation precisely matches the intersection of these two choices. Consequently, it is more accurate than DeepPoly but less efficient. \CMP is considerably more precise than DeepPoly (it refines LP), albeit less efficiently.

Regarding PRIMA \cite{prima}, the approach involves explicitly maintaining dependencies between neurons, calculating bounds layer by layer, similar to our method. This enables the retention of dependencies from many layers prior. Our method, however, handles dependencies between neurons differently, attributing them to compensation. It excels in accuracy for local dependencies but may lose precision for dependencies formed several layers back. Many dependencies are local, occurring in the last few layers (since ReLU nodes tend to clip those from far back), and \CMP achieves more accurate results than PRIMA.


$\alpha,\beta$ CROWN \cite{crown} (and other Branch and Bound algorithms, such as BaBSR \cite{BaB}) focus on running few instances of branch and bound (one per output neuron).  In the worst case, this involves considering all possible ReLU configurations, though branch and bound typically circumvents most possibilities. In simple networks, like those trained robustly, branch and bound is highly efficient, focusing on branches crucial for verifying the actual property. In contrast, {\CMP} is less efficient as it must individually consider each node from the start, including nodes with non-critical bounds. However, branch and bound hits a complexity barrier when verifying complex networks, such as naturally trained DNNs, due to an overwhelming number of branches. For instance, Branch and Bound verifies only 3.5\% more images (44.5\% from 41\%) by increasing runtime by 50 times on larger $6 \times 5000$ DNNs, and fails to match {\CMP}'s performance (65\% of images verified, Table \ref{tab:example}). 

{\color{red} MN-BaB \cite{ferrari2022complete} is another state-of-the-art verifier which can be regarded as a development of PRIMA. As shown in its name, it uses a combination technique of multi-neuron constraints and Branch and Bound. According to their experiments results, MN-BaB has similar speed and accuracy as $\alpha$,$\beta$ CROWN. Therefore, we do not compare our experiments results to MN-BaB separately.}

For hard-to-verify DNNs that are not too large,  $\alpha,\beta$ CROWN (and PRIMA) resorts to a {\em refined} path \cite{MILP2}, where the bounds {\em on the first few layers} are refined using an exact MILP encoding. In {\CMP}, we do not use an exact encoding but a partial one with the most important ReLU nodes obtained by considering the compensation strength. As it is more efficient, we compute bounds for neurons in all the layers. This would be infeasible without the selection based on the compensation. The refined version of $\alpha,\beta$-CROWN is particularly accurate on small DNNs. As the depth grows, the more work is left to BaB and \CMP is more accurate (Table \ref{tab:example}).

Finally, algorithms such as Reluplex / Marabou \cite{Reluplex,katz2019marabou}  abstract the network, and therefore,  diverge significantly from those abstracting values (PRIMA, ($\alpha$)($\beta$)-CROWN)\cite{prima,crown}, {\CMP}, and the like. These network-abstraction algorithms are designed to be {\em complete} but completeness comes at the price of significant scalability challenges, and therefore, are often unable to handle the size of networks that can be handled by those abstract values. 
