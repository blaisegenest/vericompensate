\subsection{Related Work} 

We compare {\CMP} with major verification tools for DNNs to clarify our methodology and its distinction from the existing state-of-the-art. CMP scales while preserving good accuracy, through targeting a limited number of binary variables based on compensating strength, stricking a good balance between exact encoding of a DNN using MILP~\cite{MILP} (too slow) and LP relaxation (too inaccurate). MIPplanet~\cite{MIPplanet} opts for a different selection of binary variables, and execute one large MILP instance instead of CMP's many small instances, which significantly reduce the number of binary variables necessary for each instance. In \cite{9211410}, small instances are also considered, however with a straightforward choice of binary nodes based on the weight of outgoing edge, therefore restricted to nodes in the previous layer, which is significantly less accurate than using compensating strength (see Table \ref{tab:example1}).

ERAN-DeepPoly \cite{deeppoly} computes bounds on values very quickly, by abstracting the weight of every node using two functions: an upper function and a lower function. While the upper function is fixed, the lower function offers two choices.
It relates to the LP encoding through the following new (to our knowledge) insight:  Proposition \ref{LP} state that the LP relaxation precisely matches the intersection of these two choices. Consequently, LP is more accurate (but slower) than DeepPoly, and \CMP is considerably more precise.

Regarding PRIMA \cite{prima}, the approach involves explicitly maintaining dependencies between neurons, calculating bounds layer by layer. This enables the retention of dependencies from many layers prior. CMP, however, handles dependencies between neurons differently, attributing them to compensation. CMP excels in accuracy for local dependencies but may lose precision for dependencies formed several layers back. 
Experimental accuracy results in favor of CMP points to dependencies being more local than global (see Table \ref{tab:example}).


$\alpha,\beta$ CROWN \cite{crown} (and other Branch and Bound algorithms, such as BaBSR \cite{BaB} and MN-BaB \cite{ferrari2022complete})  focus on running few instances of branch and bound (one per output neuron). In the worst case, this involves considering all possible ReLU configurations, though branch and bound typically circumvents most possibilities. In simple networks, like those trained robustly, branch and bound is highly efficient, focusing on branches crucial for verifying the actual property. In contrast, {\CMP} is less efficient as it must individually consider each node from the start, including nodes with non-critical bounds. However, branch and bound hits a complexity barrier when verifying complex networks, such as verification-agnostic DNNs, due to an overwhelming number of branches. For instance, Branch and Bound verifies only 3.5\% more images (44.5\% from 41\%) by increasing runtime by 50 times on larger $6 \times 500$ DNNs, and fails to match {\CMP}'s performance (65\% of images verified, Table \ref{tab:example}). 

%{\color{red} MN-BaB \cite{ferrari2022complete} is another state-of-the-art verifier which can be regarded as a development of PRIMA. As indicated by its name, it uses a combination technique of multi-neuron constraints and Branch and Bound. According to their experiments results, MN-BaB has similar speed and accuracy as $\alpha$,$\beta$ CROWN. Therefore, we do not compare our experiments results to MN-BaB separately.}

For verification-agnostic DNNs that are not too large,  $\beta$-CROWN (and PRIMA) resorts to a {\em refined} path \cite{MILP2}, where the bounds {\em on the first few layers} are refined using an exact MILP encoding. In {\CMP}, we do not use an exact encoding but a partial one with the most important ReLU nodes obtained by considering the compensation strength. As it is more efficient, we compute bounds for neurons in {\em all} the layers. This would be infeasible without the selection based on the compensation. The refined version of $\beta$-CROWN is particularly accurate on small DNNs. As the depth grows, the more work is left to BaB and \CMP is more accurate (Table \ref{tab:example}).

Finally, methods such as Reluplex / Marabou \cite{Reluplex,katz2019marabou}  abstract the network: they diverge significantly from those abstracting values such as PRIMA, ($\alpha$)($\beta$)-CROWN)\cite{prima,crown}, \CMP. These network-abstraction algorithms are designed to be {\em complete} but completeness comes at the price of significant scalability challenges, and in practice they time-out on hard instances.