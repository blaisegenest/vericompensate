% Encoding: UTF-8

@inProceedings{atva25,
  author    = {Liao, Yuke and Genest, Blaise and Meel, Kuldeep and Aryaman, Shaan},
  booktitle = {ATVA'25},
  title     = {{Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification}},
  year      = {2025},
  editor    = {Meenakshi D'Souza and B Srivathsan},
  pages     = {to appear},
  publisher = {Springer},
  volume    = {LNCS},
}

@inProceedings{aiware,
  author    = {De Conto, Eduardo and Genest, Blaise and Easwaran, Arvind},
  booktitle = {AIware'24},
  title     = {{Function+Data Flow: A Framework to Specify Machine Learning Pipelines for Digital Twinning.}},
  year      = {2024},
  editor    = {Adams,Bram and Zimmermann,  Thomas and Ozkaya, Ipek  and Lin, Dayi and Zhang, Jie M},
  pages     = {19-27},
  publisher = {ACM},
}



@inProceedings{prima,
  author     = {Muller, Mark Niklas and Makarchuk, Gleb and Singh, Gagandeep and P\"{u}schel, Markus and Vechev, Martin},
  journal    = {Proc. ACM Program. Lang. POPL},
  title      = {{PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations}},
  year       = {2022},
  month      = {jan},
  volume     = {6},
  abstract   = {Formal verification of neural networks is critical for their safe adoption in real-world applications. However, designing a precise and scalable verifier which can handle different activation functions, realistic network architectures and relevant specifications remains an open and difficult challenge. In this paper, we take a major step forward in addressing this challenge and present a new verification framework, called PRIMA. PRIMA is both (i) general: it handles any non-linear activation function, and (ii) precise: it computes precise convex abstractions involving multiple neurons via novel convex hull approximation algorithms that leverage concepts from computational geometry. The algorithms have polynomial complexity, yield fewer constraints, and minimize precision loss. We evaluate the effectiveness of PRIMA on a variety of challenging tasks from prior work. Our results show that PRIMA is significantly more precise than the state-of-the-art, verifying robustness to input perturbations for up to 20\%, 30\%, and 34\% more images than existing work on ReLU-, Sigmoid-, and Tanh-based networks, respectively. Further, PRIMA enables, for the first time, the precise verification of a realistic neural network for autonomous driving within a few minutes.},
  address    = {New York, NY, USA},
  articleno  = {43},
  doi        = {10.1145/3498704},
  issue_date = {January 2022},
  keywords   = {Polyhedra, Robustness, Abstract Interpretation, Convexity},
  numpages   = {33},
  publisher  = {Association for Computing Machinery},
}

@InProceedings{crown,
  author    = {Wang, Shiqi and Zhang, Huan and Xu, Kaidi and Lin, Xue and Jana, Suman and Hsieh, Cho-Jui and Kolter, J. Zico},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Beta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Neural Network Robustness Verification},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {29909--29921},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
}

@Article{deeppoly,
  author     = {Singh, Gagandeep and Gehr, Timon and P\"{u}schel, Markus and Vechev, Martin},
  journal    = {Proc. ACM Program. Lang.},
  title      = {An Abstract Domain for Certifying Neural Networks},
  year       = {2019},
  month      = {jan},
  number     = {POPL},
  volume     = {3},
  abstract   = {We present a novel method for scalable and precise certification of deep neural networks. The key technical insight behind our approach is a new abstract domain which combines floating point polyhedra with intervals and is equipped with abstract transformers specifically tailored to the setting of neural networks. Concretely, we introduce new transformers for affine transforms, the rectified linear unit (ReLU), sigmoid, tanh, and maxpool functions. We implemented our method in a system called DeepPoly and evaluated it extensively on a range of datasets, neural architectures (including defended networks), and specifications. Our experimental results indicate that DeepPoly is more precise than prior work while scaling to large networks. We also show how to combine DeepPoly with a form of abstraction refinement based on trace partitioning. This enables us to prove, for the first time, the robustness of the network when the input image is subjected to complex perturbations such as rotations that employ linear interpolation.},
  address    = {New York, NY, USA},
  articleno  = {41},
  doi        = {10.1145/3290354},
  issue_date = {January 2019},
  keywords   = {Adversarial attacks, Deep Learning, Abstract Interpretation},
  numpages   = {30},
  publisher  = {Association for Computing Machinery},
}

@InProceedings{Reluplex,
  author    = {Katz, Guy and Barrett, Clark and Dill, David L. and Julian, Kyle and Kochenderfer, Mykel J.},
  booktitle = {Computer Aided Verification},
  title     = {Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks},
  year      = {2017},
  address   = {Cham},
  editor    = {Majumdar, Rupak and Kun{\v{c}}ak, Viktor},
  pages     = {97--117},
  abstract  = {Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.},
  isbn      = {978-3-319-63387-9},
}

@InProceedings{MILP2,
  author    = {Gagandeep Singh and Timon Gehr and Markus PÃ¼schel and Martin Vechev},
  booktitle = {International Conference on Learning Representations (ICLR'19)},
  title     = {Robustness Certification with Refinement},
  year      = {2019},
}

@InProceedings{wong2018provable,
  author       = {Wong, Eric and Kolter, Zico},
  booktitle    = {International conference on machine learning},
  title        = {Provable defenses against adversarial examples via the convex outer adversarial polytope},
  year         = {2018},
  organization = {PMLR},
  pages        = {5286--5295},
}

@InProceedings{wang2018formal,
  author    = {Wang, Shiqi and Pei, Kexin and Whitehouse, Justin and Yang, Junfeng and Jana, Suman},
  booktitle = {27th USENIX Security Symposium (USENIX Security 18)},
  title     = {Formal security analysis of neural networks using symbolic intervals},
  year      = {2018},
  pages     = {1599--1614},
}

@InProceedings{anderson2019optimization,
  author    = {Anderson, Greg and Pailoor, Shankara and Dillig, Isil and Chaudhuri, Swarat},
  booktitle = {Proceedings of the 40th ACM SIGPLAN conference on programming language design and implementation},
  title     = {Optimization and abstraction: a synergistic approach for analyzing neural network robustness},
  year      = {2019},
  pages     = {731--744},
}

@Article{Bak2021,
  author  = {Bak, Stanley and Liu, Changliu and Johnson, Taylor},
  journal = {arXiv preprint arXiv:2109.00498},
  title   = {The second international verification of neural networks competition (vnn-comp 2021): Summary and results},
  year    = {2021},
}

@InProceedings{botoeva2020efficient,
  author    = {Botoeva, Elena and Kouvaros, Panagiotis and Kronqvist, Jan and Lomuscio, Alessio and Misener, Ruth},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title     = {Efficient verification of relu-based neural networks via dependency analysis},
  year      = {2020},
  number    = {04},
  pages     = {3291--3299},
  volume    = {34},
}

@InProceedings{bunel2020lagrangian,
  author       = {Bunel, Rudy and De Palma, Alessandro and Desmaison, Alban and Dvijotham, Krishnamurthy and Kohli, Pushmeet and Torr, Philip and Kumar, M Pawan},
  booktitle    = {Conference on Uncertainty in Artificial Intelligence},
  title        = {Lagrangian decomposition for neural network verification},
  year         = {2020},
  organization = {PMLR},
  pages        = {370--379},
}

@inproceedings{elboher,
  author       = {Yizhak Yisrael Elboher and
                  Justin Gottschlich and
                  Guy Katz},
  editor       = {Shuvendu K. Lahiri and
                  Chao Wang},
  title        = {An Abstraction-Based Framework for Neural Network Verification},
  booktitle    = {Computer Aided Verification - 32nd International Conference, {CAV}
                  2020, Los Angeles, CA, USA, July 21-24, 2020, Proceedings, Part {I}},
  series       = {Lecture Notes in Computer Science},
  volume       = {LNCS 12224},
  pages        = {43--65},
  publisher    = {Springer},
  year         = {2020},
}

@inproceedings{SRGR,
  author       = {Pengfei Yang and
                  Renjue Li and
                  Jianlin Li and
                  Cheng{-}Chao Huang and
                  Jingyi Wang and
                  Jun Sun and
                  Bai Xue and
                  Lijun Zhang},
  editor       = {Jan Friso Groote and
                  Kim Guldstrand Larsen},
  title        = {Improving Neural Network Verification through Spurious Region Guided
                  Refinement},
  booktitle    = {Tools and Algorithms for the Construction and Analysis of Systems
                  - 27th International Conference, {TACAS} 2021, Held as Part of the
                  European Joint Conferences on Theory and Practice of Software, {ETAPS}
                  2021, Luxembourg City, Luxembourg, March 27 - April 1, 2021, Proceedings,
                  Part {I}},
  series       = {LNCS},
  volume       = {12651},
  pages        = {389--408},
  publisher    = {Springer},
  year         = {2021}
}

@Article{BaB,
  author  = {Bunel, Rudy and Lu, Jingyue and Turkaslan, Ilker and Torr, Philip HS and Kohli, Pushmeet and Kumar, M Pawan},
  journal = {Journal of Machine Learning Research},
  title   = {Branch and bound for piecewise linear neural network verification},
  year    = {2020},
  number  = {42},
  pages   = {1--39},
  volume  = {21},
}

@Article{bunel2018unified,
  author  = {Bunel, Rudy R and Turkaslan, Ilker and Torr, Philip and Kohli, Pushmeet and Mudigonda, Pawan K},
  journal = {Advances in Neural Information Processing Systems},
  title   = {A unified view of piecewise linear neural network verification},
  year    = {2018},
  volume  = {31},
}

@Article{SDPFI,
  author  = {Dathathri, Sumanth and Dvijotham, Krishnamurthy and Kurakin, Alexey and Raghunathan, Aditi and Uesato, Jonathan and Bunel, Rudy R and Shankar, Shreya and Steinhardt, Jacob and Goodfellow, Ian and Liang, Percy S and others},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming},
  year    = {2020},
  pages   = {5318--5331},
  volume  = {33},
}

@InProceedings{alessandro,
  author       = {De Palma, Alessandro and Behl, Harkirat S and Bunel, Rudy and Torr, Philip and Kumar, M Pawan},
  booktitle    = {International Conference on Learning Representations (ICLR'21)},
  title        = {Scaling the convex barrier with active sets},
  year         = {2021},
  organization = {Open Review},
}

@Article{DePalma2021,
  author  = {De Palma, Alessandro and Bunel, Rudy and Desmaison, Alban and Dvijotham, Krishnamurthy and Kohli, Pushmeet and Torr, Philip HS and Kumar, M Pawan},
  journal = {arXiv preprint arXiv:2104.06718},
  title   = {Improved branch and bound for neural network verification via lagrangian decomposition},
  year    = {2021},
}

@InProceedings{dutta2018output,
  author       = {Dutta, Souradeep and Jha, Susmit and Sankaranarayanan, Sriram and Tiwari, Ashish},
  booktitle    = {NASA Formal Methods Symposium},
  title        = {Output range analysis for deep feedforward neural networks},
  year         = {2018},
  organization = {Springer},
  pages        = {121--138},
}

@InProceedings{dvijotham2018dual,
  author    = {Dvijotham, Krishnamurthy and Stanforth, Robert and Gowal, Sven and Mann, Timothy A and Kohli, Pushmeet},
  booktitle = {UAI},
  title     = {A Dual Approach to Scalable Verification of Deep Networks.},
  year      = {2018},
  number    = {2},
  pages     = {3},
  volume    = {1},
}

@InProceedings{dvijotham2020efficient,
  author       = {Dvijotham, Krishnamurthy Dj and Stanforth, Robert and Gowal, Sven and Qin, Chongli and De, Soham and Kohli, Pushmeet},
  booktitle    = {Uncertainty in artificial intelligence},
  title        = {Efficient neural network verification with exactness characterization},
  year         = {2020},
  organization = {PMLR},
  pages        = {497--507},
}

@InProceedings{MIPplanet,
  author       = {Ehlers, Ruediger},
  booktitle    = {Automated Technology for Verification and Analysis: 15th International Symposium, ATVA 2017, Pune, India, October 3--6, 2017, Proceedings 15},
  title        = {Formal verification of piece-wise linear feed-forward neural networks},
  year         = {2017},
  organization = {Springer},
  pages        = {269--286},
}

@Article{fazlyab2020safety,
  author    = {Fazlyab, Mahyar and Morari, Manfred and Pappas, George J},
  journal   = {IEEE Transactions on Automatic Control},
  title     = {Safety verification and robustness analysis of neural networks via quadratic constraints and semidefinite programming},
  year      = {2020},
  number    = {1},
  pages     = {1--15},
  volume    = {67},
  publisher = {IEEE},
}

@InProceedings{gehr2018ai2,
  author       = {Gehr, Timon and Mirman, Matthew and Drachsler-Cohen, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
  booktitle    = {2018 IEEE symposium on security and privacy (SP)},
  title        = {Ai2: Safety and robustness certification of neural networks with abstract interpretation},
  year         = {2018},
  organization = {IEEE},
  pages        = {3--18},
}

@Article{gowal2018effectiveness,
  author  = {Gowal, Sven and Dvijotham, Krishnamurthy and Stanforth, Robert and Bunel, Rudy and Qin, Chongli and Uesato, Jonathan and Arandjelovic, Relja and Mann, Timothy and Kohli, Pushmeet},
  journal = {Proceedingsof the IEEE International Conference on Computer Vision (ICCV)},
  title   = {On the effectiveness of interval bound propagation for training verifiably robust models},
  year    = {2019},
}

@InProceedings{huang2017safety,
  author       = {Huang, Xiaowei and Kwiatkowska, Marta and Wang, Sen and Wu, Min},
  booktitle    = {Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30},
  title        = {Safety verification of deep neural networks},
  year         = {2017},
  organization = {Springer},
  pages        = {3--29},
}

@Article{kingma2014adam,
  author  = {Kingma, Diederik P and Ba, Jimmy},
  journal = {International Conference on Learning Representations (ICLR'14)},
  title   = {Adam: A method for stochastic optimization},
  year    = {2014},
}

@Article{Vnncomp2020,
  author = {C. Liu and T. Johnson},
  title  = {Vnn comp 2020},
  url    = {https://sites.google.com/view/vnn20/vnncomp},
}

@Article{lu2019neural,
  author  = {Lu, Jingyue and Kumar, M Pawan},
  journal = {International Conference on Learning Representations (ICLR'20)},
  title   = {Neural network branching for neural network verification},
  year    = {2020},
}

@Article{PGD,
  author  = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal = {International Conference on Learning Representations (ICLR'18)},
  title   = {Towards deep learning models resistant to adversarial attacks},
  year    = {2018},
}

@InProceedings{DiffAI,
  author       = {Mirman, Matthew and Gehr, Timon and Vechev, Martin},
  booktitle    = {International Conference on Machine Learning},
  title        = {Differentiable abstract interpretation for provably robust neural networks},
  year         = {2018},
  organization = {PMLR},
  pages        = {3578--3586},
}

@Article{Precisemultineuronabstractionsforneuralnetworkcertification2021,
  author  = {M. N. MÃ¼ller, G. Makarchuk, G. Singh, M. PÃ¼schel, and M. Vechev},
  journal = {arXiv preprint arXiv:2103.03638},
  title   = {Precise multi-neuron abstractions for neural network certification},
  year    = {2021},
}

@Article{pattanaik2017robust,
  author  = {Pattanaik, Anay and Tang, Zhenyi and Liu, Shuijing and Bommannan, Gautham and Chowdhary, Girish},
  journal = {arXiv preprint arXiv:1712.03632},
  title   = {Robust deep reinforcement learning with adversarial attacks},
  year    = {2017},
}

@Article{rubies2019fast,
  author  = {Rubies-Royo, Vicenc and Calandra, Roberto and Stipanovic, Dusan M and Tomlin, Claire},
  journal = {arXiv preprint arXiv:1902.07247},
  title   = {Fast neural network verification via shadow prices},
  year    = {2019},
}

@Article{shi2020robustness,
  author  = {Shi, Zhouxing and Zhang, Huan and Chang, Kai-Wei and Huang, Minlie and Hsieh, Cho-Jui},
  journal = {International Conference on Learning Representations (ICLR'20)},
  title   = {Robustness verification for transformers},
  year    = {2020},
}

@Article{shi2021fast,
  author  = {Shi, Zhouxing and Wang, Yihan and Zhang, Huan and Yi, Jinfeng and Hsieh, Cho-Jui},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Fast certified robust training with short warmup},
  year    = {2021},
  pages   = {18335--18349},
  volume  = {34},
}

@Article{singh2018fast,
  author  = {Singh, Gagandeep and Gehr, Timon and Mirman, Matthew and P{\"u}schel, Markus and Vechev, Martin},
  journal = {Advances in neural information processing systems},
  title   = {Fast and effective robustness certification},
  year    = {2018},
  volume  = {31},
}

@InProceedings{singh2018boosting,
  author    = {Singh, Gagandeep and Gehr, Timon and P{\"u}schel, Markus and Vechev, Martin},
  booktitle = {International Conference on Learning Representations (ICLR'18)},
  title     = {Boosting robustness certification of neural networks},
  year      = {2018},
}

@Article{kpoly,
  author  = {Singh, Gagandeep and Ganvir, Rupanshu and P{\"u}schel, Markus and Vechev, Martin},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Beyond the single neuron convex barrier for neural network certification},
  year    = {2019},
  volume  = {32},
}

@Article{MILP,
  author  = {Tjeng, Vincent and Xiao, Kai and Tedrake, Russ},
  journal = {International Conference on Learning Representations (ICLR'19)},
  title   = {Evaluating robustness of neural networks with mixed integer programming},
  year    = {2019},
}

@Article{wang2018efficient,
  author  = {Wang, Shiqi and Pei, Kexin and Whitehouse, Justin and Yang, Junfeng and Jana, Suman},
  journal = {Advances in neural information processing systems},
  title   = {Efficient formal safety analysis of neural networks},
  year    = {2018},
  volume  = {31},
}

@Article{wong2018scaling,
  author  = {Wong, Eric and Schmidt, Frank and Metzen, Jan Hendrik and Kolter, J Zico},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Scaling provable adversarial defenses},
  year    = {2018},
  volume  = {31},
}

@Article{xu2020automatic,
  author  = {Xu, Kaidi and Shi, Zhouxing and Zhang, Huan and Wang, Yihan and Chang, Kai-Wei and Huang, Minlie and Kailkhura, Bhavya and Lin, Xue and Hsieh, Cho-Jui},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Automatic perturbation analysis for scalable certified robustness and beyond},
  year    = {2020},
  pages   = {1129--1141},
  volume  = {33},
}

@inproceedings{xu2020fast,
    author       = {Kaidi Xu and
                  Huan Zhang and
                  Shiqi Wang and
                  Yihan Wang and
                  Suman Jana and
                  Xue Lin and
                  Cho{-}Jui Hsieh},
  title        = {Fast and Complete: Enabling Complete Neural Network Verification with
                  Rapid and Massively Parallel Incomplete Verifiers},
  booktitle    = {International Conference on Learning Representations (ICLR'21)},
  publisher    = {OpenReview.net},
  year         = {2021},
}

@Article{zhang2018efficient,
  author  = {Zhang, Huan and Weng, Tsui-Wei and Chen, Pin-Yu and Hsieh, Cho-Jui and Daniel, Luca},
  journal = {Advances in neural information processing systems},
  title   = {Efficient neural network robustness certification with general activation functions},
  year    = {2018},
  volume  = {31},
}

@Article{zhang2019towards,
  author  = {Zhang, Huan and Chen, Hongge and Xiao, Chaowei and Gowal, Sven and Stanforth, Robert and Li, Bo and Boning, Duane and Hsieh, Cho-Jui},
  journal = {arXiv preprint arXiv:1906.06316},
  title   = {Towards stable and efficient training of verifiably robust neural networks},
  year    = {2019},
}

@Article{zhang2020robust,
  author  = {Zhang, Huan and Chen, Hongge and Xiao, Chaowei and Li, Bo and Liu, Mingyan and Boning, Duane and Hsieh, Cho-Jui},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Robust deep reinforcement learning against adversarial perturbations on state observations},
  year    = {2020},
  pages   = {21024--21037},
  volume  = {33},
}

@Article{zhang2021robust,
  author  = {Zhang, Huan and Chen, Hongge and Boning, Duane and Hsieh, Cho-Jui},
  journal = {International Conference on Learning Representations (ICLR'21)},
  title   = {Robust reinforcement learning on state observations with learned optimal adversary},
  year    = {2021},
}

@Article{anderson2020strong,
  author    = {Anderson, Ross and Huchette, Joey and Ma, Will and Tjandraatmadja, Christian and Vielma, Juan Pablo},
  journal   = {Mathematical Programming},
  title     = {Strong mixed-integer programming formulations for trained neural networks},
  year      = {2020},
  number    = {1-2},
  pages     = {3--39},
  volume    = {183},
  publisher = {Springer},
}

@Article{avis1991basis,
  author    = {Avis, David and Fukuda, Komei},
  journal   = {Applied Mathematics Letters},
  title     = {A basis enumeration algorithm for linear systems with geometric applications},
  year      = {1991},
  number    = {5},
  pages     = {39--42},
  volume    = {4},
  publisher = {Elsevier},
}

@InProceedings{avis1991pivoting,
  author    = {Avis, David and Fukuda, Komei},
  booktitle = {Proceedings of the seventh annual symposium on Computational geometry},
  title     = {A pivoting algorithm for convex hulls and vertex enumeration of arrangements and polyhedra},
  year      = {1991},
  pages     = {98--104},
}

@Article{barber1996quickhull,
  author    = {Barber, C Bradford and Dobkin, David P and Huhdanpaa, Hannu},
  journal   = {ACM Transactions on Mathematical Software (TOMS)},
  title     = {The quickhull algorithm for convex hulls},
  year      = {1996},
  number    = {4},
  pages     = {469--483},
  volume    = {22},
  publisher = {Acm New York, NY, USA},
}

@Article{bentley1982approximation,
  author    = {Bentley, Jon Louis and Preparata, Franco P and Faust, Mark G},
  journal   = {Communications of the ACM},
  title     = {Approximation algorithms for convex hulls},
  year      = {1982},
  number    = {1},
  pages     = {64--68},
  volume    = {25},
  publisher = {ACM New York, NY, USA},
}

@InProceedings{boopathy2019cnn,
  author    = {Boopathy, Akhilan and Weng, Tsui-Wei and Chen, Pin-Yu and Liu, Sijia and Daniel, Luca},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title     = {Cnn-cert: An efficient framework for certifying robustness of convolutional neural networks},
  year      = {2019},
  number    = {01},
  pages     = {3240--3247},
  volume    = {33},
}

@Article{bunel2020efficient,
  author  = {Bunel, Rudy R and Hinder, Oliver and Bhojanapalli, Srinadh and Dvijotham, Krishnamurthy},
  journal = {Advances in Neural Information Processing Systems},
  title   = {An efficient nonconvex reformulation of stagewise convex optimization problems},
  year    = {2020},
  pages   = {8247--8258},
  volume  = {33},
}

@Article{chazelle1993optimal,
  author    = {Chazelle, Bernard},
  journal   = {Discrete \& Computational Geometry},
  title     = {An optimal convex hull algorithm in any fixed dimension},
  year      = {1993},
  pages     = {377--409},
  volume    = {10},
  publisher = {Springer},
}

@InProceedings{clariso2004octahedron,
  author       = {Claris{\'o}, Robert and Cortadella, Jordi},
  booktitle    = {Static Analysis: 11th International Symposium, SAS 2004, Verona, Italy, August 26-28, 2004. Proceedings 11},
  title        = {The octahedron abstract domain},
  year         = {2004},
  organization = {Springer},
  pages        = {312--327},
}

@InProceedings{cohen2019certified,
  author       = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
  booktitle    = {international conference on machine learning},
  title        = {Certified adversarial robustness via randomized smoothing},
  year         = {2019},
  organization = {PMLR},
  pages        = {1310--1320},
}

@Book{dantzig1963linear,
  author    = {Dantzig, George},
  publisher = {Princeton university press},
  title     = {Linear programming and extensions},
  year      = {1963},
}

@Book{edelsbrunner1987algorithms,
  author    = {Edelsbrunner, Herbert},
  publisher = {Springer Science \& Business Media},
  title     = {Algorithms in combinatorial geometry},
  year      = {1987},
  volume    = {10},
}

@InProceedings{fukuda1995double,
  author       = {Fukuda, Komei and Prodon, Alain},
  booktitle    = {Franco-Japanese and Franco-Chinese conference on combinatorics and computer science},
  title        = {Double description method revisited},
  year         = {1995},
  organization = {Springer},
  pages        = {91--111},
}

@PhdThesis{genov2015convex,
  author = {Genov, Blagoy},
  school = {Universit{\"a}t Bremen},
  title  = {The convex hull problem in practice: improving the running time of the double description method},
  year   = {2015},
}

@InProceedings{gowal2019scalable,
  author    = {Gowal, Sven and Dvijotham, Krishnamurthy Dj and Stanforth, Robert and Bunel, Rudy and Qin, Chongli and Uesato, Jonathan and Arandjelovic, Relja and Mann, Timothy and Kohli, Pushmeet},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  title     = {Scalable verified training for provably robust image classification},
  year      = {2019},
  pages     = {4842--4851},
}

@InCollection{joswig2003beneath,
  author    = {Joswig, Michael},
  booktitle = {Algebra, Geometry and Software Systems},
  publisher = {Springer},
  title     = {Beneath-and-beyond revisited},
  year      = {2003},
  pages     = {1--21},
}

@InProceedings{katz2019marabou,
  author       = {Katz, Guy and Huang, Derek A and Ibeling, Duligur and Julian, Kyle and Lazarus, Christopher and Lim, Rachel and Shah, Parth and Thakoor, Shantanu and Wu, Haoze and Zelji{\'c}, Aleksandar and others},
  booktitle    = {Proc. of CAV},
  title        = {The marabou framework for verification and analysis of deep neural networks},
  year         = {2019},
  organization = {Springer},
  pages        = {443--452},
}

@InProceedings{khosravani2013simple,
  author       = {Khosravani, Hamid R and Ruano, Ant{\'o}nio E and Ferreira, Pedro M},
  booktitle    = {2013 IEEE 8th international symposium on intelligent signal processing},
  title        = {A simple algorithm for convex hull determination in high dimensions},
  year         = {2013},
  organization = {IEEE},
  pages        = {109--114},
}

@InProceedings{lecuyer2019certified,
  author       = {Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman},
  booktitle    = {2019 IEEE symposium on security and privacy (SP)},
  title        = {Certified robustness to adversarial examples with differential privacy},
  year         = {2019},
  organization = {IEEE},
  pages        = {656--672},
}

@InProceedings{lyu2020fastened,
  author    = {Lyu, Zhaoyang and Ko, Ching-Yun and Kong, Zhifeng and Wong, Ngai and Lin, Dahua and Daniel, Luca},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title     = {Fastened crown: Tightened neural network robustness certificates},
  year      = {2020},
  number    = {04},
  pages     = {5037--5044},
  volume    = {34},
}

@InProceedings{marechal2017efficient,
  author    = {Mar{\'e}chal, Alexandre and P{\'e}rin, Micha{\"e}l},
  booktitle = {Verification, Model Checking, and Abstract Interpretation (VMCAI)},
  title     = {Efficient elimination of redundancies in polyhedra using raytracing},
  year      = {2017},
  volume    = {10145},
}

@Article{morrison2016branch,
  author    = {Morrison, David R and Jacobson, Sheldon H and Sauppe, Jason J and Sewell, Edward C},
  journal   = {Discrete Optimization},
  title     = {Branch-and-bound algorithms: A survey of recent advances in searching, branching, and pruning},
  year      = {2016},
  pages     = {79--102},
  volume    = {19},
  publisher = {Elsevier},
}

@Article{motzkin1953double,
  author  = {Motzkin, Theodore S and Raiffa, Howard and Thompson, Gerald L and Thrall, Robert M},
  journal = {Contributions to the Theory of Games},
  title   = {The double description method},
  year    = {1953},
  number  = {28},
  pages   = {51--73},
  volume  = {2},
}

@Article{muller2021scaling,
  author  = {M{\"u}ller, Christoph and Serre, Fran{\c{c}}ois and Singh, Gagandeep and P{\"u}schel, Markus and Vechev, Martin},
  journal = {Proceedings of Machine Learning and Systems},
  title   = {Scaling polyhedral neural network verification on GPUs},
  year    = {2021},
  pages   = {733--746},
  volume  = {3},
}

@Article{muller2020neural,
  author  = {M{\"u}ller, Christoph and Singh, Gagandeep and P{\"u}schel, Markus and Vechev, Martin T},
  journal = {CoRR, abs/2007.10868},
  title   = {Neural network robustness verification on gpus},
  year    = {2020},
}

@Article{raghunathan2018semidefinite,
  author  = {Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy S},
  journal = {Advances in neural information processing systems},
  title   = {Semidefinite relaxations for certifying robustness to adversarial examples},
  year    = {2018},
  volume  = {31},
}

@InProceedings{ruoss2021efficient,
  author    = {Ruoss, Anian and Baader, Maximilian and Balunovi{\'c}, Mislav and Vechev, Martin},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title     = {Efficient certification of spatial robustness},
  year      = {2021},
  number    = {3},
  pages     = {2504--2513},
  volume    = {35},
}

@Article{ruoss2020learning,
  author  = {Ruoss, Anian and Balunovic, Mislav and Fischer, Marc and Vechev, Martin},
  journal = {Advances in neural information processing systems},
  title   = {Learning certified individually fair representations},
  year    = {2020},
  pages   = {7584--7596},
  volume  = {33},
}

@Article{ryou2020fast,
  author  = {Ryou, Wonryong and Chen, Jiayu and Balunovic, Mislav and Singh, Gagandeep and Dan, Andrei Marian and Vechev, Martin T},
  journal = {CoRR abs/2005.13300},
  title   = {Fast and effective robustness certification for recurrent neural networks},
  year    = {2020},
}

@Article{salman2019provably,
  author  = {Salman, Hadi and Li, Jerry and Razenshteyn, Ilya and Zhang, Pengchuan and Zhang, Huan and Bubeck, Sebastien and Yang, Greg},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Provably robust deep learning via adversarially trained smoothed classifiers},
  year    = {2019},
  volume  = {32},
}

@Article{salman2019convex,
  author  = {Salman, Hadi and Yang, Greg and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Pengchuan},
  journal = {Advances in Neural Information Processing Systems},
  title   = {A convex relaxation barrier to tight robustness verification of neural networks},
  year    = {2019},
  volume  = {32},
}

@Article{sartipizadeh2016computing,
  author  = {Sartipizadeh, Hossein and Vincent, Tyrone L},
  journal = {arXiv preprint arXiv:1603.04422},
  title   = {Computing the approximate convex hull in high dimensions},
  year    = {2016},
}

@Article{seidel1995upper,
  author    = {Seidel, Raimund},
  journal   = {Computational Geometry},
  title     = {The upper bound theorem for polytopes: an easy proof of its asymptotic version},
  year      = {1995},
  number    = {2},
  pages     = {115--116},
  volume    = {5},
  publisher = {Elsevier},
}

@Article{szegedy,
  author  = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal = {International Conference on Learning Representations (ICLR'14)},
  title   = {Intriguing properties of neural networks},
  year    = {2014},
}

@Article{optC2V,
  author  = {Tjandraatmadja, Christian and Anderson, Ross and Huchette, Joey and Ma, Will and Patel, Krunal Kishor and Vielma, Juan Pablo},
  journal = {Advances in Neural Information Processing Systems},
  title   = {The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification},
  year    = {2020},
  pages   = {21675--21686},
  volume  = {33},
}

@Article{urban2021review,
  author  = {Urban, Caterina and Min{\'e}, Antoine},
  journal = {arXiv preprint arXiv:2104.02466},
  title   = {A review of formal methods applied to machine learning},
  year    = {2021},
}

@Article{xiang2018output,
  author    = {Xiang, Weiming and Tran, Hoang-Dung and Johnson, Taylor T},
  journal   = {IEEE transactions on neural networks and learning systems},
  title     = {Output reachable set estimation and verification for multilayer neural networks},
  year      = {2018},
  number    = {11},
  pages     = {5777--5783},
  volume    = {29},
  publisher = {IEEE},
}

@InProceedings{Zhong2014,
  author       = {Zhong, Jinhong and Tang, Ke and Qin, A Kai},
  booktitle    = {2014 International Joint Conference on Neural Networks (IJCNN)},
  title        = {Finding convex hull vertices in metric space},
  year         = {2014},
  organization = {IEEE},
  pages        = {1587--1592},
}

@Article{xu1998approximate,
  author    = {Xu, Zong-Ben and Zhang, Jiang-She and Leung, Yiu-Wing},
  journal   = {Applied mathematics and computation},
  title     = {An approximate algorithm for computing multidimensional convex hulls},
  year      = {1998},
  number    = {2-3},
  pages     = {193--226},
  volume    = {94},
  publisher = {Elsevier},
}

@InProceedings{Oikarinen2021,
  author    = {Oikarinen, Tuomas and Zhang, Wang and Megretski, Alexandre and Daniel, Luca and Weng, Tsui-Wei},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Robust Deep Reinforcement Learning through Adversarial Loss},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {26156--26167},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
}

@InProceedings{weng2018towards,
  author       = {Weng, Lily and Zhang, Huan and Chen, Hongge and Song, Zhao and Hsieh, Cho-Jui and Daniel, Luca and Boning, Duane and Dhillon, Inderjit},
  booktitle    = {International Conference on Machine Learning},
  title        = {Towards fast computation of certified robustness for relu networks},
  year         = {2018},
  organization = {PMLR},
  pages        = {5276--5285},
}

@InProceedings{atva,
  author       = {Mohammad Afzal and Ashutosh Gupta and S. Akshay},
  booktitle    = {Automated Technology for Verification and Analysis (ATVA'23)},
  title        = {{Using Counterexamples to Improve Robustness Verification in Neural Networks}},
  year         = {2023},
  pages        = {422-443},
  series       = {LNCS 14215}
}

 
@InProceedings{cutting,
  author    = {Huan Zhang and Shiqi Wang and Kaidi Xu and Linyi Li and Bo Li and Suman Jana and Cho-Jui Hsieh and J. Zico Kolter},
  booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  title     = {General Cutting Planes for Bound-Propagation-Based Neural Network Verification.},
  year      = {2022},
}

@inproceedings{attack,
  author       = {Huan Zhang and
                  Shiqi Wang and
                  Kaidi Xu and
                  Yihan Wang and
                  Suman Jana and
                  Cho{-}Jui Hsieh and
                  J. Zico Kolter},
  editor       = {Kamalika Chaudhuri and
                  Stefanie Jegelka and
                  Le Song and
                  Csaba Szepesv{\'{a}}ri and
                  Gang Niu and
                  Sivan Sabato},
  title        = {A Branch and Bound Framework for Stronger Adversarial Attacks of ReLU
                  Networks},
  booktitle    = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
                  2022, Baltimore, Maryland, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {26591--26604},
  publisher    = {{PMLR}},
  year         = {2022}
}


@InProceedings{certification,
  author    = {Gagandeep Singh},
  booktitle = {Static Analysis - 30th International Symposium, {SAS} 2023, Cascais, Portugal, October 22-24, 2023, Proceedings},
  title     = {Building Trust and Safety in Artificial Intelligence with Abstract Interpretation},
  year      = {2023},
  editor    = {Manuel V. Hermenegildo and Jos{\'{e}} F. Morales},
  pages     = {28--38},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {14284},
  doi       = {10.1007/978-3-031-44245-2\_3},
}

@Article{incremental,
  author  = {Shubham Ugare and Debangshu Banerjee and Sasa Misailovic and Gagandeep Singh},
  journal = {Proc. {ACM} Program. Lang.},
  title   = {Incremental Verification of Neural Networks},
  year    = {2023},
  number  = {{PLDI}},
  pages   = {1920--1945},
  volume  = {7},
  doi     = {10.1145/3591299},
}

@misc{VNNcomp,
      title={First Three Years of the International Verification of Neural Networks Competition (VNN-COMP)}, 
      author={Christopher Brix and Mark Niklas MÃ¼ller and Stanley Bak and Taylor T. Johnson and Changliu Liu},
      year={2023},
      eprint={2301.05815},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{ferrari2022complete,
  author    = {Claudio Ferrari and Mark Niklas Mueller and Nikola Jovanovi{\'c} and Martin Vechev},
  booktitle = {International Conference on Learning Representations (ICLR'22)},
  title     = {Complete Verification via Multi-Neuron Relaxation Guided Branch-and-Bound},
  year      = {2022},
}

@Article{9211410,
  author   = {Huang, Chao and Fan, Jiameng and Chen, Xin and Li, Wenchao and Zhu, Qi},
  journal  = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  title    = {Divide and Slide: Layer-Wise Refinement for Output Range Analysis of Deep Neural Networks},
  year     = {2020},
  number   = {11},
  pages    = {3323-3335},
  volume   = {39},
  doi      = {10.1109/TCAD.2020.3013071},
  keywords = {Biological neural networks;Programming;Neurons;Microsoft Windows;Convolution;Estimation;Linear programming;Linear programming (LP);mixed-integer linear programming (MILP);neural networks;output range analysis;refinement},
}

@InProceedings{nnenum,
  author       = {Bak, Stanley},
  booktitle    = {NASA Formal Methods Symposium},
  title        = {nnenum: Verification of ReLU Neural Networks with Optimized Abstraction Refinement},
  year         = {2021},
  organization = {Springer},
  pages        = {19--36}
}

@InProceedings{Marabou,
  author    = {Katz, Guy and Huang, Derek A. and Ibeling, Duligur and Julian, Kyle and Lazarus, Christopher and Lim, Rachel and Shah, Parth and Thakoor, Shantanu and Wu, Haoze and Zelji{\'{c}}, Aleksandar and Dill, David L. and Kochenderfer, Mykel J. and Barrett, Clark},
  booktitle = {Computer Aided Verification},
  title     = {The Marabou Framework for Verification and Analysis of Deep Neural Networks},
  year      = {2019},
  address   = {Cham},
  editor    = {Dillig, Isil and Tasiran, Serdar},
  pages     = {443--452},
  publisher = {Springer International Publishing},
  abstract  = {Deep neural networks are revolutionizing the way complex systems are designed. Consequently, there is a pressing need for tools and techniques for network analysis and certification. To help in addressing that need, we present Marabou, a framework for verifying deep neural networks. Marabou is an SMT-based tool that can answer queries about a network's properties by transforming these queries into constraint satisfaction problems. It can accommodate networks with different activation functions and topologies, and it performs high-level reasoning on the network that can curtail the search space and improve performance. It also supports parallel execution to further enhance scalability. Marabou accepts multiple input formats, including protocol buffer files generated by the popular TensorFlow framework for neural networks. We describe the system architecture and main components, evaluate the technique and discuss ongoing work.},
  isbn      = {978-3-030-25540-4},
}

@InProceedings{Marabou2,
  author    = {Haoze Wu and Omri Isac and Aleksandar Zelji{\'c} and Teruhiro Tagomori and Matthew Daggitt and Wen Kokke and Idan Refaeli and Guy Amir and Kyle Julian and Shahaf Bassan and Pei Huang and Ori Lahav and Min Wu and Min Zhang and Ekaterina Komendantskaya and Guy Katz and Clark Barrett},
  booktitle = {Proceedings of the ${\it 36^{th}}$ International Conferenceon Computer Aided Verification (CAV '24)},
  title     = {Marabou 2.0: A Versatile Formal Analyzer of Neural Networks},
  year      = {2024},
  editor    = {Arie Gurfinkel and Vijay Ganesh},
  month     = jul,
  note      = {Montreal, Canada},
  pages     = {249--264},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {14681},
}

@Misc{VNNcomp23,
  author        = {Christopher Brix and Stanley Bak and Changliu Liu and Taylor T. Johnson},
  title         = {The Fourth International Verification of Neural Networks Competition (VNN-COMP 2023): Summary and Results},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2312.16760},
  primaryclass  = {cs.LG},
  url           = {https://sites.google.com/view/vnn2023},
}

@Misc{VNNcomp22,
  author        = {Mark Niklas MÃ¼ller and Christopher Brix and Stanley Bak and Changliu Liu and Taylor T. Johnson},
  title         = {The Third International Verification of Neural Networks Competition (VNN-COMP 2022): Summary and Results},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2212.10376},
  primaryclass  = {cs.LG},
  url           = {https://sites.google.com/view/vnn2022},
}

@Misc{VNNcomp24,
  author = {Christopher Brix and Stanley Bak and Changliu Liu and Taylor T. Johnson and David Shriver and Haoze (Andrew) Wu},
  title  = {5th International Verification of Neural Networks Competition (VNN-COMP'24)},
  year   = {2024},
  url    = {https://sites.google.com/view/vnn2024},
}

@InProceedings{TrainingforVerification,
  author    = {Xu, Dong and Mozumder, Nusrat Jahan and Duong, Hai and Dwyer, Matthew B.},
  booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
  title     = {Training for Verification: Increasing Neuron Stability to Scale DNN Verification},
  year      = {2024},
  address   = {Cham},
  editor    = {Finkbeiner, Bernd and Kov{\'a}cs, Laura},
  pages     = {24--44},
  publisher = {Springer Nature Switzerland},
  abstract  = {With the growing use of deep neural networks(DNN) in mission and safety-critical applications, there is an increasing interest in DNN verification. Unfortunately, increasingly complex network structures, non-linear behavior, and high-dimensional input spaces combine to make DNN verification computationally challenging. Despite tremendous advances, DNN verifiers are still challenged to scale to large verification problems. In this work, we explore how the number of stable neurons under the precondition of a specification gives rise to verification complexity. We examine prior work on the problem, adapt it, and develop several novel approaches to increase stability. We demonstrate that neuron stability can be increased substantially without compromising model accuracy and this yields a multi-fold improvement in DNN verifier performance.},
  isbn      = {978-3-031-57256-2},
}

@InProceedings{lipshitz,
  author       = {Wang, Zhilu and Huang, Chao and Zhu, Qi},
  booktitle    = {2022 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  title        = {Efficient global robustness certification of neural networks via interleaving twin-network encoding},
  year         = {2022},
  organization = {IEEE},
  pages        = {1087--1092},
}

@Article{DivideAndSlide,
  author   = {Huang, Chao and Fan, Jiameng and Chen, Xin and Li, Wenchao and Zhu, Qi},
  journal  = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  title    = {Divide and Slide: Layer-Wise Refinement for Output Range Analysis of Deep Neural Networks},
  year     = {2020},
  number   = {11},
  pages    = {3323-3335},
  volume   = {39},
  doi      = {10.1109/TCAD.2020.3013071},
  keywords = {Biological neural networks;Programming;Neurons;Microsoft Windows;Convolution;Estimation;Linear programming;Linear programming (LP);mixed-integer linear programming (MILP);neural networks;output range analysis;refinement},
}

@InProceedings{AdversarialTrainingAndProvableDefenses,
  author    = {Mislav Balunovic and Martin Vechev},
  booktitle = {International Conference on Learning Representations (ICLR'20)},
  title     = {Adversarial Training and Provable Defenses: Bridging the Gap},
  year      = {2020},
}


@InProceedings{pyrat,
  author        = {Serge Durand and Augustin Lemesle and Zakaria Chihani and Caterina Urban and Francois Terrier},
  title         = {ReCIPH: Relational Coefficients for Input Partitioning Heuristic},
  year          = {2022},
  booktitle = {1st Workshop on Formal Verification of Machine Learning (WFVML 2022)},
  url           = {https://pyrat-analyzer.com/}
}

@Article{10.1145/3656377,
  author     = {Banerjee, Debangshu and Xu, Changming and Singh, Gagandeep},
  journal    = {Proc. ACM Program. Lang.},
  title      = {Input-Relational Verification of Deep Neural Networks},
  year       = {2024},
  month      = jun,
  number     = {PLDI},
  volume     = {8},
  abstract   = {We consider the verification of input-relational properties defined over deep neural networks (DNNs) such as robustness against universal adversarial perturbations, monotonicity, etc. Precise verification of these properties requires reasoning about multiple executions of the same DNN. We introduce a novel concept of difference tracking to compute the difference between the outputs of two executions of the same DNN at all layers. We design a new abstract domain, DiffPoly for efficient difference tracking that can scale large DNNs. DiffPoly is equipped with custom abstract transformers for common activation functions (ReLU, Tanh, Sigmoid, etc.) and affine layers and can create precise linear cross-execution constraints. We implement an input-relational verifier for DNNs called RaVeN which uses DiffPoly and linear program formulations to handle a wide range of input-relational properties. Our experimental results on challenging benchmarks show that by leveraging precise linear constraints defined over multiple executions of the DNN, RaVeN gains substantial precision over baselines on a wide range of datasets, networks, and input-relational properties.},
  address    = {New York, NY, USA},
  articleno  = {147},
  doi        = {10.1145/3656377},
  issue_date = {June 2024},
  keywords   = {Abstract Interpretation, Deep Learning, Relational Verification},
  numpages   = {27},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3656377},
}

@InProceedings{DBLP_Ehlers17,
  author    = {R{\"{u}}diger Ehlers},
  booktitle = {Automated Technology for Verification and Analysis - 15th International Symposium, {ATVA} 2017, Pune, India, October 3-6, 2017, Proceedings},
  title     = {Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks},
  year      = {2017},
  editor    = {Deepak D'Souza and K. Narayan Kumar},
  pages     = {269--286},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {10482},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/atva/Ehlers17.bib},
  doi       = {10.1007/978-3-319-68167-2\_19},
  timestamp = {Tue, 21 Mar 2023 20:59:41 +0100},
  url       = {https://doi.org/10.1007/978-3-319-68167-2\_19},
}

@InBook{10.5555/3454287.3455169,
  author    = {Salman, Hadi and Yang, Greg and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Pengchuan},
  publisher = {Curran Associates Inc.},
  title     = {A convex relaxation barrier to tight robustness verification of neural networks},
  year      = {2019},
  address   = {Red Hook, NY, USA},
  abstract  = {Verification of neural networks enables us to gauge their robustness against adversarial attacks. Verification algorithms fall into two categories: exact verifiers that run in exponential time and relaxed verifiers that are efficient but incomplete. In this paper, we unify all existing LP-relaxed verifiers, to the best of our knowledge, under a general convex relaxation framework. This framework works for neural networks with diverse architectures and nonlinearities and covers both primal and dual views of neural network verification. Next, we perform large-scale experiments, amounting to more than 22 CPU-years, to obtain exact solution to the convex-relaxed problem that is optimal within our framework for ReLU networks. We find the exact solution does not significantly improve upon the gap between PGD and existing relaxed verifiers for various networks trained normally or robustly on MNIST and CIFAR datasets. Our results suggest there is an inherent barrier to tight verification for the large class of methods captured by our framework. We discuss possible causes of this barrier and potential future directions for bypassing it. Our code and trained models are available at http://github.com/Hadisalman/robust-verify-benchmark2.},
  articleno = {882},
  booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  numpages  = {12},
}

@Article{FSB,
  author  = {De Palma, Alessandro and Bunel, Rudy and Desmaison, Alban and Dvijotham, Krishnamurthy and Kohli, Pushmeet and Torr, Philip HS and Kumar, M Pawan},
  journal = {arXiv preprint arXiv:2104.06718},
  title   = {Improved branch and bound for neural network verification via lagrangian decomposition},
  year    = {2021},
}

@Misc{sensing,
  author        = {Rudy Bunel and Krishnamurthy Dvijotham and M. Pawan Kumar and Alessandro De Palma and Robert Stanforth},
  title         = {Verified Neural Compressed Sensing},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2405.04260},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2405.04260},
}

@Article{VHAGaR,
  author     = {Kabaha, Anan and Cohen, Dana Drachsler},
  journal    = {Proc. ACM Program. Lang.},
  title      = {Verification of Neural Networksâ Global Robustness},
  year       = {2024},
  month      = apr,
  number     = {OOPSLA1},
  volume     = {8},
  abstract   = {Neural networks are successful in various applications but are also susceptible to adversarial attacks. To show the safety of network classifiers, many verifiers have been introduced to reason about the local robustness of a given input to a given perturbation. While successful, local robustness cannot generalize to unseen inputs. Several works analyze global robustness properties, however, neither can provide a precise guarantee about the cases where a network classifier does not change its classification. In this work, we propose a new global robustness property for classifiers aiming at finding the minimal globally robust bound, which naturally extends the popular local robustness property for classifiers. We introduce VHAGaR, an anytime verifier for computing this bound. VHAGaR relies on three main ideas: encoding the problem as a mixed-integer programming and pruning the search space by identifying dependencies stemming from the perturbation or the networkâs computation and generalizing adversarial attacks to unknown inputs. We evaluate VHAGaR on several datasets and classifiers and show that, given a three hour timeout, the average gap between the lower and upper bound on the minimal globally robust bound computed by VHAGaR is 1.9, while the gap of an existing global robustness verifier is 154.7. Moreover, VHAGaR is 130.6x faster than this verifier. Our results further indicate that leveraging dependencies and adversarial attacks makes VHAGaR 78.6x faster.},
  address    = {New York, NY, USA},
  articleno  = {130},
  doi        = {10.1145/3649847},
  issue_date = {April 2024},
  keywords   = {Neural Network Verification, Global Robustness, Constrained Optimization},
  numpages   = {30},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3649847},
}

@Comment{jabref-meta: databaseType:bibtex;}
