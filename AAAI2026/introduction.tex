Deep neural networks (DNNs for short) have demonstrated remarkable capabilities, achieving human-like or even superior performance across a wide range of tasks. However, their robustness is often compromised by their susceptibility to input perturbations \cite{szegedy}. This vulnerability has catalyzed the verification community to develop various methodologies, each presenting a unique balance between completeness and computational efficiency \cite{Marabou,Reluplex,deeppoly}. This surge in innovation has also led to the inception of competitions such as VNNComp \cite{VNNcomp}, which aim to systematically evaluate the performance of neural network verification tools. Among them, NNenum \cite{nnenum}, Marabou \cite{Marabou,Marabou2}, and PyRAT \cite{pyrat} MnBAB \cite{ferrari2022complete}, built upon ERAN \cite{deeppoly} and PRIMA \cite{prima}; and $\alpha,\beta$-CROWN \cite{crown,xu2020fast}, the winner of the last 4 VNNcomp, benefiting from branch-and-bound based methodology \cite{cutting,BaB}.

The benchmarks usually focus on {\em local} robustness, i.e. given a DNN, an image and a small neighborhood around this image, is it the case that all the images in the neighborhood are classified in the same way by the DNN? The neighborhood is provided by a maximal perturbation of the input image, often an 
$L_\infty$-perturbation, i.e. every subpixel of the input image can vary in a very small range, typically $\frac{2}{255}$ (that is 2 levels of grey/blue/right/green), as $L_\infty$ is perfectly linear and perturbations of subpixels are independent. While the $L_\infty$-norm is the easiest to handle, it is not necessarily the most meaningful perturbation. More importantly, verification tools for local robustness are too computationally-intensive to be used in a real-time decision making pipeline: considering an autonomous car with a video feed from the dashboard, 
images of the feed cannot be certified robust in few ms to e.g. skip non-robust images and only consider certified robust images for the decision-making process.

\smallskip

In this paper, we consider {\em global} robustness, that is we do not restrict the certification process to the neighborhood of an input. We follow a two steps procedure. 
The first step, performed offline once, computes global bounds on the shift between 
output values of different decision classes due to the perturbation, as proposed in \cite{vhagar}. 
That is, considering decision classes $C$ and $D$ and perturbation $\varepsilon$, compute 
{\em global bound} $\beta^\varepsilon_{C,D} \geq \max_{I,I', |I-I'| \leq \epsilon}(value_{I}(C) - value_{I}(D) + value_{I'}(D) - value_{I'}(C))$, where $value_{J}(X)$ is the output value of class $X \in \{C,D\}$ for input image $J \in \{I,I'\}$. %Notice that as $I,I',C,D$ have symetrical roles, we can choose $\beta^\varepsilon_{D,C} = \beta^\varepsilon_{C,D}$. Also, as $I,I'$ have symetrcal role, the minimum value is exactly $-$ the maximum value. 
%Hence, if we have $n$ decision classes, we only have to compute $n (n-1)/2$ bounds. 
Bound $\beta^\varepsilon_{C,D}$ needs to be computed once for a DNN, 
and it is valid over the whole input space; compared with $k$ computations for {\em local robustness}, once for each of the $k$ input images, with results only valid for these $k$ images.

The second step is real-time, being performed with the DNN inference of the image $I$ considered: it suffices to consider the class $C$ with the highest output value $value_{I}(C)$, and check whether for every other class $D \neq C$, 
$value_{I}(C) - value_{I}(D) > \beta^\varepsilon_{C,D}$. 
If this is the case, then we are certified that image $I$ is robust for perturbation $\varepsilon$, because $\varepsilon$-perturbed image $I'$ could at most get  $value_{I'}(D) \leq \beta^\varepsilon_{C,D}  - (value_{I}(C) - value_{I}(D))  + value_{I'}(C) < value_{I'}(C)$, hence $C$ is also the predicted class for image $I'$. Otherwise, one could either skip image $I$ (in a video feed), or use safer degraded mode in the decision making process till a trustable robust image is received.


Our main contributions address the challenges to compute the {\em global bounds} $\beta^\varepsilon(C,D)$, for $C,D$ output neurons:
\begin{enumerate}
	%\item  Our first contribution studies the {\em LP relaxation} of the exact MILP encoding of ReLUs. {\color{blue} We establish in Proposition \ref{LP} its equivalence with the so-called "triangular abstraction"}.
	
	\item We develop a novel exact MILP encoding for the global robustness problem,
	called the {\em "2v" model}, where the variables are the values of the perturbed neurons, as well as the difference between the original and the perturbed neuron values (called the {\em diff variables}). We study and encode exactly in MILP how the {\em diff variables} evolve after passing through a ReLU (Prop.~\ref{Prop2}). Compared with the
	{\em classical MILP model} \cite{MILP} employed in \cite{vhagar,lipshitz},
	the linear relaxation is much more accurate, as each {\em diff variable} can be bounded after a ReLU as a function of the value of the {\em diff variables} before the ReLU, whereas the linear relaxations of the classical model is extremely inaccurate, as the variables are independent of each other. This was observed in \cite{lipshitz}, and constraints encoding
	a part of the linear relaxation of our "2v" model were added explicitly. Experimentally, 
	our "2v" model provides more accurate bounds than competing models, in the same runtime
	(Table \ref{table.classical}).
	%The number of variables a priori doubles compared with local robustness, from each neuron value in the perturbed image to each neuron value in the perturbed image {\em and} in the original image, as the original image is no more fixed. 
	%Recall that the worst case complexity of MILP is exponential in the number of binary variables \cite{DivideAndSlide}. 
	%A straightforward MILP model would be to use the  for each of these variables, as in \cite{vhagar,lipshitz}. The main issue with the classical model is that its linear relaxations is extremely inaccurate, as the variables are independent of each other. Instead, we develop another exact MILP model, 
	Further, we develop two abstract MILP models, namely the "3v" model which decouples the 
	{\em non-diff binary variables} of the "2v" model, resulting in looser bounds but with lower runtimes, as well as a computationally efficient "1v" model that only considers the 
	{\em diff variables}, however resulting in much looser bounds.
	%\item We adapt the Solution Aware Scoring from \cite{ATVA25} to our novel MILP models, in order to select the most important ReLUs to be treated using complex binary variables, while less important variables are treated using linear relaxation. The chosen number of binary variables depends upon the complexity of the DNN as well as the targeted runtime.

   \item  In terms of perturbations, we consider conjunctions of $L_\infty$ and $L_1$ norms, which allows to accurately describe perturbations. For instance, "each subpixel is perturbed by at most $\frac{20}{255}$ ($L_\infty$) and the sum of the absolute value of perturbations over all subpixels is at most $\frac{150}{255}$" ($L_1$ perturbation). While $L_1$ perturbations are not linear (because of the absolute values), reason for which it is seldom used, Prop.~\ref{prop.l1} shows how to use it as a perturbation in the MILP model without incurring any expansive binary variables (only cheap linear variables are necessary).

	\item Results obtained on the full input space have several shortcomings: while the bounds are fully correct, they are also particularly pessimistic, as all inputs, including absolutely improbable ones very far away from anything the network has been trained on, need to be accounted for. Also, the runtime to obtain the bound is particularly long. 
	%Finally, when computing worst-case 
	%pairs (image, perturbation), improbable images are generated, hence these  are not meaningful. 
	To solve this problem, we restrict the input space to ignore improbable inputs, using model order reduction techniques from engineering science \cite{Paco}. Specifically, we use Principal Component Analysis (PCA) to reduce the space to a linear input space. We choose the number of dimensions of the space to equal the accuracy of the DNN on the reduced space. 
	%(using a projection to the reduced space then the inverse projection to obtain a very similar image understandable by the DNN). 
	On the MNIST benchmark, this means 20 linear dimensions to match the $97\%$ accuracy of the DNN we considered, instead of the 784 dimensions of the full image space. Using PCA, which is linear, makes it easy to specify perturbations on the actual image (where it is meaningful) rather than on the reduced space.

\item Experimentally, this allows to obtain bounds for a considered pre-trained DNN for MNIST (with accuracy $97\%$) that can certify in {\em real-time} that $86\%$ of the images are robust for an $L_1$ perturbation of $.5$, resp. $53\%$ for an $L_1$ perturbation of $1$. We finally experimented our MILP models for a {\em regression} task as well, considering a DNN surrogate built from a slow physical finite element model, providing the deformation and the plastic strain of a pipe \cite{aiware}. The DNN has been learnt on a reduced order basis already of dimension 10, from thousands of input dimensions. The aim is to understand if the DNN surrogate that has been learnt is brittle to small perturbations, which could make it unfaithful to the physical model. Here we want to obtain bounds (which is what our models provide), as well as generating worst-case (image,perturbation) pair, where the strain is particularly different, close to the bound, and that can be run in the physical model to understand whether the DNN surrogate is faithful. Our tool was helpful to provide limited bounds, as well as an almost matching worst-case (image,perturbation) pair that was confirmed in the physical model.
Overall, the "2v" model generates accurate bounds. The decoupling of variables generating the "3v" model offers good trade-offs, reaching slightly lower bounds in the same runtime. 
The "1v" model is faster to converge, but also generates larger bounds, unless runtime is very restricted (Table \ref{table.pipe}).
\end{enumerate}

Limitation: We consider the standard ReLU activation function, though our findings can be extended to other activation functions, following similar extention by \cite{DivideAndSlide}, with updated MILP models e.g. for maxpool. 


%\newpage

%   
% 
%
%In this context, application of DNNs in safety critical applications is cautiously envisioned. For that to happen at a large scale, hard guarantees should be provided \cite{certification}, through e.g. incremental verification \cite{incremental}, so that to avoid dramatic consequences. It is the reason for the development of (hard) verification tools since 2016, with now many tools with different trade-offs from exact computation but slow (e.g. Marabou \cite{katz2019marabou}/Reluplex\cite{Reluplex}), up to very efficient but also incomplete (e.g. ERAN-DeepPoly \cite{deeppoly}). To benchmark these tools, a competition has been run since 2019, namely VNNcomp \cite{VNNcomp}. The current overall better performing verifier is $\alpha$-$\beta$-CROWN \cite{crown}, a fairly sophisticatedly engineered tool based mainly on "branch and bound" (BaB) \cite{BaB}, and which can scale all the way from complete on smaller DNNs \cite{xu2020fast} up to very efficient on larger DNNs, constantly upgraded, e.g. \cite{cutting}. 
%
%While the verification engines are generic, the benchmarks usually focus on local robustness, i.e. given a DNN, an image and a small neighbourhood around this image, 
%is it the case that all the images in the neighbourhood are classified in the same way.
%While some quite large DNNs (e.g. ResNet with tens of thousands of neurons) can be verified very efficiently (tens of seconds per input) \cite{crown}, with all inputs either certified robust or an attack on robustness is found; some smaller DNNs (with hundreds of neurons, only using the simpler ReLU activation function) cannot be analysed fully, with $12-20\%$ of inputs where neither of the decisions can be reached (\cite{crown} and Table \ref{tab:example}). Actually, DNNs which are trained to be robust (using DiffAI \cite{DiffAI} or PGD \cite{PGD}) are easier to verify, while the DNNs trained in a "natural" way are harder to verify.
%
%
%In this paper, we focus on DNNs trained in a "natural" way,
%%uncovering what makes the DNNs trained in a natural way so hard to verify (
%because for "easier" DNNs, adequate methods already exist. 
%To do so, we analyse the abstraction mechanisms at the heart of several efficient algorithms, namely Eran-DeepPoly \cite{deeppoly}, the Linear Programming approximation \cite{MILP}, PRIMA \cite{prima}, and different versions of ($\alpha$)($\beta$)-CROWN \cite{crown}. All these algorithms compute lower or/and upper bounds for the values of neurons (abstraction on values) for inputs in the considered input region, and conclude based on such bounds. For instance, if for all image $I'$ in the neighbourhood of image $I$, we have $weight_{I'}(n'-n) < 0$ for $n$ the output neuron corresponding to the expected class, then we know that the DNN is robust in the neighbourhood of image $I$. We restrict the formal study to DNNs using only the standard ReLU activation function, although nothing specific prevents the results to be extended to more general architectures. We uncover that {\em compensations} 
%(see next paragraph) is the phenomenon creating inaccuracies. We verified experimentally that a DNN trained in a natural way has heavier compensating pairs than DNNs trained in a robust way.
%
%Formally, a compensating pair is a pair of paths $(\pi,\pi')$ between a pair of neurons $(a,b)$, such that we have $w < 0 < w'$, for $w,w'$ the products of weight seen along $\pi$ and $\pi'$. Ignoring the (ReLU) activation functions, the weight of $b$ is loaded with $w \cdot weight(a)$ by $\pi$, while it is loaded with $w' \cdot weight(a)$ by $\pi'$. That is, it is loaded by $(w+w') weight(a)$. As $w,w'$ have opposite sign, they will compensate (partly) each other. The compensation is only partial due to the ReLU activation seen along the way of $\pi$ which can "clip" a part of $w \cdot weight(a)$, and similarly for $\pi'$. However, it is very hard to evaluate by how much without explicitly considering both phases of the ReLUs, which all the efficient tools try to avoid because it is very expansive (could be exponential in the number of such ReLU nodes opened).

%Our first main contribution is to formally show, in Theorem \ref{th1}, that compensation is the sole reason for the inaccuracies as (most) efficient algorithms will compute exact bounds for all neurons if there is no compensating pair of paths at all.
%While this theorem is theoretically interesting, it is not usable in practice as (almost) all networks have some compensating pairs. However, this notion of compensating pairs opens a first interesting idea concerning an exact abstraction of the network using a Mixed Integer Linear Program \cite{MILP}, where the weight of each neuron is a linear variable, and ReLU node may be associated with binary variables (exact encoding) or linear variables (overapproximation). While LP tools can scale to thousands of linear variables, MILP encoding can only be solved for a limited number of binary variables. This suggests that a simpler encoding could be used for those ReLUs that are not on compensating pairs, as their precise outcome may not be necessary.

%Our second main contribution is to show formally in Theorem \ref{th2}, that 
%encoding all ReLU nodes on a pair of compensating paths with a binary variable,
%and using linear relaxation for the other ReLU nodes, will lead to exact bounds for (most) of the algorithms considered. This theorem allows to restrict the number of integer variables, and thus to obtain encodings that are faster to solve. Practically, however, (almost) all ReLU nodes are on some compensating path, and using this exact restricted MILP encoding will be too time consuming.

%Our third main contribution is more practical, proposing Algorithm \ref{algo1} based on this knowledge that compensating pair of paths are the reason for inaccuracy. The idea is thus to use this information to rank the ReLU nodes in terms of importance, and only keep the most important ones as binary variables, and use linear relaxation for the least important ones.
%%More precisely, the algorithm will, as DeepPoly, consider layers one by one and neurons $b$ %on this layer one by one, selecting the heaviest pairs of compensating paths ending in $b$
%%and associating these nodes with a binary variable. Then an MILP tool such as Gurobi is used %to compute the lower and upper bound for node $b$. 
%Overall, the worst case complexity of algorithm \ref{algo1} is lower than $O(N 2^K LP(N))$, where $N$ is the number of nodes of the DNN, $K$ the number of ReLU nodes selected as binary variable, and $LP(N)$ is the (polynomial time) complexity of solving a linear program representing a DNN with $N$ nodes. This complexity is an upper bound, as e.g. Gurobi is fairly efficient and never need to consider all of the $2^K$ ReLU configurations to compute the bounds. Keeping $K$ reasonably low thus provides an efficient algorithm. 
%By design, it will never run into a complexity wall (unlike the full MILP encoding), although it can take a while on large networks because of the linear factor $N$ in the number of nodes. An additional interesting point is that it is extremely easy to parallelize, as all the nodes in the same layer can be run in parallel. We verify experimentally that the algorithm offers interesting trade-offs, by testing on local robustness for DNNs trained "naturally" (and thus difficult to verify).


%KSM: I suggest we move this to experimental evaluation
%This paper does not focus on producing the most efficient tool, and we did not spend engineering efforts to optimize it. The focus is instead on the novel notion of compensation, the associated methodology and its evaluation. For instance, our implementation is fully in Python, with uncompetitive runtime for our DeepPoly implementation ($\approx 100$ slower than in CROWN). Still, evaluation of the methodology versus even the most efficient tools reveals a lot of potential for the notion of compensation, opening up several opportunities for applying it in different contexts of DNN verification (see Section \ref{Discussion}). 

