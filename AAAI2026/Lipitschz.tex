\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai2026}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\pagestyle{plain}
\usepackage{threeparttable}
\input{math_commands.tex}
\usepackage{lineno}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{cases}
\captionsetup{compatibility=false}
\usepackage{epstopdf}
\usepackage{placeins}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{calc}
\usepackage{array}
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usetikzlibrary{positioning, arrows.meta,calc}
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
	/TemplateVersion (2026.1)
}

\title{Order reduction and partial MILP models \\
	for certifying Robustness of DNNs globally.}
\date{}
\author{
	%Authors
	% All authors must be in the same font size and format.
	Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
	AAAI Style Contributions by Pater Patel Schneider,
	Sunil Issar,\\
	J. Scott Penberthy,
	George Ferguson,
	Hans Guesgen,
	Francisco Cruz\equalcontrib,
	Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
	%Afiliations
	\textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
	% If you have multiple authors and multiple affiliations
	% use superscripts in text and roman font to identify them.
	% For example,
	
	% Sunil Issar\textsuperscript{\rm 2},
	% J. Scott Penberthy\textsuperscript{\rm 3},
	% George Ferguson\textsuperscript{\rm 4},
	% Hans Guesgen\textsuperscript{\rm 5}
	% Note that the comma should be placed after the superscript
	
	1101 Pennsylvania Ave, NW Suite 300\\
	Washington, DC 20004 USA\\
	% email address must be in roman text type, not monospace or sans serif
	proceedings-questions@aaai.org
	%
	% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
	Author Name
}
\affiliations{
	Affiliation\\
	Affiliation Line 2\\
	name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
	% Authors
	First Author Name\textsuperscript{\rm 1},
	Second Author Name\textsuperscript{\rm 2},
	Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
	% Affiliations
	\textsuperscript{\rm 1}Affiliation 1\\
	\textsuperscript{\rm 2}Affiliation 2\\
	firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newcommand{\vW}{\boldsymbol{W}}
\newcommand{\val}{{\textrm{value}}}
\newcommand{\Val}{{\textrm{value}}}
\newcommand{\MILP}{{\textrm{MILP}}}
\newcommand{\LP}{{\textrm{LP}}}
\newcommand{\Improve}{\mathrm{Improve}}
\newcommand{\Utility}{\mathrm{SAS}}
\newcommand{\Sol}{\mathrm{Sol}}
\newcommand{\sol}{\mathrm{sol}}
\newcommand{\UB}{\mathrm{UB}}
\newcommand{\LB}{\mathrm{LB}}
\newcommand{\ub}{\mathrm{ub}}
\newcommand{\lb}{\mathrm{lb}}
\newcommand{\B}{\mathrm{B}}
\usepackage{amsmath, amssymb, amsfonts}
\newcommand{\ReLU}{\mathrm{ReLU}}
\newcommand{\CMP}{{\textrm{CMP}}\ }
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\toolname}{Hybrid MILP}

% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		\input{abstract.tex}
		\iffalse
		Most DNNs are brittle to small perturbations. Extensive works have thus been performed to verify robustness for DNNs.
		However, these works mostly consider local robustness, i.e. in the neighborhood of an image.
		While local robustness is useful to have an idea how often non robust images happen, by repeating the verification on 1000 or 10000 pre-obtained images, the main shortcoming is that we have no guarantee that a specific new incoming image, e.g. in a video feed, is robust: The verification process takes too long and requires too much resources to be performed online on embedded systems.
		
		In this paper, we consider {\em global} robustness, that is, guarantees not restricted to a set of local images. For that, we consider {\em bounds} on the switch of values between the different decision classes of a DNN due to a given perturbation. 
		The verification question is much harder than local robustness, as the number of complex variables doubles (from the deviation image to the image and its deviation).
		Further, the values each neuron can take is no more in a small neighborhood.
		Therefore, the global verification process is very complex.
		To obtain useable bounds, we develop several novel partial MILP models for global robustness, with different trade-offs. Last, we use order reduction techniques to reduce the space of images considered, avoiding unrealistic inputs, by using linear PCA. 
		This results into usable bounds, allowing in real time to certify robustness for $87\%$ of incoming images in the MNIST benchmark for a L1-perturbation of $0.5$, as well as for a surrogate computing the hidden plastic strain associated to a deformation map of a pipe.
		\fi
	\end{abstract}
	
	
	\section{Introduction}
	
	\input{introduction}
	
	\input{notation}
	
	\input{global}
	
	\input{a_trick}
	
	
	
	
	
	


	\section{Experimental Evaluation}
	
	We implemented our code in Python 3.8.
	Gurobi 9.52 was used for solving LP and MILP problems. We conducted our evaluation on an AMD Threadripper 7970X  ($32$ cores$@4.0$GHz) with 256 GB of main memory and 2 NVIDIA RTX 4090. 
	
	We will do experiments on two networks for MNIST dataset and another physical model. Two MNIST have the same structure. They are full-connected DNN with 5, 10 outputs and 784 inputs; each hidden layer has 100 neurons. One of them is a normally trained neural network and another is for trained for adversarial robustness. We call them MNIST $5\times100$-Normal and MNIST $5\times 100$-DiffAI. As the name shows MNIST $5\times 100$-DiffAI has a better robustness. The physical network is a simplified model with 10 input neurons, 26 output neurons, and 2 hidden layers with 50 neurons each.
	
	\subsection{Experiment results for robustness (MNIST)}
	

	
	\begin{table}[h!]
	\begin{tabular}{|l|c|c|c|c|}\hline\hline
		$L_1\leq 0.5$ &        Bound $\downarrow$ &  Sol. &      Worst-Case $\uparrow$ &  Time \\\hline \hline
		1v, $375 \times 1$ & 17.4724 & 0.9089  ?? & 0.0917 & 43200s \\\hline 
		2v, $375 \times 2$ & 21.0135 & 8.4672& 0.2229 & 43200s \\\hline		
		2v, $450 \times 2$ & 12.0097 & 3.4408 & {\bf 0.2584} & 43200s \\\hline
		1v, $500 \times 1$ & ?? & ?? & ?? & 43200s \\\hline 
		3v, $500 \times 3$ & ?? & ?? & ?? & 43200s \\\hline 
	 2v, $500 \times 2$ & {\bf 8.7608} & nan & nan & 43200s \\\hline\hline

	 2v, $485 \times 2$ & 20.6460 & 1.9152 & {\bf 0.3722} & 130000s \\\hline\hline

		 2v, $500 \times 2$ & {\bf 8.2573} & nan & nan & 260000s \\\hline\hline
		 
	\end{tabular}
	\caption{Comparison of "1v", "3v" and "2v" models 
	to obtain bounds on $\beta^{0.5}_{0,1}$ on the {\bf full dimension} MNIST DNN, for timeouts of 43200s, 130000s and 260000s, where 375, 450, 485 or 500 ($\times 1$, $\times 2$, $\times 3$) neurons use binary variables.}
\end{table}

On the full space, we reach bounds of $8.2$ (on $\beta^{.5}_{0,1}$), which is too pessimistic as it allows to certify 0 image robust. The worst-case found by the "2v" $485 \times 2$ binary variables after 130000s only displays a difference of 
$0.37$, $20 \times$ away from the bound found.

\begin{figure*}[t!]
	\centering
\includegraphics[scale=0.5]{image.png} \hspace{0.8cm}
\includegraphics[scale=0.5]{perturb.png}
\caption{An improbable image and its perturbation (difference around x=11, y=18) 
with maximal $\beta^{.5}_{0,1}=0.37$ for MNIST as obtained by the "2v" $485 \times 2$ model in full 758 dimension image space.}
\end{figure*}	




\paragraph{Reduced space}

We now consider a PCA model order reduction to avoid considering improbable images, 
speed up obtaining the bounds, and obtain less pessimistic bounds.
We settle on 20 orders, as the MNIST DNN considered run on a images obtained from projecting to the reduced order and projected back to the full dimension 
display the same accuracy of $97$\% as the DNN on the original images, meaning no accuracy is lost from reducing to 20 dimensions, which is the only thing which matters. On such a reduced space, bounds obtained are much more precise, and one can certify robustness of images online.



	\begin{table}[h!]
	\begin{tabular}{|l|c|c|c|c|}\hline\hline
		$L_1\leq 0.5$ &        Bound $\downarrow$ &  Sol. &      Worst-Case $\uparrow$ &  Time \\\hline \hline
		
1v, $?? \times 1$ & ?? & ?? & ?? & ?? \\\hline 
		3v, $?? \times 3$ & ?? & ?? & ?? & ?? \\\hline 
	 2v, $?? \times 2$ & ?? & ?? & ?? & ?? \\\hline\hline
	 
		1v, $100 \times 1$ & 1.508 & 0.1856 & ?? & 14400 \\\hline 
		3v, $100 \times 3$ & 1.256 & 0.1336 & ?? & 14400 \\\hline 
	 2v, $100 \times 2$ & 1.2417 & ?? & ?? & 14400 \\\hline\hline
	 
	\end{tabular}
	\caption{Comparison of "1v", "3v" and "2v" models 
	to obtain bounds on $\beta^{0.5}_{0,1}$ on the {\bf 20 dimension}  reduced order MNIST DNN, for timeouts of ??s, and ??s , where ??,  or 500 ($\times 1$, $\times 2$, $\times 3$) neurons use binary variables.}
\end{table}






\begin{figure*}[t!]
	\centering
\includegraphics[scale=0.5]{redimage.png} \hspace{0.8cm}
\includegraphics[scale=0.5]{redperturb.png}
\caption{An image and its perturbation from the 20-dimension reduced space with maximal $\beta^{.5}_{0,1}=0.17$ for MNIST as obtained by the "2v" $500 \times 2$ model.}
\end{figure*}	

	
	
\begin{table}[h!]
	\begin{tabular}{|l|c|c|c|c|}\hline\hline
		model &    $L_1\leq 0.5$ & $L_1\leq 1$ & $L_1\leq 1.5$ &  $L_1\leq 2$ \\\hline \hline
		1v, $? \times 1$ & $80 \%$ & $32\%$ & $7\%$ & $0\%$ \\\hline
		3v, $? \times 3$ & {\bf 86 \%} & {\bf 53\%} & {\bf 20\%} & {\bf 4\%} \\\hline
		2v, $? \times 2$ & 84\% & 51\% & 16\% & 4\% \\\hline \hline
	\end{tabular}
	\caption{Comparison of "1v", "3v" and "2v" models 
	in the percentage of images they can certify in real-time for different values of $L_1$ perturbations.}
\end{table}


	


\subsection{Experiment results for regression (Pipe strain)}
	
	For the pipe system, to find a example of large difference on outputs is one of the main aims.

	\begin{figure*}[t!]
\includegraphics[scale=0.5]{deform.png} \hspace{0.8cm}
\includegraphics[scale=0.5]{strain.png}
\caption{2 slightly different deformations and their associated quite different strain as obtained by the "2v" $100 \times 2$ model.}
\end{figure*}	


	The network of Pipe system is relatively simple and in the tests, we will open all $\ReLU$ nodes.

	

	We will compare the results of different modeling methods.
	
	\vspace*{1ex}
	
	\iffalse
	\begin{table}[h!]
	\begin{tabular}{|l|l|l|l|l|}\hline
		$L_1\leq 0.83$ &        Bound $\downarrow$ &  Solution $\uparrow$ &      Real $\uparrow$ &  Time \\\hline
		1v,open 100 &     {\bf 0.035613} &  0.035613 &                       0.01288 & 10608 \\\hline
		3v,open 100 &     0.040074 &  0.028934 &                      0.021441 & 10922 \\\hline
		%3v,open 100 &     0.039824 &  0.028832 &                      0.022255 & 22153 \\\hline
		2v,open 100 &     0.046719 &  0.024364 &  {\bf 0.024436} & 10922 \\\hline
	\end{tabular}
	\caption{Comparison of 1v,2v and 3v models on the pipe system with a fixed timeout of 10.000s.}
\end{table}
\fi
	
		
	\begin{table}[h!]
	\begin{tabular}{|l|c|c|c|c|}\hline\hline
		$L_1\leq 0.83$ &        Bound $\downarrow$ &  Sol. &      Worst-Case $\uparrow$ &  Time \\\hline \hline
		1v, $100 \times 1$ &     {\bf 0.0356} &  $0.0356$ & $0.0191$ &   1000s \\\hline
		3v, $100 \times 3$&     0.0414 &  0.0254 &  0.0166 &  1000s \\\hline
		2v, $100 \times 2$&     0.0418 &  0.0229 &   {\bf 0.0229} &  1000s \\\hline \hline
		3v, $90 \times 3$&      ?? &  ?? &  ?? & 14523s \\\hline
		3v, $100 \times 3$&      {\bf 0.0350} &  0.0272 &  0.0216 & 14523s \\\hline
		2v, $90 \times 2$&     ?? &  ?? &   ?? & 14523s \\\hline
		2v, $100 \times 2$&     0.0360 &  0.0236 &    {\bf 0.0236} & 14523s \\\hline \hline
		3v, $100 \times 3$&     {\bf 0.0329} &  0.0277 &  0.0165 & 72126s \\\hline
		2v, $100 \times 2$&     0.03375 &  0.0244 &  {\bf 0.0245} & 72126s \\\hline\hline
	\end{tabular}
	\caption{Comparison of "1v", "3v" and "2v" models on the pipe system with a fixed timeout of 1000s, 14523s and 72126s, where 90 ($\times 2$, $\times 3$) or all 100 ($\times 1, \times 2,\times 3$) neurons use binary variables.
	L1 corresponds to $3.9$ or $4$, and results should be the sum of 10 pixels, so around 10 times higher values.}
\end{table}

	
	
	
	
	
	 
	
	
	\newpage

	\hfill

	\newpage
	
	
	\bibliography{references}
	
	
\end{document}
