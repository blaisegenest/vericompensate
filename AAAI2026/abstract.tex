Deep neural networks (DNNs) are often brittle to small perturbations, leading to extensive research into methods to verify their robustness. However, most existing methods focus on local robustness, i.e., the model's behavior in the neighboorhood of an specific input (e.g., an image). While local robustness enables to evaluate how often non robust images appear in a large dataset (e.g., by repeating the verification on 10000 pre-obtained images), it fails to provide guarantees on whether a new specific incoming image is robust, e.g., real-time inputs in a video stream. Moreover, current verification techniques are computationally intensive, rendering them impractical for real-time deployment on embedded systems. 

In this paper, we consider {\em global} robustness, that is, guarantees not restricted to a set of local images. We focus on deriving {\em bounds} on the variation of output values across different decision classes of a DNN under a given perturbation. This global verification problem is significantly more complex than local robustness, as the number of variables doubles (from the deviation image to the image and its deviation). Further, the values each neuron can take are no longer limited to a small neighborhood. To obtain usable bounds, we develop several novel partial MILP models for global robustness, with different trade-offs. Last, we use order reduction techniques to reduce the space of images considered, avoiding unrealistic inputs, by using principal component analysis (PCA). Hence, we are able to certify in real time the robustness for 87\% of incoming images in the MNIST benchmark for an $\ell_1$-perturbation of 0.5, as well as for a surrogate computing the latent plastic strain associated with a pipe deformation map.