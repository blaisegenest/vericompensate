Deep neural networks (DNNs) are often brittle to small perturbations, leading to extensive research into methods to verify their robustness. Most existing methods focus on local robustness, i.e., the model's behavior in the neighborhood of a specific input. While local robustness enables to evaluate how often non robust images appear in a large dataset (e.g., by repeating the verification on 1000 pre-obtained images), it fails to provide guarantees on whether new specific incoming inputs are robust, e.g., real-time images in a video stream. Moreover, most verification techniques are computationally intensive, rendering them impractical for real-time deployment on embedded systems. 

In this paper, we consider {\em global} robustness, that is, guarantees not restricted to a set of local images. We focus on deriving {\em bounds} on the variation of output values across different decision classes of a DNN under a given perturbation. This global verification problem is significantly more complex than local robustness, as the number of variables doubles (from the deviation image to the image and its deviation). Further, the values each neuron can take are no longer limited to a small neighborhood. To obtain usable bounds, we develop several novel  MILP models for global robustness, with different trade-offs. Last, we use order reduction techniques to reduce the space of inputs considered, avoiding unrealistic images, by using principal component analysis (PCA). This results into the {\em real-time} certification of the robustness for $86\%$ of incoming images in the MNIST benchmark for an $L_1$-perturbation of $.5$, as well as for a surrogate computing the 
%latent
plastic strain associated with a pipe deformation map.