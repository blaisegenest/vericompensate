\documentclass[]{article}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newcommand{\ReLU}{\mathrm{ReLU}}



\title{Proof No Diamond (3 layers)}
\date{}

\begin{document}

\maketitle

\begin{definition}
	A pair of paths $(\pi,\pi')$
	is called {\em compensating} if they start in the same neuron $a$ and 
	ends in the same neuron $z$, and the product of weights over $\pi$ is strictly positive and the product of weights over $\pi'$ is strictly negative.
\end{definition}

Intuitively, compensating paths will partially cancel out each other as they conribute the same weight $w(a)$ to the weight of the same neuron $w(z)$, but with opposite sign. 
It is not simple to take this compensation into account because of ReLUs: the particular compensation will depend upon the weight of intermediate nodes seen along $\pi$ and $\pi'$, 
as when one of this node gets negative input, it will clip it to 0.


We assume all active functions are ReLU function.

\begin{definition}

	\begin{itemize}
	 \item  We use $a_i$ to denote nodes in the input layer, use $b_j$ to denote nodes in the first hidden layer, etc and finally use $z_i$ to denote the output nodes.
	For $L,L'$ consecutive layers, for $x \in L$ and $y \in L'$, we denote 
	$w({x y})$ to denote the fixed weight from $x$ to $y$, while we use
	$weight_Y(x)$ to denote the weights of node $x$ from initial configuration $Y$. 
	We use $bias$ to denote bias weightt.
	
	\item We use $b^-$ to denote the node before ReLU and $b^+$ to denote the node after ReLU: $b^+ = \ReLU(b^-)$.
	\end{itemize}
	
%	3. $\bar{f}$ is the upper bound approximation function of DeepPoly, and $\underline{f}$ is the lower bound approximation function.
\end{definition}





\begin{theorem}
	\label{th1}
	If for all $(x,y)$ there is no compensation paths $(\pi,\pi')$ 
	in the network from $x$ to $y$, then the LP approximation is 
	$100\%$ accurate (or deepoly with the abstraction $\underline{f}(x) = 0$ for the lower bound, never using the $\underline{f}(x) = x$ abstraction). 
\end{theorem}

\subsection*{Proof of Theorem \ref{th1}}

First, notice that if $weight_Y(b^-)$ is maximal (resp. minimal), 
then $weight_Y(b^+=\ReLU(b^-))$ also gets maximal (resp. minimal).
The following definition is the most important concept in this proof.

\begin{definition}[Sign of Node]
	We define a sign function $S$ on nodes $n$ such that: 	(1). $S(n)=0$ if all path from $n$ to $c$ has 0 weight; (2). $S(n)=1$ if all path from $n$ to $c$ has non-negative weight, and at least one path has a positive weight; (3). $S(n)=-1$ if all path from $n$ to $c$ has non-positive weight, and at least one path has a negative weight. 
	In general, $S$ may not be defined on every node (e.g. if there is a negative and a positive path from $a$ to $c$). However, if there is no Diamond, $S$ is defined on all nodes: any node fulfills one of above cases (1),(2),(3).


	For instance, for an input node $a_i$ of any 3 layer DNN:
	\begin{enumerate}
      \item  $S(a_i)=0$ if 
	  for all $b_j, w(a_i b_j)\cdot w(b_j c) = 0$
	
	
	 \item  $S(a_i)=1$ if for all $b_j, w(a_i b_j)\cdot w(b_j c) \geq 0$ and there exists 
	 $j, w(a_i b_j)\cdot w(b_j c) > 0$
	
	\item $S(a_i)=-1$ if for all $b_j\ w(a_i b_j)\cdot w(b_j c) \leq 0$ and there exists 
	$j, w(a_i b_j)\cdot w(b_j c) < 0$ 
\end{enumerate}

For $b_j$ in the hidden layer, we have $S(b_j)=1,-1,0$ if $w(b_j c)$ is positive, negative, or 0 respectively. Finally, for the output node $c$, we define $S(c)=1$.
\end{definition}


\begin{lemma}[Sign]
	\label{lemma1}
Let $L,L'$  are consecutive layers be of a DNN without compensation. 
Then if both 
$w(m n) \neq 0$ and $S(n) \neq 0$, then 
	$S(m)=S(n)\mathrm{Sign}(w(n m))$.
\end{lemma}

\begin{proof}
	If $S(n) \neq 0$, then there is a path $\pi$ from $n$ to the output node $c$ with a nonzero weight of the same sign as $S(n)$. 
	
	Hence there is a non zero path from $m$ to $c$: $(m n) \pi$, which is of sign 
	$S(n)\mathrm{Sign}(w(mn))$. As there is no compensation, $S(m)=S(n)\mathrm{Sign}(w(mn))$.
\end{proof}


For a node $n$, we use $n_s$ to denote $S(n)\cdot n$. 
Notice that for $S(n)=1$, $n_s$ gets maximal value whenever $n$ gets maximal value; 
while for $S(n)=-1$, $n_s$ gets maximal value whenever $n$ gets minimal value (and vice versa). For $S(n)=0$, $n_s=0$ and thus always reach his minimal and maximal.



\begin{lemma}
	\label{lemma2}
	Let $L,L'$ be consecutive layers of a compensation free DNN, and $n \in L'$. Then:
		$$ \max(n^-_s)=\sum_{m \in L}w(m n) \max(m^+_s)+bias_n \text{ and }$$
		$$\max(n^+_s)=ReLU(\sum_{m \in L}w(m n) max(m^+_s)+bias_n)$$

	
	
	Similarly  for minimal value,	
	$ \min(n^-_s)=\sum_{m \in L}w(m n)\min(m^+_s)+bias_n $ and
	$ \min(n^+_s)=ReLU(\sum_{m \in L}w(m n)\min(m^+_s)+bias_n)$
\end{lemma}

\begin{proof}
We choose $\bar{Y}$ maximizing each $a_s \in \{a,-a\}$ in the input layer $L=L_0$,
	that is setting $a$ as $max(a)$ for $S(a)=1$ and $min(a)$ for $S(a)=-1$.
At configuration $\bar{Y}$, every $a_s(\bar{Y}))=\max_Y {a_s(Y))}=max(a_s)$ is maximized.  

Consider any $b$ in the next layer $L_1$.
Assume first that $S(b)=+1$.
We have $weight(b_s)= weight(b) = \sum_{a \in L_0} w(a b) weight(a) + bias_b$.
If every $w(a b) weight(a)$ is maximized in the same time, 
then $weight(b_s)= weight(b)$ is also maximized. 

We claim that this is the case at 
configuration $\bar{Y}$.
Indeed, if $w(a b)=0$, any $weight(a)$ can be chosen to maximize 
$w(a b) weight(a)$. If $w(a b)>0$ then $S(a) = +1$ by Lemma \ref{lemma1}, since
$S(b)=+1$. We have $max(a_s) = max(a)$, maximizing $w(a b) weight(a)$.
Finally, if $w(a b)<0$ then $S(a_s) = -1$, and we have 
$max(a_s) = -min(a)$, maximizing $w(a b) weight(a)$.

Hence $b$ reaches is maximum in configuration $\bar{Y}$, and the value satisfies: 
$$\max(b)=\sum_{m \in L}w(a b) \max(a_s)+bias_b$$

The case $S(b)=-1$ is symetric:
$\max(b_s)= -\min(b)$ is reached when minimizing $b$, which is also 
satisfied at $\bar{Y}$.
In both case, $weight(b_s)$ is maximized at $\bar{Y}$. Notice that $\bar{Y}$ does not depend upon $b$, so choosing $\bar{Y}$ uniformly maximize all $weight(b_s)$.

This implies that $ReLU(b^-_s)$ is maximized at $\bar{Y}$, and its value is 
$ReLU(\sum_{m \in L}w(a b) \max(a_s)+bias_b)$, again uniformly reached over all $b$ at configuration $\bar{Y}$.

We proceed by induction over every layer of the DNN till the output layer, proving that 
$max(z_s)=max(z)$ is reached at configuration $\bar{Y}$.

The case for $min(x_s)$ is similar, reached at configuration $\underline{Y}$.
\end{proof}



Notice that for all $x$, either $S(x)=+1$ and 
$(\min(x),\max(x))=(\min(x_s),\max(x_s))$, 
or $S(x)=-1$ and $(\min(x),\max(x))=(-\max(x_s),-\min(x_s))$.
In both case, these two bounds are found in Lemma \ref{lemma2},
from configuration $\bar{Y}$ and $\underline{Y}$.
We now show that DeepPoly will generate the same bounds 
$\underline{f}(x)=\min(x)$ and $\bar{f}(x)=\max(x)$.

\paragraph{DeepPoly bounds}

The DeepPoly function for $b$ can be expressed by the following steps.
First, we partition the set of indices $i$ as $I,I_0,I^+,I^-$ with:
\begin{itemize}
	\item $i \in I$ if the sign of $b_i^-=\sum_{j} w_{a_j b_i} a_j + B_{b_i}$ is always positive irrespective of $Y=(a_j)_j$, in which case 
	$\bar{f}(b_i^+)(Y)=\underline{f}(b_i^+)=\sum_{j} w_{a_j b_i} a_j + B_{b_i}$, and else:
	\item $i \in I_0$ if the sign of $b_i^-=\sum_{j} w_{a_j b_i} a_j + B_{b_i}$ is always negative irrespective of $Y=(a_j)_j$, in which case $\bar{f}(b_i^+)=\underline{f}(b_i^+)=0$,	and else:
	\item $i \in I^+$ if $w_{b_i c}>0$, and:
	\item $i \in I^-$ if $w_{b_i c}<0$.
	\end{itemize}
	
Remark that for all $i \notin I$, we have $\underline{f_i}(b_i^+)(Y)=0$.
	

We have:
\begin{align*}
	\bar{f}(c)(Y) = &
	bias_c + \sum_{b \in I} w_{bc}\sum_{a} w_{a b} weight(a) + bias_{b} \\
	& + \sum_{b \in I^+} w_{b c} \cdot \bar{f}(ReLU(\sum_{a} w_{a b} weight(a) 
	+ Bias_{b}))(Y) 
	\end{align*}



We proceed inductively, and prove the inductive step.
The initialization is obvious as DeepPoly is always exact for the initialization layer.

\begin{lemma}
	Let $L,L'$ two consecutive layers.
	Assume for all node $m$ of $L$ that the lower and upper bounds for $m$ used by DeepPoly equals to the exact lower and upper bounds of $m$.
	
	Then DeepPoly computes the exact lower and upper bounds for every node $n$ of layer $L'$.
\end{lemma}

\begin{proof}
	Suppose $n$ is a node in $L'$ such that $S(n)\neq 0$ (otherwise, it is trivial). 
	First assume $S(n)=1$. The case $S(n)=-1$ is similar. 

	Deeppoly computes its upper bound using $m=\sum_{n}w(mn)\hat{n}+b,$ by taking all $n$ that $w(mn)>0$ to upper bound and taking all $n$ that $w(mn)<0$ to lower bound.
	
	On the other hand, by assumption, for all $n$ that $S(n)\neq 0$(which is equivalent to $w(mn)\neq 0$), the upper and lower bounds stored by Interval Arithmetic is equal to their exact upper and lower bounds, i.e., their maximal and minimal values. And for every $n$, its upper bound if $w(mn)>0$ or lower bound if $w(mn)<0$ is equal to the maximal value of $n^s$. 
	
	Hence $$u(m)_{IA}=\sum_{n}w(mn)\ReLU(\max(n^s))+b=\max(m^s).$$ This is what we want to show.
\end{proof}

Use Lemma 3, we can see the Interval Arithmetic upper and lower bounds of the output node $z$ are equal to the actual upper and lower bounds of $z$



\end{document}
