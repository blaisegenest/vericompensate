% Encoding: UTF-8

@Article{prima,
  author     = {M\"{u}ller, Mark Niklas and Makarchuk, Gleb and Singh, Gagandeep and P\"{u}schel, Markus and Vechev, Martin},
  journal    = {Proc. ACM Program. Lang.},
  title      = {PRIMA: General and Precise Neural Network Certification via Scalable Convex Hull Approximations},
  year       = {2022},
  month      = {jan},
  number     = {POPL},
  volume     = {6},
  abstract   = {Formal verification of neural networks is critical for their safe adoption in real-world applications. However, designing a precise and scalable verifier which can handle different activation functions, realistic network architectures and relevant specifications remains an open and difficult challenge. In this paper, we take a major step forward in addressing this challenge and present a new verification framework, called PRIMA. PRIMA is both (i) general: it handles any non-linear activation function, and (ii) precise: it computes precise convex abstractions involving multiple neurons via novel convex hull approximation algorithms that leverage concepts from computational geometry. The algorithms have polynomial complexity, yield fewer constraints, and minimize precision loss. We evaluate the effectiveness of PRIMA on a variety of challenging tasks from prior work. Our results show that PRIMA is significantly more precise than the state-of-the-art, verifying robustness to input perturbations for up to 20\%, 30\%, and 34\% more images than existing work on ReLU-, Sigmoid-, and Tanh-based networks, respectively. Further, PRIMA enables, for the first time, the precise verification of a realistic neural network for autonomous driving within a few minutes.},
  address    = {New York, NY, USA},
  articleno  = {43},
  doi        = {10.1145/3498704},
  issue_date = {January 2022},
  keywords   = {Polyhedra, Robustness, Abstract Interpretation, Convexity},
  numpages   = {33},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3498704},
}

@Article{crown1,
  author    = {Shiqi Wang and Huan Zhang and Kaidi Xu and Xue Lin and Suman Jana and Cho-Jui Hsieh and J Zico Kolter},
  title     = {Beta-{CROWN}: Efficient Bound Propagation with Per-neuron Split Constraints for Neural Network Robustness Verification},
  year      = {2021},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
  url       = {https://openreview.net/forum?id=ahYIlRBeCFw},
}

@InProceedings{crown,
  author    = {Wang, Shiqi and Zhang, Huan and Xu, Kaidi and Lin, Xue and Jana, Suman and Hsieh, Cho-Jui and Kolter, J. Zico},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Beta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Neural Network Robustness Verification},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {29909--29921},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/file/fac7fead96dafceaf80c1daffeae82a4-Paper.pdf},
}

@Article{deeppoly,
  author     = {Singh, Gagandeep and Gehr, Timon and P\"{u}schel, Markus and Vechev, Martin},
  journal    = {Proc. ACM Program. Lang.},
  title      = {An Abstract Domain for Certifying Neural Networks},
  year       = {2019},
  month      = {jan},
  number     = {POPL},
  volume     = {3},
  abstract   = {We present a novel method for scalable and precise certification of deep neural networks. The key technical insight behind our approach is a new abstract domain which combines floating point polyhedra with intervals and is equipped with abstract transformers specifically tailored to the setting of neural networks. Concretely, we introduce new transformers for affine transforms, the rectified linear unit (ReLU), sigmoid, tanh, and maxpool functions. We implemented our method in a system called DeepPoly and evaluated it extensively on a range of datasets, neural architectures (including defended networks), and specifications. Our experimental results indicate that DeepPoly is more precise than prior work while scaling to large networks. We also show how to combine DeepPoly with a form of abstraction refinement based on trace partitioning. This enables us to prove, for the first time, the robustness of the network when the input image is subjected to complex perturbations such as rotations that employ linear interpolation.},
  address    = {New York, NY, USA},
  articleno  = {41},
  doi        = {10.1145/3290354},
  issue_date = {January 2019},
  keywords   = {Adversarial attacks, Deep Learning, Abstract Interpretation},
  numpages   = {30},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3290354},
}

@InProceedings{Reluplex,
  author    = {Katz, Guy and Barrett, Clark and Dill, David L. and Julian, Kyle and Kochenderfer, Mykel J.},
  booktitle = {Computer Aided Verification},
  title     = {Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks},
  year      = {2017},
  address   = {Cham},
  editor    = {Majumdar, Rupak and Kun{\v{c}}ak, Viktor},
  pages     = {97--117},
  publisher = {Springer International Publishing},
  abstract  = {Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.},
  isbn      = {978-3-319-63387-9},
}

@Comment{jabref-meta: databaseType:bibtex;}
