\documentclass[]{article}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newcommand{\ReLU}{\mathrm{ReLU}}



\title{Proof No Diamond (3 layers)}
\date{}

\begin{document}

\maketitle

\begin{definition}
	A pair of paths $(\pi,\pi')$
	is called {\em compensating} if they start in the same neuron $a$ and 
	ends in the same neuron $z$, and the product of weights over $\pi$ is strictly positive and the product of weights over $\pi'$ is strictly negative.
\end{definition}

Intuitively, compensating paths will partially cancel out each other as they conribute the same weight $w(a)$ to the weight of the same neuron $w(z)$, but with opposite sign. 
It is not simple to take this compensation into account because of ReLUs: the particular compensation will depend upon the weight of intermediate nodes seen along $\pi$ and $\pi'$, 
as when one of this node gets negative input, it will clip it to 0.


We assume all active functions are ReLU function.

\begin{definition}

	\begin{itemize}
	 \item  We use $a_i$ to denote nodes in the input layer, use $b_j$ to denote nodes in the first hidden layer, etc and finally use $z_i$ to denote the output nodes.
	For $L,L'$ consecutive layers, for $x \in L$ and $y \in L'$, we denote 
	$w({x y})$ to denote the fixed weight from $x$ to $y$, while we use
	$weight_Y(x)$ to denote the weights of node $x$ from initial configuration $Y$. 
	We use $bias$ to denote bias weightt.
	
	\item We use $b^-$ to denote the node before ReLU and $b^+$ to denote the node after ReLU: $b^+ = \ReLU(b^-)$.
	\end{itemize}
	
%	3. $\bar{f}$ is the upper bound approximation function of DeepPoly, and $\underline{f}$ is the lower bound approximation function.
\end{definition}


\section{Theorem 1}



\begin{theorem}
	\label{th1}
	If for all $(x,y)$ there is no compensation paths $(\pi,\pi')$ 
	in the network from $x$ to $y$, then the LP approximation is 
	$100\%$ accurate (or deepoly with the abstraction $\underline{f}(x) = 0$ for the lower bound, never using the $\underline{f}(x) = x$ abstraction). 
\end{theorem}

\subsection*{Proof of Theorem \ref{th1}}

First, notice that if $weight_Y(b^-)$ is maximal (resp. minimal), 
then $weight_Y(b^+=\ReLU(b^-))$ also gets maximal (resp. minimal).
The following definition is the most important concept in this proof.

\begin{definition}[Sign of Node]\label{sign_of_nodes}
	We define a sign function $S$ on nodes $n$ such that: 	(1). $S(n)=0$ if all path from $n$ to $c$ has 0 weight; (2). $S(n)=1$ if all path from $n$ to $c$ has non-negative weight, and at least one path has a positive weight; (3). $S(n)=-1$ if all path from $n$ to $c$ has non-positive weight, and at least one path has a negative weight. 
	In general, $S$ may not be defined on every node (e.g. if there is a negative and a positive path from $a$ to $c$). However, if there is no Diamond, $S$ is defined on all nodes: any node fulfills one of above cases (1),(2),(3).


	For instance, for an input node $a_i$ of any 3 layer DNN:
	\begin{enumerate}
      \item  $S(a_i)=0$ if 
	  for all $b_j, w(a_i b_j)\cdot w(b_j c) = 0$
	
	
	 \item  $S(a_i)=1$ if for all $b_j, w(a_i b_j)\cdot w(b_j c) \geq 0$ and there exists 
	 $j, w(a_i b_j)\cdot w(b_j c) > 0$
	
	\item $S(a_i)=-1$ if for all $b_j\ w(a_i b_j)\cdot w(b_j c) \leq 0$ and there exists 
	$j, w(a_i b_j)\cdot w(b_j c) < 0$ 
\end{enumerate}

For $b_j$ in the hidden layer, we have $S(b_j)=1,-1,0$ if $w(b_j c)$ is positive, negative, or 0 respectively. Finally, for the output node $c$, we define $S(c)=1$.
\end{definition}


\begin{lemma}[Sign]
	\label{lemma1}
Let $L,L'$  are consecutive layers be of a DNN without compensation. 
Then if both 
$w(m n) \neq 0$ and $S(n) \neq 0$, then 
	$S(m)=S(n)\mathrm{Sign}(w(n m))$.
\end{lemma}

\begin{proof}
	If $S(n) \neq 0$, then there is a path $\pi$ from $n$ to the output node $c$ with a nonzero weight of the same sign as $S(n)$. 
	
	Hence there is a non zero path from $m$ to $c$: $(m n) \pi$, which is of sign 
	$S(n)\mathrm{Sign}(w(mn))$. As there is no compensation, $S(m)=S(n)\mathrm{Sign}(w(mn))$.
\end{proof}


For a node $n$, we use $n_s$ to denote $S(n)\cdot n$. 
Notice that for $S(n)=1$, $n_s$ gets maximal value whenever $n$ gets maximal value; 
while for $S(n)=-1$, $n_s$ gets maximal value whenever $n$ gets minimal value (and vice versa). For $S(n)=0$, $n_s=0$ and thus always reach his minimal and maximal.



\begin{lemma}
	\label{lemma2}
	Let $L,L'$ be consecutive layers of a compensation free DNN, and $n \in L'$. 
	Then:
		$$ \max(n^-_s)=\sum_{m \in L}w(m n) \max(m^+_s)+bias_n \text{ and }$$
		$$\max(n^+_s)=ReLU(\sum_{m \in L}w(m n) max(m^+_s)+bias_n)$$
		
	
	
	Similarly  for minimal value,	
	$ \min(n^-_s)=\sum_{m \in L}w(m n)\min(m^+_s)+bias_n $ and
	$ \min(n^+_s)=ReLU(\sum_{m \in L}w(m n)\min(m^+_s)+bias_n)$
\end{lemma}

\begin{proof}
We choose $\bar{Y}$ maximizing each $a_s \in \{a,-a\}$ in the input layer $L=L_0$,
	that is setting $a$ as $max(a)$ for $S(a)=1$ and $min(a)$ for $S(a)=-1$.
At configuration $\bar{Y}$, every $a_s(\bar{Y}))=\max_Y {a_s(Y))}=max(a_s)$ is maximized.  

Consider any $b$ in the next layer $L_1$.
Assume first that $S(b)=+1$.
We have $weight(b_s)= weight(b) = \sum_{a \in L_0} w(a b) weight(a) + bias_b$.
If every $w(a b) weight(a)$ is maximized in the same time, 
then $weight(b_s)= weight(b)$ is also maximized. 

We claim that this is the case at 
configuration $\bar{Y}$.
Indeed, if $w(a b)=0$, any $weight(a)$ can be chosen to maximize 
$w(a b) weight(a)$. If $w(a b)>0$ then $S(a) = +1$ by Lemma \ref{lemma1}, since
$S(b)=+1$. We have $max(a_s) = max(a)$, maximizing $w(a b) weight(a)$.
Finally, if $w(a b)<0$ then $S(a_s) = -1$, and we have 
$max(a_s) = -min(a)$, maximizing $w(a b) weight(a)$.

Hence $b$ reaches is maximum in configuration $\bar{Y}$, and the value satisfies: 
$$\max(b)=\sum_{m \in L}w(a b) \max(a_s)+bias_b$$

The case $S(b)=-1$ is symetric:
$\max(b_s)= -\min(b)$ is reached when minimizing $b$, which is also 
satisfied at $\bar{Y}$.
In both case, $weight(b_s)$ is maximized at $\bar{Y}$. Notice that $\bar{Y}$ does not depend upon $b$, so choosing $\bar{Y}$ uniformly maximize all $weight(b_s)$.

This implies that $ReLU(b^-_s)$ is maximized at $\bar{Y}$, and its value is 
$ReLU(\sum_{m \in L}w(a b) \max(a_s)+bias_b)$, again uniformly reached over all $b$ at configuration $\bar{Y}$.

We proceed by induction over every layer of the DNN till the output layer, proving that 
$max(z_s)=max(z)$ is reached at configuration $\bar{Y}$.

The case for $min(x_s)$ is similar, reached at configuration $\underline{Y}$.
\end{proof}

As direct corollary of Lemma \ref{lemma2}, we obtain:

\begin{corollary}
	\label{cor1}
	$$\max(n^-)=\sum_{m \in L, w(m n)>0}w(m n) \max(m^+) + \sum_{m \in L, w(m n)<0}w(m n) \min(m^+) + bias_n$$
	$$\min(n^-)=\sum_{m \in L, w(m n)>0}w(m n) \min(m^+) + \sum_{m \in L, w(m n)<0}w(m n) \max(m^+) + bias_n$$
\end{corollary}
	

Notice that for all $x$, either $S(x)=+1$ and 
$(\min(x),\max(x))=(\min(x_s),\max(x_s))$, 
or $S(x)=-1$ and $(\min(x),\max(x))=(-\max(x_s),-\min(x_s))$.
In both case, these two bounds are found in Lemma \ref{lemma2},
from configuration $\bar{Y}$ and $\underline{Y}$.
We now show that DeepPoly will generate the same bounds 
$\underline{f}(x)=\min(x)$ and $\bar{f}(x)=\max(x)$.

\paragraph{DeepPoly bounds}

To prove that DeepPoly bounds are the same as the exact bounds computed above, 
we show that even the box abstraction (which is easier) would reach the exact bounds.
We proceed inductively, and prove the inductive step.
The initialization is obvious as Box Abstraction/DeepPoly is always exact for the initialization layer.

\begin{lemma}
	Let $L,L'$ be two consecutive layers.
	Assume for all node $m$ of $L$ that the lower and upper bounds for $m$ used by box abstraction equals to the exact lower and upper bounds of $m$ (as expressed above), i.e.
	$\bar{f}(m)=max(m)$ and $\underline{f}(m)=min(m)$.
	
	Then Box abstraction computes the exact lower and upper bounds for every node $n$ of layer $L'$, ie $\bar{f}(n)=max(n)$ and $\underline{f}(n)=min(n)$.
\end{lemma}

\begin{proof}
	Suppose $n$ is a node in $L'$ such that $S(n)\neq 0$ (otherwise, it is trivial). 
	First assume $S(n)=1$. The case $S(n)=-1$ is similar. 

	The Box absraction computes its upper bound using:
	$$\bar{f}(n^-)= \sum_{w(mn)>0} w(mn) \bar{f}(m^+) + \sum_{w(mn)<0} w(mn) \underline{f}(m^+) + bias_n$$

	By induction hypothesis, we have 
	$\bar{f}(m^+)=max(m^+)$ and
	$\underline{f}(m^+)=min(m^+)$, thus 
	applying Corollary \ref{cor1}, we obtain
	$\bar{f}(n^-)=max(n^-)$ and 
	$\underline{f}(n^-)=min(n^-)$.

	Now, if $\underline{f}(n^-)=min(n^-)<0$, 
	then $\underline{f}(n^+)=min(n^+)=0$, 
	and otherwise 
	$\underline{f}(n^+)=min(n^+)=\underline{f}(n^-)=min(n^-)$.

	Similarly, 
	if $\bar{f}(n^-)=max(n^-)<0$, 
	then $\bar{f}(n^+)=max(n^+)=0$, 
	and otherwise 
	$\bar{f}(n^+)=max(n^+)=\bar{f}(n^-)=bar(n^-)$.
	
	Hence we have equality before and after ReLU in all cases.
\end{proof}


\section{Theorem 2}

We now show that if all intermediates nodes that are on compensating pairs are opened as MILP nodes, then MILP will be correct. First, we do the proof with 3 layers, assuming (without loss of generality) a unique output node $z$.

\begin{theorem}
	\label{no_diamond_2}
	Using MILP method, if every nodes $b_j$ in any compensating path pair
	 $(\pi,\pi')$ is encoded as a binary/integer variable, then the upper and lower 
	 bounds computed by MILP are the exact max and min value of $z$.
\end{theorem}

By symetry, we only show the max side. 



\begin{definition}
	Let $K$ be the set of all input nodes $a_k$. 
	We define a decomposition $K=I\sqcup J$ as follows:  
	\begin{itemize}
 \item $k \in I$  if
 \begin{enumerate}
	 \item every path from $a_k$ to $z$ has weight $\geq 0$, or
	\item every path from $a_k$ to $z$ has weight $\leq 0$.
 \end{enumerate}
That is, $a_k$ is not a source of a compensating pair.
	\item $k \in J$ if $k \notin I$, that is there exists two paths $\pi,\pi'$ from $a_k$, 
	one with positive and one with negative weight.
\end{itemize}
\end{definition} 


\begin{lemma} \label{lem:open_node}
	A node $b$ in the hidden layer will not be on a compensating pair iff one of the following two happens:
	\begin{enumerate}
	 \item $w_{b,z}=0$, or
	 \item For every input node $j\in J$, we have $w_{a_j,b}=0$.
	\end{enumerate}
	We denote $B_{pure}$ the set of such nodes $b$ such that at least one of the above holds.
\end{lemma}

\begin{proof}
	First, we show that if either one of 1,2 happens, then $b_i$ will not be opened. If 1), it is obvious. For 2, we reason by contradiction: assume there is a pair of compensating paths 	$(\pi,\pi')$ starting with $a$, with $k$ in $\pi$ and weight$(\pi) > 0$. It means that $a \in J$. A contradiction as 2) $w_{a_j,b}=0$ implies weight$(\pi)=0$.
	
	Second, we show that if neither 1 nor 2 hold, then $b$ will be on a compensating path.
	Because 2) does not hold, there is a $j \in J$ with $w_{a_j,b} \neq 0$, say $>0$.
	Because 1) does not hold, $w_{b,z} \neq 0$, say $>0$.
	Now, by definition of $J$, there is a pair of compensating paths $\pi,\pi'$ 
	from $a_j$ to $z$, say with $\pi'$ with weight $<0$.
	Then $((a_j,b,z), \pi')$ is also a compensating pair.
\end{proof}

Consider the sign of nodes function from Definition \ref{sign_of_nodes}. Because now we allow compensating paths, we cannot define this function over all nodes.


\begin{definition}\label{sign_of_nodes_in_I}
	We define a partial sign function $S$ over nodes $n$ such that : 	
	\begin{enumerate} 
		 \item all paths from $n$ to $z$ have 0 weight, and then $S(n)=0$; 
		 \item all paths from $n$ to $z$ have non-negative weight, and at least one path has a positive weight, and then $S(n)=1$; 
		 \item all path from $n$ to $z$ has non-positive weight, and at least one path has a negative weight, and then $S(n)=-1$.
		 \item Otherwise $S(n)$ is undefined.
	\end{enumerate}
\end{definition}	
	
Notice that $S$ is defined on all nodes $a_i$ with $i \in I$ (it can also be defined trivially for all nodes $b$ as there is a unique path to $z$, and at $z$ also). Hence it is undefined only for nodes $a_j$ with $j \in J$.


We denote $a_S$ for any subset $S\subseteq K$ to refer the input vector $\langle a_k\rangle_{a_k\in S}$. We also denote $a_I\oplus a_J = a_K$ and $z=z(a_K)=z(a_I,a_J)$.
Consider $a_I^*$ the input vector such that for all $i \in I$, the value for $a_i$ gets its maximal value (if $S(a_i)=1$) or $a_i$ gets its minimal value (if $S(a_i)=-1$).

\begin{lemma} \label{lem:reach_max}
	$max_a (weight_{a}(z)) = max_{\{a \mid a_I=a^*_I\}} (weight_{a}(z))$
	
	Further, for every intermediate node $b$ in the hidden layer, for any valuation $a^0_J$, 
	we have $max_{\{a \mid a_J=a^0_J\}} (weight_{a}(b)) = max_{\{a \mid a_J=a^0_J,a_I=a_I^*\}} (weight_{a}(b))$.	
\end{lemma}

\begin{proof}
	For the first statement, we use 
	$$max_{a_K} (weight_{a_K}(z)) = max_{a_J} max_{a_I} (weight_{a_I,a_J}(z))$$
	
	Now, for any fixed input $a^0_J$, we can regard $weight_{a_I,a^0_J}(z)$ as a DNN $D'$ with input nodes $a_i\in I$, with all $a^0_J$ %and their propagation in hidden layers 
	as bias. In the simplified $D'$, there is no compensating path because of the definition of $I$. Therefore we can apply Theorem \ref{th1} (make a more precise corollary) to get that $z(\cdot,a^0_J)$ reaches its maximal value for $a_I=a_I^*$, and we are done.
	
	The second statement is simpler because for each fixed partial input $a^0_J$, 
	$$b= B^0 +\sum_{a_i\in I} w_{a_i, b} a_i,$$ where $B^0$ is a constant which is the sum of the term including bias and $a^0_J$. Applying Lemma \ref{lemma1}, we obtain the statement.
\end{proof}

\subsection{MILP abstraction}

Consider now an MILP abstraction using the abstraction from \cite{MILP}, 
where each variable $\alpha_b$ for the ReLU from $b^-$ to $b^+$ is:
\begin{itemize}
	\item linear  (i.e. $\alpha_b \in [0,1]$) for $b \in B_{pure}$
	\item binary/integer (i.e. $\alpha_b \in \{0,1\}$) for $b \notin B_{pure}$
\end{itemize}

Notice that there is only one layer of ReLUs, in the hidden layer.
Denote by $\mathrm{UB}$ the maximal bound on $z$ considering the MILP constraints above, with ReLUs being either linear or binary depending on whether $b \in B_{pure}$ or not.

We want to show that $\mathrm{UB} = \max z$. As the MILP abstraction is a sound overapproximation, 
it suffices to show that $\mathrm{UB}\leq \max c$.

For any $b_h\in B_{pure}$, we have $b_h=\sum_{a_i\in I}w_{b_ha_i}+B_h$ ($B_h$ is the bias). 
Denote by $B_O$ the set of nodes $b$ for which $\alpha_b$ is binary, i.e. 
$b \notin B_{pure}$. %For any subdomain $D \subseteq B_O$, 
Let $a^0_J$ and $a^0_I$ be fixed inputs, and consider the associated upper bound 
$\mathrm{UB}_{a^0_J,a^0_I}$. We have:
%We show that this upper bound does not exceed the maximal value of $z$: 

\begin{align*}
	\mathrm{UB}_{a^0_J,a^0_I} = B_z + \sum_{b\in B_O} w_{b,z}\ReLU(weight_{a^0_I,a^0_J}(b)) + \sum_{b\in B_{pure}} w_{b,z} \mathrm{AppB}(weight_{a^0_I,a^0_J}(b)).
\end{align*} 

where $\mathrm{AppB}(weight_{a^0_I,a^0_J}(b))$ is the upper bound of 
the (LP) approximation of $weight_{a^0_I,a^0_J}(b)$ if $w_{b,z}>0$, and the lower bound
of its (LP) approximation if $w_{b,z}<0$. 

By Lemma \ref{lem:reach_max}, for any fixed input $a^0_J$, for all nodes $b$ in the hidden layer, if $S(b)=\mathrm{sign}(w_{zb})=1$, then 
$weight_{a_I,a^0_J}(b)$ will get its maximal value for $a_I=a_I^*$,
and if $S(b)=\mathrm{sign}(w_{zb})=-1$, then $weight_{a^0_I,a^0_J}(b)$ will get its minimal value for $a_I=a_I^*$.
Notice that $\mathrm{ReLU}$ and $\mathrm{AppB}$ are both non decreasing functions, and thus:

\begin{align*}
	\mathrm{UB}_{a^0_J,a^0_I} \leq B_z + \sum_{b\in B_O} w_{b,z}\ReLU(weight_{a^*_I,a^0_J}(b)) +
	\sum_{b\in B_{pure}} w_{b,z} \mathrm{AppB}(weight_{a^*_I,a^0_J}(b)).
\end{align*} 

Added FIX:
Consider $B_{pure}=B^1 \cup B^2$, with $b \in B^1$ iff
$w_{b,z} = 0$ and $b \in B^2$ iff $w_{b,z} \neq 0$.
Hence:

$$\sum_{b\in B_{pure}} w_{b,z} \mathrm{AppB}(weight_{a^*_I,a^0_J}(b)) = 
\sum_{b\in B^2} w_{b,z} \mathrm{AppB}(weight_{a^*_I,a^0_J}(b))$$

For $b \in B^2$, we have $w_{a_j,b}=0$ for all $j \in J$
by definition of $B_{pure}$ (Lemma 4.), hence

$$\sum_{b\in B^2} w_{b,z} \mathrm{AppB}(weight_{a^*_I,a^0_J}(b)) = 
\sum_{b\in B^2} w_{b,z} \mathrm{AppB}(weight_{a^*_I}(b))
=\sum_{b\in B^2} w_{b,z} \mathrm{ReLU}(weight_{a^*_I}(b))
$$

as $\mathrm{AppB}(weight_{a^*_I}(b))=\mathrm{ReLU}(weight_{a^*_I}(b))$ since 
$weight_{a^*_I}(b)$ is an extremal value (minimum or maximal) and 
$\mathrm{AppB}$ equals $\mathrm{ReLU}$ for extremal values (make a lemma about that in the begining).

Hence we have: \begin{align*}
	\mathrm{UB}_{a^0_J,a^0_I}  \leq & B_z+\sum_{b}w_{zb}\ReLU(b(a_I^*,\bar{a}_J))
	= \max_{a_I} z(a_I,\bar{a}_J) \\
	\leq & \max_{a_J}\max_{a_I} z(a_I,a_J) 	= \max_{a_K} weight_{a_K}(z)
\end{align*}

Considering the maximal over all inputs $a_J,a_I$, we obtain 

$$\mathrm{UB} = max_{a_J,a_I} \mathrm{UB}_{a_J,a_I} \leq \max_{a_J,a_I} weight_{a_J,a_I}(z)$$




\end{document}












%
%The DeepPoly function for $b$ can be expressed by the following steps.
%First, we partition the set of indices $i$ as $I,I_0,I^+,I^-$ with:
%\begin{itemize}
%	\item $i \in I$ if the sign of $b_i^-=\sum_{j} w_{a_j b_i} a_j + B_{b_i}$ is always positive irrespective of $Y=(a_j)_j$, in which case 
%	$\bar{f}(b_i^+)(Y)=\underline{f}(b_i^+)=\sum_{j} w_{a_j b_i} a_j + B_{b_i}$, and else:
%	\item $i \in I_0$ if the sign of $b_i^-=\sum_{j} w_{a_j b_i} a_j + B_{b_i}$ is always negative irrespective of $Y=(a_j)_j$, in which case $\bar{f}(b_i^+)=\underline{f}(b_i^+)=0$,	and else:
%	\item $i \in I^+$ if $w_{b_i c}>0$, and:
%	\item $i \in I^-$ if $w_{b_i c}<0$.
%	\end{itemize}
%	
%Remark that for all $i \notin I$, we have $\underline{f_i}(b_i^+)(Y)=0$.
%	
%
%We have:
%\begin{align*}
%	\bar{f}(c)(Y) = &
%	bias_c + \sum_{b \in I} w_{bc}\sum_{a} w_{a b} weight(a) + bias_{b} \\
%	& + \sum_{b \in I^+} w_{b c} \cdot \bar{f}(ReLU(\sum_{a} w_{a b} weight(a) 
%	+ Bias_{b}))(Y) 
%	\end{align*}

