\documentclass[]{article}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newcommand{\ReLU}{\mathrm{ReLU}}



\title{Proof No Diamond (3 layers)}
\date{}

\begin{document}

\maketitle

\begin{definition}
	A pair of paths $(\pi,\pi')$
	is called {\em compensating} if they start in the same neuron $a$ and 
	ends in the same neuron $z$, and the product of weights over $\pi$ is strictly positive and the product of weights over $\pi'$ is strictly negative.
\end{definition}

Intuitively, compensating paths will partially cancel out each other as they conribute the same weight $w(a)$ to the weight of the same neuron $w(z)$, but with opposite sign. 
It is not simple to take this compensation into account because of ReLUs: the particular compensation will depend upon the weight of intermediate nodes seen along $\pi$ and $\pi'$, 
as when one of this node gets negative input, it will clip it to 0.


We assume all active functions are ReLU function.

\begin{definition}

	\begin{itemize}
	 \item  We use $a_i$ to denote nodes in the input layer, use $b_j$ to denote nodes in the first hidden layer, etc and finally use $z_i$ to denote the output nodes.
	For $L,L'$ consecutive layers, for $x \in L$ and $y \in L'$, we denote 
	$w({x y})$ to denote the fixed weight from $x$ to $y$, while we use
	$weight_Y(x)$ to denote the weights of node $x$ from initial configuration $Y$. 
	We use $bias$ to denote bias weightt.
	
	\item We use $b^-$ to denote the node before ReLU and $b^+$ to denote the node after ReLU: $b^+ = \ReLU(b^-)$.
	\end{itemize}
	
%	3. $\bar{f}$ is the upper bound approximation function of DeepPoly, and $\underline{f}$ is the lower bound approximation function.
\end{definition}





\begin{theorem}
	\label{th1}
	If for all $(x,y)$ there is no compensation paths $(\pi,\pi')$ 
	in the network from $x$ to $y$, then the LP approximation is 
	$100\%$ accurate (or deepoly with the abstraction $\underline{f}(x) = 0$ for the lower bound, never using the $\underline{f}(x) = x$ abstraction). 
\end{theorem}

\subsection*{Proof of Theorem \ref{th1}}

First, notice that if $weight_Y(b^-)$ is maximal (resp. minimal), 
then $weight_Y(b^+=\ReLU(b^-))$ also gets maximal (resp. minimal).
The following definition is the most important concept in this proof.

\begin{definition}[Sign of Node]\label{sign_of_nodes}
	We define a sign function $S$ on nodes $n$ such that: 	(1). $S(n)=0$ if all path from $n$ to $c$ has 0 weight; (2). $S(n)=1$ if all path from $n$ to $c$ has non-negative weight, and at least one path has a positive weight; (3). $S(n)=-1$ if all path from $n$ to $c$ has non-positive weight, and at least one path has a negative weight. 
	In general, $S$ may not be defined on every node (e.g. if there is a negative and a positive path from $a$ to $c$). However, if there is no Diamond, $S$ is defined on all nodes: any node fulfills one of above cases (1),(2),(3).


	For instance, for an input node $a_i$ of any 3 layer DNN:
	\begin{enumerate}
      \item  $S(a_i)=0$ if 
	  for all $b_j, w(a_i b_j)\cdot w(b_j c) = 0$
	
	
	 \item  $S(a_i)=1$ if for all $b_j, w(a_i b_j)\cdot w(b_j c) \geq 0$ and there exists 
	 $j, w(a_i b_j)\cdot w(b_j c) > 0$
	
	\item $S(a_i)=-1$ if for all $b_j\ w(a_i b_j)\cdot w(b_j c) \leq 0$ and there exists 
	$j, w(a_i b_j)\cdot w(b_j c) < 0$ 
\end{enumerate}

For $b_j$ in the hidden layer, we have $S(b_j)=1,-1,0$ if $w(b_j c)$ is positive, negative, or 0 respectively. Finally, for the output node $c$, we define $S(c)=1$.
\end{definition}


\begin{lemma}[Sign]
	\label{lemma1}
Let $L,L'$  are consecutive layers be of a DNN without compensation. 
Then if both 
$w(m n) \neq 0$ and $S(n) \neq 0$, then 
	$S(m)=S(n)\mathrm{Sign}(w(n m))$.
\end{lemma}

\begin{proof}
	If $S(n) \neq 0$, then there is a path $\pi$ from $n$ to the output node $c$ with a nonzero weight of the same sign as $S(n)$. 
	
	Hence there is a non zero path from $m$ to $c$: $(m n) \pi$, which is of sign 
	$S(n)\mathrm{Sign}(w(mn))$. As there is no compensation, $S(m)=S(n)\mathrm{Sign}(w(mn))$.
\end{proof}


For a node $n$, we use $n_s$ to denote $S(n)\cdot n$. 
Notice that for $S(n)=1$, $n_s$ gets maximal value whenever $n$ gets maximal value; 
while for $S(n)=-1$, $n_s$ gets maximal value whenever $n$ gets minimal value (and vice versa). For $S(n)=0$, $n_s=0$ and thus always reach his minimal and maximal.



\begin{lemma}
	\label{lemma2}
	Let $L,L'$ be consecutive layers of a compensation free DNN, and $n \in L'$. 
	Then:
		$$ \max(n^-_s)=\sum_{m \in L}w(m n) \max(m^+_s)+bias_n \text{ and }$$
		$$\max(n^+_s)=ReLU(\sum_{m \in L}w(m n) max(m^+_s)+bias_n)$$
		
	
	
	Similarly  for minimal value,	
	$ \min(n^-_s)=\sum_{m \in L}w(m n)\min(m^+_s)+bias_n $ and
	$ \min(n^+_s)=ReLU(\sum_{m \in L}w(m n)\min(m^+_s)+bias_n)$
\end{lemma}

\begin{proof}
We choose $\bar{Y}$ maximizing each $a_s \in \{a,-a\}$ in the input layer $L=L_0$,
	that is setting $a$ as $max(a)$ for $S(a)=1$ and $min(a)$ for $S(a)=-1$.
At configuration $\bar{Y}$, every $a_s(\bar{Y}))=\max_Y {a_s(Y))}=max(a_s)$ is maximized.  

Consider any $b$ in the next layer $L_1$.
Assume first that $S(b)=+1$.
We have $weight(b_s)= weight(b) = \sum_{a \in L_0} w(a b) weight(a) + bias_b$.
If every $w(a b) weight(a)$ is maximized in the same time, 
then $weight(b_s)= weight(b)$ is also maximized. 

We claim that this is the case at 
configuration $\bar{Y}$.
Indeed, if $w(a b)=0$, any $weight(a)$ can be chosen to maximize 
$w(a b) weight(a)$. If $w(a b)>0$ then $S(a) = +1$ by Lemma \ref{lemma1}, since
$S(b)=+1$. We have $max(a_s) = max(a)$, maximizing $w(a b) weight(a)$.
Finally, if $w(a b)<0$ then $S(a_s) = -1$, and we have 
$max(a_s) = -min(a)$, maximizing $w(a b) weight(a)$.

Hence $b$ reaches is maximum in configuration $\bar{Y}$, and the value satisfies: 
$$\max(b)=\sum_{m \in L}w(a b) \max(a_s)+bias_b$$

The case $S(b)=-1$ is symetric:
$\max(b_s)= -\min(b)$ is reached when minimizing $b$, which is also 
satisfied at $\bar{Y}$.
In both case, $weight(b_s)$ is maximized at $\bar{Y}$. Notice that $\bar{Y}$ does not depend upon $b$, so choosing $\bar{Y}$ uniformly maximize all $weight(b_s)$.

This implies that $ReLU(b^-_s)$ is maximized at $\bar{Y}$, and its value is 
$ReLU(\sum_{m \in L}w(a b) \max(a_s)+bias_b)$, again uniformly reached over all $b$ at configuration $\bar{Y}$.

We proceed by induction over every layer of the DNN till the output layer, proving that 
$max(z_s)=max(z)$ is reached at configuration $\bar{Y}$.

The case for $min(x_s)$ is similar, reached at configuration $\underline{Y}$.
\end{proof}

As direct corrolary of Lemma \ref{lemma2}, we obtain:

\begin{corollary}
	\label{cor1}
	$$\max(n^-)=\sum_{m \in L, w(m n)>0}w(m n) \max(m^+) + \sum_{m \in L, w(m n)<0}w(m n) \min(m^+) + bias_n$$
	$$\min(n^-)=\sum_{m \in L, w(m n)>0}w(m n) \min(m^+) + \sum_{m \in L, w(m n)<0}w(m n) \max(m^+) + bias_n$$
\end{corollary}
	

Notice that for all $x$, either $S(x)=+1$ and 
$(\min(x),\max(x))=(\min(x_s),\max(x_s))$, 
or $S(x)=-1$ and $(\min(x),\max(x))=(-\max(x_s),-\min(x_s))$.
In both case, these two bounds are found in Lemma \ref{lemma2},
from configuration $\bar{Y}$ and $\underline{Y}$.
We now show that DeepPoly will generate the same bounds 
$\underline{f}(x)=\min(x)$ and $\bar{f}(x)=\max(x)$.

\paragraph{DeepPoly bounds}

To prove that DeepPoly bounds are the same as the exact bounds computed above, 
we show that even the box abstraction (which is easier) would reach the exact bounds.
We proceed inductively, and prove the inductive step.
The initialization is obvious as Box Abstraction/DeepPoly is always exact for the initialization layer.

\begin{lemma}
	Let $L,L'$ be two consecutive layers.
	Assume for all node $m$ of $L$ that the lower and upper bounds for $m$ used by box abstraction equals to the exact lower and upper bounds of $m$ (as expressed above), i.e.
	$\bar{f}(m)=max(m)$ and $\underline{f}(m)=min(m)$.
	
	Then Box abstraction computes the exact lower and upper bounds for every node $n$ of layer $L'$, ie $\bar{f}(n)=max(n)$ and $\underline{f}(n)=min(n)$.
\end{lemma}

\begin{proof}
	Suppose $n$ is a node in $L'$ such that $S(n)\neq 0$ (otherwise, it is trivial). 
	First assume $S(n)=1$. The case $S(n)=-1$ is similar. 

	The Box absraction computes its upper bound using:
	$$\bar{f}(n^-)= \sum_{w(mn)>0} w(mn) \bar{f}(m^+) + \sum_{w(mn)<0} w(mn) \underline{f}(m^+) + bias_n$$

	By induction hypothesis, we have 
	$\bar{f}(m^+)=max(m^+)$ and
	$\underline{f}(m^+)=min(m^+)$, thus 
	applying Corollary \ref{cor1}, we obtain
	$\bar{f}(n^-)=max(n^-)$ and 
	$\underline{f}(n^-)=min(n^-)$.

	Now, if $\underline{f}(n^-)=min(n^-)<0$, 
	then $\underline{f}(n^+)=min(n^+)=0$, 
	and otherwise 
	$\underline{f}(n^+)=min(n^+)=\underline{f}(n^-)=min(n^-)$.

	Similarly, 
	if $\bar{f}(n^-)=max(n^-)<0$, 
	then $\bar{f}(n^+)=max(n^+)=0$, 
	and otherwise 
	$\bar{f}(n^+)=max(n^+)=\bar{f}(n^-)=bar(n^-)$.
	
	Hence we have equality before and after ReLU in all cases.
\end{proof}


	\section{No Diamond theorem part 2}

Now we try to show the part 2 of No Diamond theorem. Again, we begin with 3 layers with one output node $z$.

\begin{theorem}[No Diamond theorem, part 2] \label{no_diamond_2}
	Using MILP method, if all nodes $b_j$ such that there is a compensating path pair $\pi,\pi'$ with $\pi = a_k,b_j,z$ are opened, then the upper and lower bounds computed by MILP are the exact max and min value of $z$.
\end{theorem}

With out loss of generality, we only need to show the max side. The min side is almost the same.



\begin{definition}
	Let $K$ be the set of all input nodes $a_k$. We define a decomposition $K=I\sqcup J$ as follows:  
	
	An input node $a_k$ is in $I$ if for any two paths $\pi,\pi'$ that begin with $a_k$ (end with $z$), they are not compensating, i.e., the weights of them are both non negative or both non positive; hence all path $\pi$ that begin with $a_k$ (end with $z$) are non negative (or non positive).
	
	An input node $a_k$ is in $J$ if it is not in $I$. We use $a_i$ to denote input nodes in $I$ and use $a_j$ to denote input nodes in $J$.
\end{definition} 


\begin{lemma} \label{lem:open_node}
	A node $b_i$ in the hidden layer will not be opened in Theorem \ref{no_diamond_2} iff one of the following two happen:
	
	1. $w_{zb_i}=0$.
	
	2. For any input nodes $a_j\in J$, $w_{b_ia_j}=0$.
\end{lemma}

\begin{proof}
	First, we show if one of 1,2 happens, then $b_i$ will not be opened. If $w_{zb_i}=0$, it is obvious.
	
	If 2 happens, suppose $\pi,\pi'$ is a pair of compensating path, then $\pi = a_k,b_i,z$. Because this is a compensating path, $w_{b_ia_k}\neq 0$. But because of 2, this means that $a_k$ is in $I$, and hence there is a compensating path begin with $a_k\in I$, a contradiction. Therefore, $b_i$ cannot be a node in a compensating pair.
	
	Second, we show if both 1,2 not happen, then $b_i$ will be opened. Because of negation 2, there is a node $a_j$ in $J$ such that $w_{a_jb_i}\neq 0$, and since $w_{zb_i}\neq 0$, $\pi=a_j,b_i,z$ is a path with a  nonzero weight. By symmetry, we assume, $w_{zb_i}w_{b_ia_j}>0$. 
	
	Now, since $a_j$ is in $J$, there are two paths $\pi',\pi''$ that begin with $a_j$ but $\pi''$ has a positive weight and $\pi'$ has a negative weight. Consider $\pi'$. We claim that $\pi,\pi'$ is a compensating pair that make $b_i$ opened. $\pi'$ cannot be $a_j,b_i,z=\pi$ because $\pi$ has a positive weight. So $\pi,\pi'$ are two different paths with opposite signs of values.
	
	So, $b_i$ will be opened. Now we prove this lemma. 
\end{proof}


Now we need the function in Definition \ref{sign_of_nodes}. The difference is that now it is a partial function.

\begin{definition}\label{sign_of_nodes_in_I}
	We define a partial sign function all nodes such that: 	(1). $S(n)=0$ if all path from $n$ to $z$ has 0 weight; (2). $S(n)=1$ if all path from $n$ to $z$ has non-negative weight, and at least one path has a positive weight; (3). $S(n)=-1$ if all path from $n$ to $z$ has non-positive weight, and at least one path has a negative weight. 
	
	
	For an input node $a_i$ in $I$:
	\begin{enumerate}
		\item  $S(a_i)=0$ if 
		for all $b_j, w(a_i b_j)\cdot w(b_j z) = 0$
		
		
		\item  $S(a_i)=1$ if for all $b_j, w(a_i b_j)\cdot w(b_j z) \geq 0$ and there exists 
		$j, w(a_i b_j)\cdot w(b_j z) > 0$
		
		\item $S(a_i)=-1$ if for all $b_j\ w(a_i b_j)\cdot w(b_j z) \leq 0$ and there exists 
		$j, w(a_i b_j)\cdot w(b_j z) < 0$ 
	\end{enumerate}
\end{definition}







Because for input nodes in $I$, they do not have compensating path pair, so the sign function is defined on
all nodes in $I$.



We use $a_S$ where $S\subseteq K$ to refer the input vector $\langle a_k\rangle_{a_k\in S}$. Then $a_I\oplus a_J = a_K$ and $z=z(a_K)=z(a_I,a_J)$.		Let $a_I^*$ be the input vector that for all $a_i$ in $I$, $a_i$ gets its maximal (if $S(a_i)=1$) or $a_i$ gets its minimal (if $S(a_i)=-1$).

\begin{lemma} \label{lem:reach_max}
	
	1. Node $z$ can get its maximal value if $a_I=a_I^*$
	
	2. Similarly, for every node $b$ in the hidden layer (notice that $S(b)$ is always defined since there is only one path from $b$ to $z$), for any fixed input of $a_J$,  $b(\cdot,a_J)$ will get its max/min (if $S(b)=1$ or $-1$) if $a_I=a_I^*$.
\end{lemma}

\begin{proof}
	
	
	
	1. This is because \begin{align*}
		\max_{a_K} z(a_K) = \max_{a_J}\max_{a_I} z(a_I,a_J)
	\end{align*}
	
	For any fixed input of $a_J$, $z(\cdot, a_J)$ can be regarded as a DNN $D'$ with input nodes $a_i\in I$ by regarding all $a_J$ and their propagation in hidden layers as bias. In the simplified $D'$, there is no compensating path because of the definition of $I$. Therefore we can apply the No Diamond theorem, part 1 to get that: (for fixed $a_J$) $z(\cdot,a_J)$ can get its maximal value if for all $a_i$ in $I$, $a_i$ gets its maximal if $S(a_i)=1$ or $a_i$ gets its minimal if $S(a_i)=-1$.
	
	If we use $\max(\tilde{a}_I$ to denote above values, then we have $$\max_{a_I}z(a_I,a_J)=z(\max(\tilde{a}_I),a_J).$$ And therefore, $\max z=\max_{a_J} z(\max\tilde{a}_I,a_J)$
	
	2. This is simpler because for each fixed input of $a_J$, $$b= B'+\sum_{a_i\in I} w_{ba_i}a_i,$$ where $B'$ is a constant which is the sum of rest terms, including bias and $a_J$. By Lemma \ref{lemma1}, we can easily see the claim of this lemma is true.
\end{proof}


Because for any  sound method, like MILP, the upper bound computed will not be smaller that the actual maximal value, so we only need to show, $\mathrm{UB}_{\mathrm{MILP}}\leq \max c$ in any case.

Suppose $O$ is the set of nodes opened. Then for any $b_h\notin O$, $b_h=\sum_{a_i\in I}w_{b_ha_i}+B_h$ ($B_h$ is the bias). Then for any subdomain $d$ in one of  $2^{|O|}$ subdomains, suppose we compute an upper bound when $a_J=\bar{a}_J$ and $a_I=\bar{a}_I$, we show that this upper bound does not exceed the maximal value of $z$. 

By the algorithm, \begin{align*}
	\mathrm{UB} = B_z+\sum_{b_h\in O}w_{zb_h}\ReLU(b_h(\bar{a}_I,\bar{a}_J))+\sum_{b_k\notin O}w_{zb_k}\mathrm{Approx}(b_k(\bar{a}_I,\bar{a}_J)).
\end{align*} Here, $\mathrm{Approx}$ is the approximation function of $\mathrm{ReLU}$. In our case (LP), it will equal to $\ReLU$ if $w_{zb_k}<0$, or a linear function if  if $w_{zb_k}>0$. $\mathrm{ReLU}$ and $\mathrm{Approx}$ are both non-decrease function.

Then by Lemma \ref{lem:reach_max}, for any fixed $a_J$, for all nodes $b$ in the hidden layer, $b$ will get its max/min (if $S(b)=\mathrm{sign}(w_{zb})=1,-1$) if $a_I=a_I^*$. Hence we have \begin{align*}
	\mathrm{UB} \leq  B_z+\sum_{b_h\in O}w_{zb_h}\ReLU(b_h(a_I^*,\bar{a}_J))+\sum_{b_k\notin O}w_{zb_k}\mathrm{Approx}(b_k(a_I^*,\bar{a}_J)).
\end{align*} Meanwhile, when $b_k$ reaches its max or min, we have $\mathrm{Approx}(b_k)=\ReLU(b_k)$, hence we have: \begin{align*}
	\mathrm{UB} \leq & B_z+\sum_{b}w_{zb}\ReLU(b(a_I^*,\bar{a}_J))\\
	= & \max_{a_I} z(a_I,\bar{a}_J)\\
	\leq & \max_{a_J}\max_{a_I} z(a_I,a_J)\\
	= & \max_{a_K} z(a_K) = \max z
\end{align*}



\end{document}












%
%The DeepPoly function for $b$ can be expressed by the following steps.
%First, we partition the set of indices $i$ as $I,I_0,I^+,I^-$ with:
%\begin{itemize}
%	\item $i \in I$ if the sign of $b_i^-=\sum_{j} w_{a_j b_i} a_j + B_{b_i}$ is always positive irrespective of $Y=(a_j)_j$, in which case 
%	$\bar{f}(b_i^+)(Y)=\underline{f}(b_i^+)=\sum_{j} w_{a_j b_i} a_j + B_{b_i}$, and else:
%	\item $i \in I_0$ if the sign of $b_i^-=\sum_{j} w_{a_j b_i} a_j + B_{b_i}$ is always negative irrespective of $Y=(a_j)_j$, in which case $\bar{f}(b_i^+)=\underline{f}(b_i^+)=0$,	and else:
%	\item $i \in I^+$ if $w_{b_i c}>0$, and:
%	\item $i \in I^-$ if $w_{b_i c}<0$.
%	\end{itemize}
%	
%Remark that for all $i \notin I$, we have $\underline{f_i}(b_i^+)(Y)=0$.
%	
%
%We have:
%\begin{align*}
%	\bar{f}(c)(Y) = &
%	bias_c + \sum_{b \in I} w_{bc}\sum_{a} w_{a b} weight(a) + bias_{b} \\
%	& + \sum_{b \in I^+} w_{b c} \cdot \bar{f}(ReLU(\sum_{a} w_{a b} weight(a) 
%	+ Bias_{b}))(Y) 
%	\end{align*}

