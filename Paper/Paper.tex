\documentclass[]{article}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newcommand{\ReLU}{\mathrm{ReLU}}



\title{Proof No Diamond (3 layers)}
\date{}

\begin{document}

\maketitle

\begin{definition}
	A pair of paths $(\pi,\pi')$
	is called {\em compensating} if they start in the same neuron $a$ and 
	ends in the same neuron $z$, and the product of weights over $\pi$ is strictly positive and the product of weights over $\pi'$ is strictly negative.
\end{definition}

Intuitively, compensating paths will partially cancel out each other as they conribute the same weight $w(a)$ to the weight of the same neuron $w(z)$, but with opposite sign. 
It is not simple to take this compensation into account because of ReLUs: the particular compensation will depend upon the weight of intermediate nodes seen along $\pi$ and $\pi'$, 
as when one of this node gets negative input, it will clip it to 0.


We assume all active functions are ReLU function.

\begin{definition}

	\begin{itemize}
	 \item  We use $a_i$ to denote nodes in the input layer, use $b_j$ to denote nodes in the first hidden layer, etc and finally use $z_i$ to denote the output nodes.
	For $L,L'$ consecutive layers, for $x \in L$ and $y \in L'$, we denote 
	$w({x y})$ to denote the fixed weight from $x$ to $y$, while we use
	$weight_Y(x)$ to denote the weights of node $x$ from initial configuration $Y$. 
	We use $bias$ to denote bias weightt.
	
	\item We use $b^-$ to denote the node before ReLU and $b^+$ to denote the node after ReLU: $b^+ = \ReLU(b^-)$.
	\end{itemize}
	
%	3. $\bar{f}$ is the upper bound approximation function of DeepPoly, and $\underline{f}$ is the lower bound approximation function.
\end{definition}


\section{Theorem 1}



\begin{theorem}
	\label{th1}
	If for all $(x,y)$ there is no compensation paths $(\pi,\pi')$ 
	in the network from $x$ to $y$, then the LP approximation is 
	$100\%$ accurate (or deepoly with the abstraction $\underline{f}(x) = 0$ for the lower bound, never using the $\underline{f}(x) = x$ abstraction). 
\end{theorem}

\subsection*{Proof of Theorem \ref{th1}}

First, notice that if $weight_Y(b^-)$ is maximal (resp. minimal), 
then $weight_Y(b^+=\ReLU(b^-))$ also gets maximal (resp. minimal).
The following definition is the most important concept in this proof.

\begin{definition}[Sign of Node]\label{sign_of_nodes}
	We define a sign function $S$ on nodes $n$ such that: 	(1). $S(n)=0$ if all path from $n$ to $c$ has 0 weight; (2). $S(n)=1$ if all path from $n$ to $c$ has non-negative weight, and at least one path has a positive weight; (3). $S(n)=-1$ if all path from $n$ to $c$ has non-positive weight, and at least one path has a negative weight. 
	In general, $S$ may not be defined on every node (e.g. if there is a negative and a positive path from $a$ to $c$). However, if there is no Diamond, $S$ is defined on all nodes: any node fulfills one of above cases (1),(2),(3).


	For instance, for an input node $a_i$ of any 3 layer DNN:
	\begin{enumerate}
      \item  $S(a_i)=0$ if 
	  for all $b_j, w(a_i b_j)\cdot w(b_j c) = 0$
	
	
	 \item  $S(a_i)=1$ if for all $b_j, w(a_i b_j)\cdot w(b_j c) \geq 0$ and there exists 
	 $j, w(a_i b_j)\cdot w(b_j c) > 0$
	
	\item $S(a_i)=-1$ if for all $b_j\ w(a_i b_j)\cdot w(b_j c) \leq 0$ and there exists 
	$j, w(a_i b_j)\cdot w(b_j c) < 0$ 
\end{enumerate}

For $b_j$ in the hidden layer, we have $S(b_j)=1,-1,0$ if $w(b_j c)$ is positive, negative, or 0 respectively. Finally, for the output node $c$, we define $S(c)=1$.
\end{definition}


\begin{lemma}[Sign]
	\label{lemma1}
Let $L,L'$  are consecutive layers be of a DNN without compensation. 
Then if both 
$w(m n) \neq 0$ and $S(n) \neq 0$, then 
	$S(m)=S(n)\mathrm{Sign}(w(n m))$.
\end{lemma}

\begin{proof}
	If $S(n) \neq 0$, then there is a path $\pi$ from $n$ to the output node $c$ with a nonzero weight of the same sign as $S(n)$. 
	
	Hence there is a non zero path from $m$ to $c$: $(m n) \pi$, which is of sign 
	$S(n)\mathrm{Sign}(w(mn))$. As there is no compensation, $S(m)=S(n)\mathrm{Sign}(w(mn))$.
\end{proof}


For a node $n$, we use $n_s$ to denote $S(n)\cdot n$. 
Notice that for $S(n)=1$, $n_s$ gets maximal value whenever $n$ gets maximal value; 
while for $S(n)=-1$, $n_s$ gets maximal value whenever $n$ gets minimal value (and vice versa). For $S(n)=0$, $n_s=0$ and thus always reach his minimal and maximal.



\begin{lemma}
	\label{lemma2}
	Let $L,L'$ be consecutive layers of a compensation free DNN, and $n \in L'$. 
	Then:
		$$ \max(n^-_s)=\sum_{m \in L}w(m n) \max(m^+_s)+bias_n \text{ and }$$
		$$\max(n^+_s)=ReLU(\sum_{m \in L}w(m n) max(m^+_s)+bias_n)$$
		
	
	
	Similarly  for minimal value,	
	$ \min(n^-_s)=\sum_{m \in L}w(m n)\min(m^+_s)+bias_n $ and
	$ \min(n^+_s)=ReLU(\sum_{m \in L}w(m n)\min(m^+_s)+bias_n)$
\end{lemma}

\begin{proof}
We choose $\bar{Y}$ maximizing each $a_s \in \{a,-a\}$ in the input layer $L=L_0$,
	that is setting $a$ as $max(a)$ for $S(a)=1$ and $min(a)$ for $S(a)=-1$.
At configuration $\bar{Y}$, every $a_s(\bar{Y}))=\max_Y {a_s(Y))}=max(a_s)$ is maximized.  

Consider any $b$ in the next layer $L_1$.
Assume first that $S(b)=+1$.
We have $weight(b_s)= weight(b) = \sum_{a \in L_0} w(a b) weight(a) + bias_b$.
If every $w(a b) weight(a)$ is maximized in the same time, 
then $weight(b_s)= weight(b)$ is also maximized. 

We claim that this is the case at 
configuration $\bar{Y}$.
Indeed, if $w(a b)=0$, any $weight(a)$ can be chosen to maximize 
$w(a b) weight(a)$. If $w(a b)>0$ then $S(a) = +1$ by Lemma \ref{lemma1}, since
$S(b)=+1$. We have $max(a_s) = max(a)$, maximizing $w(a b) weight(a)$.
Finally, if $w(a b)<0$ then $S(a_s) = -1$, and we have 
$max(a_s) = -min(a)$, maximizing $w(a b) weight(a)$.

Hence $b$ reaches is maximum in configuration $\bar{Y}$, and the value satisfies: 
$$\max(b)=\sum_{m \in L}w(a b) \max(a_s)+bias_b$$

The case $S(b)=-1$ is symetric:
$\max(b_s)= -\min(b)$ is reached when minimizing $b$, which is also 
satisfied at $\bar{Y}$.
In both case, $weight(b_s)$ is maximized at $\bar{Y}$. Notice that $\bar{Y}$ does not depend upon $b$, so choosing $\bar{Y}$ uniformly maximize all $weight(b_s)$.

This implies that $ReLU(b^-_s)$ is maximized at $\bar{Y}$, and its value is 
$ReLU(\sum_{m \in L}w(a b) \max(a_s)+bias_b)$, again uniformly reached over all $b$ at configuration $\bar{Y}$.

We proceed by induction over every layer of the DNN till the output layer, proving that 
$max(z_s)=max(z)$ is reached at configuration $\bar{Y}$.

The case for $min(x_s)$ is similar, reached at configuration $\underline{Y}$.
\end{proof}

As direct corollary of Lemma \ref{lemma2}, we obtain:

\begin{corollary}
	\label{cor1}
	$$\max(n^-)=\sum_{m \in L, w(m n)>0}w(m n) \max(m^+) + \sum_{m \in L, w(m n)<0}w(m n) \min(m^+) + bias_n$$
	$$\min(n^-)=\sum_{m \in L, w(m n)>0}w(m n) \min(m^+) + \sum_{m \in L, w(m n)<0}w(m n) \max(m^+) + bias_n$$
\end{corollary}
	

Notice that for all $x$, either $S(x)=+1$ and 
$(\min(x),\max(x))=(\min(x_s),\max(x_s))$, 
or $S(x)=-1$ and $(\min(x),\max(x))=(-\max(x_s),-\min(x_s))$.
In both case, these two bounds are found in Lemma \ref{lemma2},
from configuration $\bar{Y}$ and $\underline{Y}$.
We now show that DeepPoly will generate the same bounds 
$\underline{f}(x)=\min(x)$ and $\bar{f}(x)=\max(x)$.

\paragraph{DeepPoly bounds}

To prove that DeepPoly bounds are the same as the exact bounds computed above, 
we show that even the box abstraction (which is easier) would reach the exact bounds.
We proceed inductively, and prove the inductive step.
The initialization is obvious as Box Abstraction/DeepPoly is always exact for the initialization layer.

\begin{lemma}
	Let $L,L'$ be two consecutive layers.
	Assume for all node $m$ of $L$ that the lower and upper bounds for $m$ used by box abstraction equals to the exact lower and upper bounds of $m$ (as expressed above), i.e.
	$\bar{f}(m)=max(m)$ and $\underline{f}(m)=min(m)$.
	
	Then Box abstraction computes the exact lower and upper bounds for every node $n$ of layer $L'$, ie $\bar{f}(n)=max(n)$ and $\underline{f}(n)=min(n)$.
\end{lemma}

\begin{proof}
	Suppose $n$ is a node in $L'$ such that $S(n)\neq 0$ (otherwise, it is trivial). 
	First assume $S(n)=1$. The case $S(n)=-1$ is similar. 

	The Box absraction computes its upper bound using:
	$$\bar{f}(n^-)= \sum_{w(mn)>0} w(mn) \bar{f}(m^+) + \sum_{w(mn)<0} w(mn) \underline{f}(m^+) + bias_n$$

	By induction hypothesis, we have 
	$\bar{f}(m^+)=max(m^+)$ and
	$\underline{f}(m^+)=min(m^+)$, thus 
	applying Corollary \ref{cor1}, we obtain
	$\bar{f}(n^-)=max(n^-)$ and 
	$\underline{f}(n^-)=min(n^-)$.

	Now, if $\underline{f}(n^-)=min(n^-)<0$, 
	then $\underline{f}(n^+)=min(n^+)=0$, 
	and otherwise 
	$\underline{f}(n^+)=min(n^+)=\underline{f}(n^-)=min(n^-)$.

	Similarly, 
	if $\bar{f}(n^-)=max(n^-)<0$, 
	then $\bar{f}(n^+)=max(n^+)=0$, 
	and otherwise 
	$\bar{f}(n^+)=max(n^+)=\bar{f}(n^-)=bar(n^-)$.
	
	Hence we have equality before and after ReLU in all cases.
\end{proof}


\section{Theorem 2}

We now show that if all intermediates nodes that are on compensating pairs are opened as MILP nodes, then MILP will be correct. First, we do the proof with 3 layers, assuming (without loss of generality) a unique output node $z$.

\begin{theorem}
	\label{no_diamond_2}
	Using MILP method, if every nodes $b_j$ in any compensating path pair
	 $(\pi,\pi')$ is encoded as a binary/integer variable, then the upper and lower 
	 bounds computed by MILP are the exact max and min value of $z$.
\end{theorem}

By symetry, we only show the max side. 



\begin{definition}
	Let $K$ be the set of all input nodes $a_k$. 
	We define a decomposition $K=I\sqcup J$ as follows:  
	\begin{itemize}
 \item $k \in I$  if
 \begin{enumerate}
	 \item every path from $a_k$ to $z$ has weight $\geq 0$, or
	\item every path from $a_k$ to $z$ has weight $\leq 0$.
 \end{enumerate}
That is, $a_k$ is not a source of a compensating pair.
	\item $k \in J$ if $k \notin I$, that is there exists two paths $\pi,\pi'$ from $a_k$, 
	one with positive and one with negative weight.
\end{itemize}
\end{definition} 


\begin{lemma} \label{lem:open_node}
	A node $b$ in the hidden layer will not be on a compensating pair iff one of the following two happens:
	\begin{enumerate}
	 \item $w_{b,z}=0$, or
	 \item For every input node $j\in J$, we have $w_{a_j,b}=0$.
	\end{enumerate}
	We denote $B_{pure}$ the set of such nodes $b$ such that at least one of the above holds.
\end{lemma}

\begin{proof}
	First, we show that if either one of 1,2 happens, then $b_i$ will not be opened. If 1), it is obvious. For 2, we reason by contradiction: assume there is a pair of compensating paths 	$(\pi,\pi')$ starting with $a$, with $k$ in $\pi$ and weight$(\pi) > 0$. It means that $a \in J$. A contradiction as 2) $w_{a_j,b}=0$ implies weight$(\pi)=0$.
	
	Second, we show that if neither 1 nor 2 hold, then $b$ will be on a compensating path.
	Because 2) does not hold, there is a $j \in J$ with $w_{a_j,b} \neq 0$, say $>0$.
	Because 1) does not hold, $w_{b,z} \neq 0$, say $>0$.
	Now, by definition of $J$, there is a pair of compensating paths $\pi,\pi'$ 
	from $a_j$ to $z$, say with $\pi'$ with weight $<0$.
	Then $((a_j,b,z), \pi')$ is also a compensating pair.
\end{proof}

Consider the sign of nodes function from Definition \ref{sign_of_nodes}. Because now we allow compensating paths, we cannot define this function over all nodes.


\begin{definition}\label{sign_of_nodes_in_I}
	We define a partial sign function $S$ over nodes $n$ such that : 	
	\begin{enumerate} 
		 \item all paths from $n$ to $z$ have 0 weight, and then $S(n)=0$; 
		 \item all paths from $n$ to $z$ have non-negative weight, and at least one path has a positive weight, and then $S(n)=1$; 
		 \item all path from $n$ to $z$ has non-positive weight, and at least one path has a negative weight, and then $S(n)=-1$.
		 \item Otherwise $S(n)$ is undefined.
	\end{enumerate}
\end{definition}	
	
Notice that $S$ is defined on all nodes $a_i$ with $i \in I$ (it can also be defined trivially for all nodes $b$ as there is a unique path to $z$, and at $z$ also). Hence it is undefined only for nodes $a_j$ with $j \in J$.


We denote $a_S$ for any subset $S\subseteq K$ to refer the input vector $\langle a_k\rangle_{a_k\in S}$. We also denote $a_I\oplus a_J = a_K$ and $z=z(a_K)=z(a_I,a_J)$.
Consider $a_I^*$ the input vector such that for all $i \in I$, the value for $a_i$ gets its maximal value (if $S(a_i)=1$) or $a_i$ gets its minimal value (if $S(a_i)=-1$).

\begin{lemma} \label{lem:reach_max}
	$max_a (weight_{a}(z)) = max_{\{a \mid a_I=a^*_I\}} (weight_{a}(z))$
	
	Further, for every intermediate node $b$ in the hidden layer, for any valuation $a^0_J$, 
	we have $max_{\{a \mid a_J=a^0_J\}} (weight_{a}(b)) = max_{\{a \mid a_J=a^0_J,a_I=a_I^*\}} (weight_{a}(b))$.	
\end{lemma}

\begin{proof}
	For the first statement, we use 
	$$max_{a_K} (weight_{a_K}(z)) = max_{a_J} max_{a_I} (weight_{a_I,a_J}(z))$$
	
	Now, for any fixed input $a^0_J$, we can regard $weight_{a_I,a^0_J}(z)$ as a DNN $D'$ with input nodes $a_i\in I$, with all $a^0_J$ %and their propagation in hidden layers 
	as bias. In the simplified $D'$, there is no compensating path because of the definition of $I$. Therefore we can apply Theorem \ref{th1} (make a more precise corollary) to get that $z(\cdot,a^0_J)$ reaches its maximal value for $a_I=a_I^*$, and we are done.
	
	The second statement is simpler because for each fixed partial input $a^0_J$, 
	$$b= B^0 +\sum_{a_i\in I} w_{a_i, b} a_i,$$ where $B^0$ is a constant which is the sum of the term including bias and $a^0_J$. Applying Lemma \ref{lemma1}, we obtain the statement.
\end{proof}

\subsection{MILP abstraction}

Consider now an MILP abstraction using the abstraction from \cite{MILP}, 
where each variable $\alpha_b$ for the ReLU from $b^-$ to $b^+$ is:
\begin{itemize}
	\item linear  (i.e. $\alpha_b \in [0,1]$) for $b \in B_{pure}$
	\item binary/integer (i.e. $\alpha_b \in \{0,1\}$) for $b \notin B_{pure}$
\end{itemize}


\begin{lemma}\label{lem:AppB=ReLU}
	Let $\mathrm{AppB}$ be the upper bound of lower bound approximation functions of LP for $\ReLU$ function with known upper and lower bound (numbers) $u,l$. Then when $x$ reaches the extremal values (maximum or minimum values, $u$ or $l$), then $\mathrm{AppB}(x)=\ReLU(x)$.
	
\end{lemma}

\begin{proof}
	This is simply by definition of LP approximation functions.
\end{proof}

Notice that there is only one layer of ReLUs, in the hidden layer.
Denote by $\mathrm{UB}$ the maximal bound on $z$ considering the MILP constraints above, with ReLUs being either linear or binary depending on whether $b \in B_{pure}$ or not.

We want to show that $\mathrm{UB} = \max z$. As the MILP abstraction is a sound overapproximation, 
it suffices to show that $\mathrm{UB}\leq \max c$.

For any $b_h\in B_{pure}$, we have $b_h=\sum_{a_i\in I}w_{b_ha_i}+B_h$ ($B_h$ is the bias). 
Denote by $B_O$ the set of nodes $b$ for which $\alpha_b$ is binary, i.e. 
$b \notin B_{pure}$. %For any subdomain $D \subseteq B_O$, 
Let $a^0_J$ and $a^0_I$ be fixed inputs, and consider the associated upper bound 
$\mathrm{UB}_{a^0_J,a^0_I}$. We have:
%We show that this upper bound does not exceed the maximal value of $z$: 

\begin{align*}
	\mathrm{UB}_{a^0_J,a^0_I} = B_z + \sum_{b\in B_O} w_{b,z}\ReLU(weight_{a^0_I,a^0_J}(b)) + \sum_{b\in B_{pure}} w_{b,z} \mathrm{AppB}(weight_{a^0_I,a^0_J}(b)).
\end{align*} where $\mathrm{AppB}(weight_{a^0_I,a^0_J}(b))$ is the upper bound of 
the (LP) approximation of $weight_{a^0_I,a^0_J}(b)$ if $w_{b,z}>0$, and the lower bound
of its (LP) approximation if $w_{b,z}<0$. 

By Lemma \ref{lem:reach_max}, for any fixed input $a^0_J$, for all nodes $b$ in the hidden layer, if $S(b)=\mathrm{sign}(w_{zb})=1$, then 
$weight_{a_I,a^0_J}(b)$ will get its maximal value for $a_I=a_I^*$,
and if $S(b)=\mathrm{sign}(w_{zb})=-1$, then $weight_{a^0_I,a^0_J}(b)$ will get its minimal value for $a_I=a_I^*$.
Notice that $\mathrm{ReLU}$ and $\mathrm{AppB}$ are both non decreasing functions, and thus:

\begin{align*}
	\mathrm{UB}_{a^0_J,a^0_I} \leq B_z + \sum_{b\in B_O} w_{b,z}\ReLU(weight_{a^*_I,a^0_J}(b)) +
	\sum_{b\in B_{pure}} w_{b,z} \mathrm{AppB}(weight_{a^*_I,a^0_J}(b)).
\end{align*} 

Added FIX:
Consider $B_{pure}=B^1 \cup B^2$, with $b \in B^1$ iff
$w_{b,z} = 0$ and $b \in B^2$ iff $w_{b,z} \neq 0$.
Hence:

$$\sum_{b\in B_{pure}} w_{b,z} \mathrm{AppB}(weight_{a^*_I,a^0_J}(b)) = 
\sum_{b\in B^2} w_{b,z} \mathrm{AppB}(weight_{a^*_I,a^0_J}(b))$$

For $b \in B^2$, we have $w_{a_j,b}=0$ for all $j \in J$
by definition of $B_{pure}$ (Lemma 4.), hence

$$\sum_{b\in B^2} w_{b,z} \mathrm{AppB}(weight_{a^*_I,a^0_J}(b)) = 
\sum_{b\in B^2} w_{b,z} \mathrm{AppB}(weight_{a^*_I}(b))
=\sum_{b\in B^2} w_{b,z} \mathrm{ReLU}(weight_{a^*_I}(b))
$$ as $\mathrm{AppB}(weight_{a^*_I}(b))=\mathrm{ReLU}(weight_{a^*_I}(b))$ since 
$weight_{a^*_I}(b)$ is an extremal value (minimum or maximal) and 
$\mathrm{AppB}$ equals $\mathrm{ReLU}$ for extremal values.

Hence we have: \begin{align*}
	\mathrm{UB}_{a^0_J,a^0_I}  \leq & B_z+\sum_{b}w_{zb}\ReLU(b(a_I^*,\bar{a}_J))
	= \max_{a_I} z(a_I,\bar{a}_J) \\
	\leq & \max_{a_J}\max_{a_I} z(a_I,a_J) 	= \max_{a_K} weight_{a_K}(z)
\end{align*}

Considering the maximal over all inputs $a_J,a_I$, we obtain 

$$\mathrm{UB} = max_{a_J,a_I} \mathrm{UB}_{a_J,a_I} \leq \max_{a_J,a_I} weight_{a_J,a_I}(z)$$ And this us what we want to show.


\section{Proof of Theorem 2, full version}

In this section, we will try to prove the full version of theorem 2, that is 

\begin{theorem}
	\label{no_diamond_3}
	
	Suppose we have a DNN with $\ReLU$ as the unique activation functions. We consider compensating pairs in between any two nodes (i.d., the source node or target node can in hidden layers.)
	
	Using MILP method, if every nodes $b$ in any compensating pair
	$(\pi,\pi')$ is encoded as a binary/integer variable, then the upper and lower 
	bounds computed by MILP are the exact max and min value of $z$.
\end{theorem}


Before the proof of above theorem, we will try to weaker one, that is, consider not only compensating pairs, but also pairs that allows common nodes (the two paths in a compensating pairs cannot have a common nodes). We call such pair a weak compensating pair.

We uses very similar definitions as in previous section.

\begin{definition}
	Let $K$ be the set of all input nodes $a_k$. 
	We define a decomposition $K=I\sqcup J$ as follows:  
	\begin{itemize}
		\item $k \in I$  if
		\begin{enumerate}
			\item every path from $a_k$ to $z$ has weight $\geq 0$, or
			\item every path from $a_k$ to $z$ has weight $\leq 0$.
		\end{enumerate}
		That is, $a_k$ is not a source of a weak compensating pair.
		\item $k \in J$ if $k \notin I$, that is there exists two paths $\pi,\pi'$ from $a_k$, 
		one with positive and one with negative weight.
	\end{itemize}
\end{definition} 



\begin{lemma} \label{lem:open_node_2}
	A node $b$ in the hidden layer will not be on a weak compensating pair iff one of the following two happens:
	\begin{enumerate}
		\item $w_{b,z}=0$, or
		\item For every input node $j\in J$, we have $w_{a_j,b}=0$.
	\end{enumerate}
	We denote $B_{pure}$ the set of such nodes $b$ such that at least one of the above holds.
\end{lemma}

\begin{proof}
	First, we show that if either one of 1,2 happens, then $b$ will not be opened. If 1), it is obvious. For 2, we reason by contradiction: assume there is a pair of weak compensating paths 	$(\pi,\pi')$ starting with $a$, with $b$ in $\pi$ and weight$(\pi) > 0$. It means that $a \in J$. A contradiction as 2) $w_{a_j,b}=0$ implies weight$(\pi)=0$.
	
	Second, we show that if neither 1 nor 2 hold, then $b$ will be on a weak compensating path.
	Because 2) does not hold, there is a $j \in J$ with $w_{a_j,b} \neq 0$, say $>0$.
	Because 1) does not hold, $w_{b,z} \neq 0$, say $>0$.
	Now, by definition of $J$, there is a pair of weak compensating paths $\pi,\pi'$ 
	from $a_j$ to $z$, say with $\pi'$ with weight $<0$.
	Then $((a_j,b,z), \pi')$ is also a weak compensating pair.
\end{proof}


Consider the sign of nodes function from Definition \ref{sign_of_nodes}. Because now we allow compensating paths, we cannot define this function over all nodes.


\begin{definition}\label{sign_of_nodes_in_I_2}
	We define a partial sign function $S$ over nodes $n$ such that : 	
	\begin{enumerate} 
		\item all paths from $n$ to $z$ have 0 weight, and then $S(n)=0$; 
		\item all paths from $n$ to $z$ have non-negative weight, and at least one path has a positive weight, and then $S(n)=1$; 
		\item all path from $n$ to $z$ has non-positive weight, and at least one path has a negative weight, and then $S(n)=-1$.
		\item Otherwise $S(n)$ is undefined.
	\end{enumerate}
\end{definition}	

Notice that $S$ is defined on all nodes $a_i$ with $i \in I$ (it can also be defined trivially for all nodes $b$ as there is a unique path to $z$, and at $z$ also). Hence it is undefined only for nodes $a_j$ with $j \in J$.




We denote $a_S$ for any subset $S\subseteq K$ to refer the input vector $\langle a_k\rangle_{a_k\in S}$. We also denote $a_I\oplus a_J = a_K$ and $z=z(a_K)=z(a_I,a_J)$.
Consider $a_I^*$ the input vector such that for all $i \in I$, the value for $a_i$ gets its maximal value (if $S(a_i)=1$) or $a_i$ gets its minimal value (if $S(a_i)=-1$).

\begin{lemma} \label{lem:reach_max_2}
	$max_a (weight_{a}(z)) = max_{\{a \mid a_I=a^*_I\}} (weight_{a}(z))$
	
	Further, for every intermediate node $b$ in the hidden layer, for any valuation $a^0_J$, 
	we have $max_{\{a \mid a_J=a^0_J\}} (weight_{a}(b)) = max_{\{a \mid a_J=a^0_J,a_I=a_I^*\}} (weight_{a}(b))$.	
\end{lemma}

\begin{proof}
	For the first statement, we use 
	$$max_{a_K} (weight_{a_K}(z)) = max_{a_J} max_{a_I} (weight_{a_I,a_J}(z))$$
	
	Now, for any fixed input $a^0_J$, we can regard $weight_{a_I,a^0_J}(z)$ as a DNN $D'$ with input nodes $a_i\in I$, with all $a^0_J$ %and their propagation in hidden layers 
	as bias. In the simplified $D'$, there is no compensating path because of the definition of $I$. Therefore we can apply Theorem \ref{th1} to get that $z(\cdot,a^0_J)$ reaches its maximal value for $a_I=a_I^*$, and we are done.
	
	For the second part, for any $b$ that is at least 2 layers after $a$, we can regard $b$ as the output node and apply the same argument above.
	
	If $b$ is immediately after $a$, then the proof is the same as last part of Lemma \ref{lem:reach_max} last section.
	

\end{proof}


\subsection{MILP abstraction}

Consider now an MILP abstraction using the abstraction from \cite{MILP}, 
where each variable $\alpha_b$ for the ReLU from $b^-$ to $b^+$ is:
\begin{itemize}
	\item linear  (i.e. $\alpha_b \in [0,1]$) for $b \in B_{pure}$
	\item binary/integer (i.e. $\alpha_b \in \{0,1\}$) for $b \notin B_{pure}$
\end{itemize}

Denote by $\mathrm{UB}$ the maximal bound on $z$ and $\mathrm{LB}$ the minimal bound on $z$ considering the MILP constraints above, with ReLUs being either linear or binary depending on whether $b \in B_{pure}$ or not.

For an input vector $a^0_I,a^0_J$, let $UB_{a^0_I,a^0_J}$ be the upper bound in the fixed formulation with input $a^0_I,a^0_J$, and similarly for $LB$ lower bound. We have the following lemma:

\begin{lemma} In an MILP formulation, if $c$ is a node:
	
	1. $UB(b)=\max_{a^0_I,a^0_J}UB_{a^0_I,a^0_J}(c)$. 
	
		1. $LB(b)=\min_{a^0_I,a^0_J}LB_{a^0_I,a^0_J}(c)$. 
\end{lemma}

\begin{proof}
	This is by the definition of MILP formulation and $UB_{a^0_I,a^0_J}, LB_{a^0_I,a^0_J}$.
\end{proof}

\begin{lemma}\label{lem:pure_node}
	For a node $c\in B_{pure}$ such that $w_{b,z}\neq 0$, $UB_{a^0_I,a^0_J}$ (and $LB$ the same) only depends on $a^0_I$.
\end{lemma}

\begin{proof}
	We prove this lemma by induction. For the first hidden layer, this is trivial.
	
	Suppose for layer up to $l_i$, this lemma is true, we show it for layer $l_{i+1}$. For a node $c$ in layer $l^{i+1}$, we have that: \begin{align*}
		UB_{a^0_I,a^0_J}(c) = B_c + \sum_{b\in B_O\cap l_i} w_{b,c}\ReLU(B_{a^0_I,a^0_J}(b)) + \sum_{b\in B_{pure}\cap l_{i}} w_{b,c} \mathrm{AppB}(B_{a^0_I,a^0_J}(b)).
	\end{align*} But since $c\in B_{pure}$, for any $b\in B_O$, we must have $w_{b,c}=0$. Therefore, we will have:
	
	\begin{align*}
		UB_{a^0_I,a^0_J}(c) = B_c + \sum_{b\in B_{pure}\cap l_{i}} w_{b,c} \mathrm{AppB}(B_{a^0_I,a^0_J}(b)).
	\end{align*} By induction, for any $b\in B_{pure}\cap l^i$, either $w_{b,z}=0$ or it only depends on $a^0_I$. Since $w_{c,z}\neq 0$, if $w_{b,z}=0$, we must have $w_{b,c}=0$. Therefore, we will have:
	
	\begin{align*}
		UB_{a^0_I,a^0_J}(c) = B_c + \sum_{b\in B_{pure}\cap l_{i}\wedge w_{b,c}\neq0} w_{b,c} \mathrm{AppB}(B_{a^0_I}(b)).
	\end{align*} So, $UB_{a^0_I,a^0_J}(c)$ only depends on $a_I^0$.
\end{proof}


We want to show that $\mathrm{UB}(z) = \max z$ and $\mathrm{LB}(z) = \min z$ for the unique output nodes and we will prove this for all $c$ such $w_{c,z}\neq 0$. As the MILP abstraction is a sound overapproximation, 
it suffices to show that $\mathrm{UB}\leq \max c$ and $\mathrm{LB} \geq \min c$. We will prove this by induction on layers from the first hidden layer to the output layer. More specifically, we will prove the following lemma:

\begin{lemma}
	For the $i$-th layer $l^i$ ($1$-th layer is the first hidden layer), for a node $c\in l^i$, if $w_{c,z}\neq 0$, then:
	
	 For any fixed $a^0_J$, $\max_{a_I} UB_{a_I,a^0_J}(c)=UB_{a^*_I,a^0_J}(c)= weigth(a^*_I,a^0_J)(c) = \max_{a_I} weigth(a_I,a^0_J)(c)$, where $a^*_I$ is described in previous section.
	
The similar is true for lower bound $LB$.
\end{lemma}

\begin{proof}
	
	For $i\leq 2$, we have proved in previous section.
	
	Suppose we have proved this lemma up to layer $i\geq 2$, and we are going to prove it for $i+1$-th layer. Let $c$ be a node on $i+1$-th layer such that $w_{c,z}\neq 0$.
	
	By definition, we have that for any input vector $a^0_I,a^0_J$:
	
	\begin{align*}
		\mathrm{UB}_{a^0_J,a^0_I}(c) = B_c + \sum_{b\in B_O\cap l_i} w_{b,c}\ReLU(B_{a^0_I,a^0_J}(b)) + \sum_{b\in B_{pure}\cap l_{i}} w_{b,c} \mathrm{AppB}(B_{a^0_I,a^0_J}(b)).
	\end{align*} where $\mathrm{AppB}$ is the same as in previous section  whose definition is independent with $a^0_I,a^0_J$. $B$ is one of $UB,LB$  that, if $w_{b,c}>0$, then $B$ is $UB$ and otherwise $B$ is $LB$.
	
	By Lemma \ref{lem:pure_node}, we can simplify above equation to \begin{align*}
			\mathrm{UB}_{a^0_J,a^0_I}(c) = B_c + \sum_{b\in B_O\cap l_i} w_{b,c}\ReLU(B_{a^0_I,a^0_J}(b)) + \sum_{b\in B_{pure}\cap l_{i}\wedge w_{b,c}\neq 0} w_{b,c} \mathrm{AppB}(B_{a^0_I}(b)).
	\end{align*}
	
	By induction hypothesis, since both $\ReLU$ and $\mathrm{AppB}$ are non-decreasing functions, we have that \begin{align*}
		\mathrm{UB}_{a^0_J,a^0_I}(c)\leq 
		 &B_c + \sum_{b\in B_O\cap l_i} w_{b,c}\ReLU(B_{a^*_I,a^0_J}(b)) + \sum_{b\in B_{pure}\cap l_{i}\wedge w_{b,c}\neq 0} w_{b,c} \mathrm{AppB}(B_{a^*_I}(b))\\
		= & \mathrm{UB}_{a^0_J,a^*_I}(c) \\
	\end{align*} 
	
Now, since when $x$ reaches the max or min of $\mathrm{AppB}$'s domain, $\mathrm{AppB}(x)=\ReLU(x)$, and by induction hypothesis part 2, for any $b\in B_{pure}\cap l_i\wedge w_{b,c}\neq 0$, $\mathrm{AppB}(B_{a^*_I}(b))=\ReLU(B_{a^*_I})$. Hence, by induction hypothesis, we will have \begin{align*}
	\mathrm{UB}_{a^0_J,a^0_I}(c)\leq & \ \mathrm{UB}_{a^0_J,a^*_I}(c) \\
	= &B_c + \sum_{b\in B_O\cap l_i} w_{b,c}\ReLU(B_{a^*_I,a^0_J}(b)) + \sum_{b\in B_{pure}\cap l_{i}\wedge w_{b,c}\neq 0} w_{b,c} \mathrm{AppB}(B_{a^*_I}(b))\\
	= &B_c + \sum_{b\in B_O\cap l_i} w_{b,c}\ReLU(weight_{a^*_I,a^0_J}(b)) + \sum_{b\in B_{pure}\cap l_{i}} w_{b,c} \mathrm{ReLU}(weight_{a^*_I}(b))\\
	= & weight_{a^*_I,a^0_J}(c).
\end{align*} This finishes the proof.
	\end{proof}


Now, we have that for any fixed $a^0_J$, $\max_{a_I} UB_{a_I,a^0_J}(c)= \max_{a_I} weigth(a_I,a^0_J)(c)$, therefore, $\max UB_{a_I,a_J}(c)= \max weigth(a_I,a_J)(c)$ and this is to say $UB(c)=\max(c)$.








\end{document}














