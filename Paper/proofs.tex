	\section{Proof of Theorem \ref{th1}}	
	
	First, notice that if $\Val_{\vx}(b)$ is maximal (resp. minimal), 
	then $\Val_{\boldsymbol{x}}(\hat{b})$ also gets maximal (resp. minimal).
	The following definition is the most important concept in this proof.
	
	\begin{definition}[Sign of Node]\label{sign_of_nodes}
		We define the partial sign function $S$ on neurons $n$: 	
		\begin{enumerate}
			 \item $S(n)=0$ if all path from $n$ to $z$ have 0 weight; 
			 \item $S(n)=1$ if all path from $n$ to $z$ hafe non-negative weight, and at least one path has a positive weight; 
			 \item $S(n)=-1$ if all path from $n$ to $z$ have non-positive weight, and at least one path has a negative weight. 
		\end{enumerate}
	\end{definition}
	
	In general, $S$ may not be defined on every node (e.g. if there is a negative and a positive path from $a$ to $c$). However, if there is no compensating pair, $S$ is defined on all nodes: any node fulfills one of above cases (1),(2),(3).
		For instance, for an input node $a_i$ of any 3 layer DNN:
		\begin{enumerate}
			\item  $S(a_i)=0$ if 
			for all $b_j, W_{a_i b_j}\cdot W_{b_j c} = 0$
			
			
			\item  $S(a_i)=1$ if for all $b_j, W_{a_i b_j}\cdot W_{b_j c} \geq 0$ and there exists 
			$j, W_{a_i b_j}\cdot W_{b_j c} > 0$
			
			\item $S(a_i)=-1$ if for all $b_j\ W_{a_i b_j}\cdot W_{b_j c} \leq 0$ and there exists 
			$j, W_{a_i b_j}\cdot W_{b_j c} < 0$ 
		\end{enumerate}
		
	For $b_j$ in the hidden layer, we have $S(b_j)=1,-1,0$ if $W_{b_j c}$ is positive, negative, or 0 respectively. Finally, for the output node $z$, we define $S(z)=1$.
	

	
	\begin{lemma}[Sign]
		\label{lemma1}
		Let $L,L'$  consecutive layers of a DNN without compensation, 
		and $m\in L$ $n\in L'$. Then if both 
		$w(m n) \neq 0$ and $S(n) \neq 0$, then 
		$S(m)=S(n)\mathrm{Sign}(W_{n m})$.
	\end{lemma}
	
	\begin{proof}
		If $S(n) \neq 0$, then there is a path $\pi$ from $n$ to the output node $z$ with a nonzero weight of the same sign as $S(n)$. 
		
		Hence there is a non zero path from $m$ to $z$: $(m n) \pi$, which is of sign 
		$S(n)\mathrm{Sign}(W_{n m})$. As there is no compensation, $S(m)=S(n)\mathrm{Sign}(W_{n m})$.
		\hfill $\square$
	\end{proof}
	
	
	For a node $n$, we use $n_s$ to denote $S(n)\cdot n$. 
	Notice that for $S(n)=1$, $n_s$ gets maximal value whenever $n$ gets maximal value; 
	while for $S(n)=-1$, $n_s$ gets maximal value whenever $n$ gets minimal value (and vice versa). For $S(n)=0$, $n_s=0$ and thus always reach his minimal and maximal.
	
	
	
	\begin{lemma}
		\label{lemma2}
		Let $L,L'$ be consecutive layers of a compensation free DNN, and $n \in L'$. 
		Then:
		$$ S(n) \max(S(n) n)=\sum_{m \in L}W_{m n} S(m) \max(S(m) \hat{m})+bias_n \text{ and }$$
		$$S(n) \max(S(n)\hat{n}_s)=\ReLU(\sum_{m \in L}W_{m n} S(n)\max(S(n)\hat{m}_s)+bias_n)$$
		
		Similarly for minimal value.
		
		%	for minimal value,	
		%	$ \min(n)=\sum_{m \in L}W_{m n)\min(\)+bias_n $ and
			%	$ \min(\hat{n}_s)=ReLU(\sum_{m \in L}W_{m n)\min(\hat{m}s}+bias_n)$
		\end{lemma}
		
		\begin{proof}
			We choose $\boldsymbol{x}^*$ maximizing each $a_s \in \{a,-a\}$ in the input layer $L=L_0$,
			that is setting the value for node $a$ be $\max(a)$ for $S(a)=1$ and $\min(a)$ for $S(a)=-1$.
			At configuration $\boldsymbol{x}^*$, every $a_s(\boldsymbol{x}^*))=\max_{\boldsymbol{x}} {a_s(\boldsymbol{x}))}=\max(a_s)$ is maximized.  
			
			Consider any $b$ in the first hidden layer $L_1$.
			Assume first that $S(b)=+1$.
			We have $\Val(b_s)= \Val(b) = \sum_{a \in L_0} W_{ab} \Val(a) + bias_b$.
			If every $W_{ab} \Val(a)$ is maximized in the same time, 
			then $\Val(b_s)= \Val(b)$ is also maximized. 
			
			We claim that this is the case at 
			configuration $\boldsymbol{x}^*$.
			Indeed, if $W_{ab}=0$, any $\Val(a)$ can be chosen to maximize 
			$W_{ab}\Val(a)$. If $W_{ab}>0$ then $S(a) = +1$ by Lemma \ref{lemma1}, since
			$S(b)=+1$. We have $\max(a_s) = \max(a)$, maximizing $W_{ab}\Val(a)$.
			Finally, if $W_{a b}<0$ then $S(a_s) = -1$, and we have 
			$\max(a_s) = min(a)$, maximizing $W_{ab}\Val(a)$.
			
			Hence $b$ reaches is maximum in configuration $\boldsymbol{x}^*$, and the value satisfies: 
			$$\max(b)=\sum_{m \in L}W_{a b} \max(a_s)+bias_b$$
			
			The case $S(b)=-1$ is symmetric:
			$\max(b_s)= -\min(b)$ is reached when minimizing $b$, which is also 
			satisfied at $\boldsymbol{x}^*$.
			In both case, $\Val(b_s)$ is maximized at $\boldsymbol{x}^*$. Notice that $\boldsymbol{x}^*$ does not depend upon $b$, so choosing $\boldsymbol{x}^*$ uniformly maximize all $\Val(b_s)$.
			
			This implies that $\ReLU(b_s)$ is maximized at $\boldsymbol{x}^*$, and its value is 
			$\ReLU(\sum_{m \in L}W_{a }\max(a_s)+bias_b)$, again uniformly reached over all $b$ at configuration $\boldsymbol{x}^*$.
			
			We proceed by induction over every layer of the DNN till the output layer, proving that 
			$\max(z_s)=\max(z)$ is reached at configuration $\boldsymbol{x}^*$.
			
			The case for $\min$ is similar, with another input vector $\boldsymbol{x}^\#$ instead of $\boldsymbol{x}^*$.
			\hfill $\square$
		\end{proof}
		
		As direct corollary of Lemma \ref{lemma2}, we obtain:
		
		\begin{corollary}
			\label{cor1}
			$$\max(n)=\sum_{m \in L, W_{m n}>0}W_{m n} \max(\hat{m}) + \sum_{m \in L, W_{m n}<0}W_{mn} \min(\hat{m}) + bias_n$$
			$$\min(n)=\sum_{m \in L, W_{m n}>0}W_{m n} \min(\hat{m}) + \sum_{m \in L, W_{m n}<0}W_{mn} \max(\hat{m}) + bias_n$$
		\end{corollary}
		
		
		Notice that for all $x$, either $S(x)=+1$ and 
		$(\min(x),\max(x))=(\min(x_s),\max(x_s))$, 
		or $S(x)=-1$ and $(\min(x),\max(x))=(-\max(x_s),-\min(x_s))$.
		In both case, these two bounds are found in Lemma \ref{lemma2},
		from configuration $\boldsymbol{x}^*$ and $\boldsymbol{x}^\#$.
		
		
		
		Once we have above corollary, things become very clear and simple. We now show that approximations like DeepPoly will generate the same bounds 
		$\underline{f}(x)=\min(x)$ and $\bar{f}(x)=\max(x)$.
		
		\paragraph{DeepPoly bounds}
		
		To prove that DeepPoly bounds are the same as the exact bounds computed above, 
		we show that even the box abstraction (which is easier) would reach the exact bounds.
		We proceed inductively, and prove the inductive step.
		The initialization is obvious as Box Abstraction/DeepPoly is always exact for the initialization layer.
		
		\begin{lemma}
			Let $L,L'$ be two consecutive layers.
			Assume for all node $m$ of $L$ that the lower and upper bounds for $m$ used by box abstraction equals to the exact lower and upper bounds of $m$ (as expressed above), i.e.
			$\bar{f}(m)=max(m)$ and $\underline{f}(m)=min(m)$.
			
			Then Box abstraction computes the exact lower and upper bounds for every node $n$ of layer $L'$, ie $\bar{f}(n)=max(n)$ and $\underline{f}(n)=min(n)$.
		\end{lemma}
		
		\begin{proof}
			Suppose $n$ is a node in $L'$ such that $S(n)\neq 0$ (otherwise, it is trivial). 
			First assume $S(n)=1$. The case $S(n)=-1$ is similar. 
			
			The Box absraction computes its upper bound using:
			$$\bar{f}(n)= \sum_{W_{mn}>0} W_{mn} \bar{f}(\hat{m}) + \sum_{W_{mn}<0} W_{mn} \underline{f}(\hat{m}) + bias_n$$
			
			By induction hypothesis, we have 
			$\bar{f}(\hat{m})=\max(\hat{m})$ and
			$\underline{f}(\hat{m})=\min(\hat{m})$, thus 
			applying Corollary \ref{cor1}, we obtain
			$\bar{f}(n)=\max(n)$ and 
			$\underline{f}(n)=\min(n)$.
			
			Now, if $\underline{f}(n)=\min(n)<0$, 
			then $\underline{f}(\hat{n})=\min(\hat{n})=0$, 
			and otherwise 
			$\underline{f}(\hat{n})=\min(\hat{n})=\underline{f}(n)=\min(n)$.
			
			Similarly, 
			if $\bar{f}(n)=\max(n)<0$, 
			then $\bar{f}(\hat{n})=\max(\hat{n})=0$, 
			and otherwise 
			$\bar{f}(\hat{n})=\max(\hat{n})=\bar{f}(n)=\max(n)$.
			
			Hence we have equality before and after ReLU in all cases.
		\end{proof}
		
		
		%\section{Theorem 2}
		%
		%We now show that if all intermediates nodes that are on compensating pairs are opened as MILP nodes, then MILP will be correct. First, we do the proof with 3 layers, assuming (without loss of generality) a unique output node $z$.
		%
		%\begin{theorem}
		%	\label{no_diamond_2}
		%	Using MILP method, if every nodes $b_j$ in any compensating path pair
		%	 $(\pi,\pi')$ is encoded as a binary/integer variable, then the upper and lower 
		%	 bounds computed by MILP are the exact max and min value of $z$.
		%\end{theorem}
		%
		%By symetry, we only show the max side. 
		%
		%
		%
		%\begin{definition}
		%	Let $K$ be the set of all input nodes $a_k$. 
		%	We define a decomposition $K=I\sqcup J$ as follows:  
		%	\begin{itemize}
			% \item $k \in I$  if
			% \begin{enumerate}
				%	 \item every path from $a_k$ to $z$ has weight $\geq 0$, or
				%	\item every path from $a_k$ to $z$ has weight $\leq 0$.
				% \end{enumerate}
			%That is, $a_k$ is not a source of a compensating pair.
			%	\item $k \in J$ if $k \notin I$, that is there exists two paths $\pi,\pi'$ from $a_k$, 
			%	one with positive and one with negative weight.
			%\end{itemize}
			%\end{definition} 
			%
			%
			%\begin{lemma} \label{lem:open_node}
			%	A node $b$ in the hidden layer will not be on a compensating pair iff one of the following two happens:
			%	\begin{enumerate}
				%	 \item $w_{b,z}=0$, or
				%	 \item For every input node $j\in J$, we have $w_{a_j,b}=0$.
				%	\end{enumerate}
			%	We denote $B_{pure}$ the set of such nodes $b$ such that at least one of the above holds.
			%\end{lemma}
			%
			%\begin{proof}
			%	First, we show that if either one of 1,2 happens, then $b_i$ will not be opened. If 1), it is obvious. For 2, we reason by contradiction: assume there is a pair of compensating paths 	$(\pi,\pi')$ starting with $a$, with $k$ in $\pi$ and weight$(\pi) > 0$. It means that $a \in J$. A contradiction as 2) $w_{a_j,b}=0$ implies weight$(\pi)=0$.
			%	
			%	Second, we show that if neither 1 nor 2 hold, then $b$ will be on a compensating path.
			%	Because 2) does not hold, there is a $j \in J$ with $w_{a_j,b} \neq 0$, say $>0$.
			%	Because 1) does not hold, $w_{b,z} \neq 0$, say $>0$.
			%	Now, by definition of $J$, there is a pair of compensating paths $\pi,\pi'$ 
			%	from $a_j$ to $z$, say with $\pi'$ with weight $<0$.
			%	Then $((a_j,b,z), \pi')$ is also a compensating pair.
			%\end{proof}
			%
			%Consider the sign of nodes function from Definition \ref{sign_of_nodes}. Because now we allow compensating paths, we cannot define this function over all nodes.
			%
			%
			%\begin{definition}\label{sign_of_nodes_in_I}
			%	We define a partial sign function $S$ over nodes $n$ such that : 	
			%	\begin{enumerate} 
				%		 \item all paths from $n$ to $z$ have 0 weight, and then $S(n)=0$; 
				%		 \item all paths from $n$ to $z$ have non-negative weight, and at least one path has a positive weight, and then $S(n)=1$; 
				%		 \item all path from $n$ to $z$ has non-positive weight, and at least one path has a negative weight, and then $S(n)=-1$.
				%		 \item Otherwise $S(n)$ is undefined.
				%	\end{enumerate}
			%\end{definition}	
			%	
			%Notice that $S$ is defined on all nodes $a_i$ with $i \in I$ (it can also be defined trivially for all nodes $b$ as there is a unique path to $z$, and at $z$ also). Hence it is undefined only for nodes $a_j$ with $j \in J$.
			%
			%
			%We denote $a_S$ for any subset $S\subseteq K$ to refer the input vector $\langle a_k\rangle_{a_k\in S}$. We also denote $\boldsymbol{x}_I\oplus \boldsymbol{x}_J = \boldsymbol{x}_K$ and $z=z(\boldsymbol{x}_K)=z(\boldsymbol{x}_I,\boldsymbol{x}_J)$.
			%Consider $\boldsymbol{x}_I^*$ the input vector such that for all $i \in I$, the value for $a_i$ gets its maximal value (if $S(a_i)=1$) or $a_i$ gets its minimal value (if $S(a_i)=-1$).
			%
			%\begin{lemma} \label{lem:reach_max}
			%	$max_a (weight_{a}(z)) = max_{\{a \mid \boldsymbol{x}_I=\boldsymbol{x}^*_I\}} (weight_{a}(z))$
			%	
			%	Further, for every intermediate node $b$ in the hidden layer, for any valuation $\boldsymbol{x}^0_J$, 
			%	we have $max_{\{a \mid \boldsymbol{x}_J=\boldsymbol{x}^0_J\}} (weight_{a}(b)) = max_{\{a \mid \boldsymbol{x}_J=\boldsymbol{x}^0_J,\boldsymbol{x}_I=\boldsymbol{x}_I^*\}} (weight_{a}(b))$.	
			%\end{lemma}
			%
			%\begin{proof}
			%	For the first statement, we use 
			%	$$max_{\boldsymbol{x}_K} (weight_{\boldsymbol{x}_K}(z)) = max_{\boldsymbol{x}_J} max_{\boldsymbol{x}_I} (weight_{\boldsymbol{x}_I,\boldsymbol{x}_J}(z))$$
			%	
			%	Now, for any fixed input $\boldsymbol{x}^0_J$, we can regard $weight_{\boldsymbol{x}_I,\boldsymbol{x}^0_J}(z)$ as a DNN $D'$ with input nodes $a_i\in I$, with all $\boldsymbol{x}^0_J$ %and their propagation in hidden layers 
			%	as bias. In the simplified $D'$, there is no compensating path because of the definition of $I$. Therefore we can apply Theorem \ref{th1} (make a more precise corollary) to get that $z(\cdot,\boldsymbol{x}^0_J)$ reaches its maximal value for $\boldsymbol{x}_I=\boldsymbol{x}_I^*$, and we are done.
			%	
			%	The second statement is simpler because for each fixed partial input $\boldsymbol{x}^0_J$, 
			%	$$b= B^0 +\sum_{a_i\in I} w_{a_i, b} a_i,$$ where $B^0$ is a constant which is the sum of the term including bias and $\boldsymbol{x}^0_J$. Applying Lemma \ref{lemma1}, we obtain the statement.
			%\end{proof}
			%
			%\subsection{MILP abstraction}
			%
			%Consider now an MILP abstraction using the abstraction from \cite{MILP}, 
			%where each variable $\alpha_b$ for the ReLU from $b^-$ to $b^+$ is:
			%\begin{itemize}
			%	\item linear  (i.e. $\alpha_b \in [0,1]$) for $b \in B_{pure}$
			%	\item binary/integer (i.e. $\alpha_b \in \{0,1\}$) for $b \notin B_{pure}$
			%\end{itemize}
			%
			%
			%\begin{lemma}\label{lem:AppB=ReLU}
			%	Let $\mathrm{AppB}$ be the upper bound of lower bound approximation functions of LP for $\ReLU$ function with known upper and lower bound (numbers) $u,l$. Then when $x$ reaches the extremal values (maximum or minimum values, $u$ or $l$), then $\mathrm{AppB}(x)=\ReLU(x)$.
			%	
			%\end{lemma}
			%
			%\begin{proof}
			%	This is simply by definition of LP approximation functions.
			%\end{proof}
			%
			%Notice that there is only one layer of ReLUs, in the hidden layer.
			%Denote by $\mathrm{UB}$ the maximal bound on $z$ considering the MILP constraints above, with ReLUs being either linear or binary depending on whether $b \in B_{pure}$ or not.
			%
			%We want to show that $\mathrm{UB} = \max z$. As the MILP abstraction is a sound overapproximation, 
			%it suffices to show that $\mathrm{UB}\leq \max c$.
			%
			%For any $b_h\in B_{pure}$, we have $b_h=\sum_{a_i\in I}w_{b_ha_i}+B_h$ ($B_h$ is the bias). 
			%Denote by $B_{open}$ the set of nodes $b$ for which $\alpha_b$ is binary, i.e. 
			%$b \notin B_{pure}$. %For any subdomain $D \subseteq B_{open}$, 
			%Let $\boldsymbol{x}^0_J$ and $\boldsymbol{x}^0_I$ be fixed inputs, and consider the associated upper bound 
			%$\mathrm{UB}_{\boldsymbol{x}^0_J,\boldsymbol{x}^0_I}$. We have:
			%%We show that this upper bound does not exceed the maximal value of $z$: 
			%
			%\begin{align*}
			%	\mathrm{UB}_{\boldsymbol{x}^0_J,\boldsymbol{x}^0_I} = B_z + \sum_{b\in B_{open}} w_{b,z}\ReLU(weight_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b)) + \sum_{b\in B_{pure}} w_{b,z} \mathrm{AppB}(weight_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b)).
			%\end{align*} where $\mathrm{AppB}(weight_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b))$ is the upper bound of 
			%the (LP) approximation of $weight_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b)$ if $w_{b,z}>0$, and the lower bound
			%of its (LP) approximation if $w_{b,z}<0$. 
			%
			%By Lemma \ref{lem:reach_max}, for any fixed input $\boldsymbol{x}^0_J$, for all nodes $b$ in the hidden layer, if $S(b)=\mathrm{sign}(w_{zb})=1$, then 
			%$weight_{\boldsymbol{x}_I,\boldsymbol{x}^0_J}(b)$ will get its maximal value for $\boldsymbol{x}_I=\boldsymbol{x}_I^*$,
			%and if $S(b)=\mathrm{sign}(w_{zb})=-1$, then $weight_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b)$ will get its minimal value for $\boldsymbol{x}_I=\boldsymbol{x}_I^*$.
			%Notice that $\mathrm{ReLU}$ and $\mathrm{AppB}$ are both non decreasing functions, and thus:
			%
			%\begin{align*}
			%	\mathrm{UB}_{\boldsymbol{x}^0_J,\boldsymbol{x}^0_I} \leq B_z + \sum_{b\in B_{open}} w_{b,z}\ReLU(weight_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(b)) +
			%	\sum_{b\in B_{pure}} w_{b,z} \mathrm{AppB}(weight_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(b)).
			%\end{align*} 
			%
			%Added FIX:
			%Consider $B_{pure}=B^1 \cup B^2$, with $b \in B^1$ iff
			%$w_{b,z} = 0$ and $b \in B^2$ iff $w_{b,z} \neq 0$.
			%Hence:
			%
			%$$\sum_{b\in B_{pure}} w_{b,z} \mathrm{AppB}(weight_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(b)) = 
			%\sum_{b\in B^2} w_{b,z} \mathrm{AppB}(weight_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(b))$$
			%
			%For $b \in B^2$, we have $w_{a_j,b}=0$ for all $j \in J$
			%by definition of $B_{pure}$ (Lemma 4.), hence
			%
			%$$\sum_{b\in B^2} w_{b,z} \mathrm{AppB}(weight_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(b)) = 
			%\sum_{b\in B^2} w_{b,z} \mathrm{AppB}(weight_{\boldsymbol{x}^*_I}(b))
			%=\sum_{b\in B^2} w_{b,z} \mathrm{ReLU}(weight_{\boldsymbol{x}^*_I}(b))
			%$$ as $\mathrm{AppB}(weight_{\boldsymbol{x}^*_I}(b))=\mathrm{ReLU}(weight_{\boldsymbol{x}^*_I}(b))$ since 
			%$weight_{\boldsymbol{x}^*_I}(b)$ is an extremal value (minimum or maximal) and 
			%$\mathrm{AppB}$ equals $\mathrm{ReLU}$ for extremal values.
			%
			%Hence we have: \begin{align*}
				%	\mathrm{UB}_{\boldsymbol{x}^0_J,\boldsymbol{x}^0_I}  \leq & B_z+\sum_{b}w_{zb}\ReLU(b(\boldsymbol{x}_I^*,\bar{a}_J))
				%	= \max_{\boldsymbol{x}_I} z(\boldsymbol{x}_I,\bar{a}_J) \\
				%	\leq & \max_{\boldsymbol{x}_J}\max_{\boldsymbol{x}_I} z(\boldsymbol{x}_I,\boldsymbol{x}_J) 	= \max_{\boldsymbol{x}_K} weight_{\boldsymbol{x}_K}(z)
				%\end{align*}
				%
				%Considering the maximal over all inputs $\boldsymbol{x}_J,\boldsymbol{x}_I$, we obtain 
				%
				%$$\mathrm{UB} = max_{\boldsymbol{x}_J,\boldsymbol{x}_I} \mathrm{UB}_{\boldsymbol{x}_J,\boldsymbol{x}_I} \leq \max_{\boldsymbol{x}_J,\boldsymbol{x}_I} weight_{\boldsymbol{x}_J,\boldsymbol{x}_I}(z)$$ And this us what we want to show.
				
				
				\section{Proof of Theorem \ref{th2}}
				
				In this section, we will try to prove theorem 2, that is 
				
				\begin{theorem}
					\label{no_diamond_3}
					
					Suppose we have a DNN with $\ReLU$ as the unique activation functions and with a unique output node $z$. We consider compensating pairs in between any two nodes (i.d., the source node or target node can in hidden layers.)
					
					Using MILP method, if every nodes $b$ in any compensating pair
					$(\pi,\pi')$ (except the start node and end node) is encoded as a binary/integer variable (opened), then the upper and lower 
					bounds computed by MILP are the exact max and min value of $z$.
				\end{theorem}
				
				
				Before the proof of above theorem, we will try to weaker one, that is, consider not only compensating pairs, but also pairs that allows common nodes (the two paths in a compensating pairs cannot have a common nodes). We call such pair a weak compensating pair.
				
				
				\begin{definition}
					Let $K$ be the set of  all input nodes $a_k$. 
					We define a decomposition $K=I\sqcup J$ as follows:  
					\begin{itemize}
						\item $a_k \in I$  if
						\begin{enumerate}
							\item every path from $a_k$ to $z$ has weight $\geq 0$, or
							\item every path from $a_k$ to $z$ has weight $\leq 0$.
						\end{enumerate}
						That is, $a_k$ is not a source of a weak compensating pair.
						\item $a_k \in J$ if $a_k \notin I$, that is there exists two paths $\pi,\pi'$ from $k$, 
						one with positive and one with negative weight.
					\end{itemize}
				\end{definition} 
				
				
				
				\begin{lemma} \label{lem:open_node_2}
					A node $b$ in the hidden layer will not be on a weak compensating pair if and only if one of the following two happens:
					\begin{enumerate}
						\item Any path from $b$ to the output node $z$ has a weight $0$, and we use $w_{b,z}=0$ to denote this case, or
						\item For every input node $a_j\in J$, for any path from $a_j$ to $b$ has a weight of $0$. We use $w_{a_j,b}=0$ to denote this case.
					\end{enumerate}
					We denote $B_{zero}$ the set of such nodes $b$ that case 1 holds, and denote $B_{pure}$ the of such nodes $b$ that case 2 holds but case 1 does not hold.
					
					We denote $B_{open}$ the set of all nodes $b$ in a compensating path.
				\end{lemma}
				
				\begin{proof}
					First, we show that if one of case 1, 2 happens, then $b$ will not be on a weak compensating pair. If 1), it is obvious. For 2, we reason by contradiction: assume there is a pair of weak compensating paths 	$(\pi,\pi')$ starting with $a$, with $b$ in $\pi$ and weight$(\pi) > 0$. It means that $a \in J$. A contradiction as 2) $w_{a_j,b}=0$ implies weight$(\pi)=0$.
					
					Second, we show that if neither 1 nor 2 hold, then $b$ will be on a weak compensating path.
					Because 2) does not hold, there is a $j \in J$ with $w_{a_j,b} \neq 0$, say $>0$.
					Because 1) does not hold, $w_{b,z} \neq 0$, say $>0$.
					Now, by definition of $J$, there is a pair of weak compensating paths $\pi,\pi'$ 
					from $a_j$ to $z$, say with $\pi'$ with weight $<0$.
					Then $((a_j,b,z), \pi')$ is also a weak compensating pair.
				\end{proof}
				
				\begin{lemma}\label{lem:subnetwork}
					$B_{pure}$ and $I$ can form a sub-network: every node in $B_{pure}\cup I$ only replies on nodes in $B_{pure}\cup I$ in previous layers. We call this sub-network $N_I$. In $N_I$, there is no compensating pair.
				\end{lemma}
				
				\begin{proof}
					This is simply because for a node $c\in B_{pure}$ in a hidden layer or the output layer, for a node $b$ in one layer before, if $b\notin B_{pure}\cup I$, then:
					
					1. if $b\in J$, by definition, $w_{bc}=0$.
					
					2. if $b\in B_{O}$, then $w_{bc}=0$; otherwise there exists $<a_j,b,c>$ a path from $a_j\in J$ to $c$ with a nonzero value, a contradiction.
					
					3. if $b\in B_{zero}$, then $w_{bc}=0$; otherwise there exists $<b,c,z>$ a path from $b$ to $z$ the output node with a nonzero value, a contradiction.
				\end{proof}
				
				
				
				Consider the sign of nodes function from Definition \ref{sign_of_nodes}. Because now we allow compensating paths, we cannot define this function over all nodes.
				
				
				
				
				\begin{definition}\label{sign_of_nodes_in_I_2}
					We define a partial sign function $S$ over nodes $n$ such that : 	
					\begin{enumerate} 
						\item all paths from $n$ to $z$ have 0 weight, and then $S(n)=0$; 
						\item all paths from $n$ to $z$ have non-negative weight, and at least one path has a positive weight, and then $S(n)=1$; 
						\item all path from $n$ to $z$ has non-positive weight, and at least one path has a negative weight, and then $S(n)=-1$.
						\item Otherwise $S(n)$ is undefined.
					\end{enumerate}
				\end{definition}	
				
				Notice that $S$ is defined on all nodes $a_i \in I$ (it can also be defined trivially for all nodes $b$ as there is a unique path to $z$, and at $z$ also). And similarly, for nodes in  $B_{zero}$, $S$ is defined. We introduce the following lemma:
				
				\begin{lemma}\label{lem:sign}
					For a node $b$, if there exists at least one path from a node $a_i\in I$ with nonzero weight, then  $S$ is defined on $b$.
				\end{lemma}
				
				\begin{proof}
					Otherwise, suppose there exists two paths $P_1,P_2$ from $b$ to $z$ with (strictly) different signs. Then consider $\langle a_i,b\rangle+P_1$ and $\langle a_i,b\rangle+P_2$, they are two paths with weights with different signs. This is a contradiction since $a_i\in I$.
				\end{proof}
				Therefore, for any node $b$  that $S$ is undefined on $b$, it only depends on the values on $J$.  
				
				\begin{lemma}
					If $c$ is in the next layer of $b$, $W_{bc}\neq 0$, and $S$ is defined on both $b$ and $c$ and $S(c)\neq 0$, then $W_{bc}S(c)=S(b)$.
				\end{lemma}
				
				\begin{proof}
					This is proved in the first section.
				\end{proof}
				
				\begin{lemma}\label{lem:subnetwork2}
					The input set $J$ with nodes that only depending on $J$ forms a sub-network. This network contains all nodes that $S$ is not defined on them. We call this sub-network $N_J$.
				\end{lemma}
				
				\begin{proof}
					This is because if there exists nodes $b,c$ such that $W_{bc}\neq 0$ and $c$ is in above set but $b$ is not, then $b$ has a path with nonzero weight from a node $a_i$ not in $J$ but this means so does $c$, a contradiction. Therefore any $b$ such that $W_{bc}\neq 0$ is also in above set.
				\end{proof}
				
				By definition, for any $b\in N_J$, either $b\in B_{open}$, or $b$ is a constant node.
				
				We denote $\boldsymbol{x}_S$ for any subset $S\subseteq K$ to refer the input vector $\langle x_{a_k}\rangle_{a_k\in S}$. We also denote $\boldsymbol{x}_I\oplus \boldsymbol{x}_J = \boldsymbol{x}_K$ and $\Val(z)=\Val_{\boldsymbol{x}_K}(z)=\Val_{\boldsymbol{x}_I,\boldsymbol{x}_J} (z)$. We can see that for a node $b$ that $S$ is not defined on it, if $\boldsymbol{x}_J$ is fixed, then the value of $b$ becomes a constant.
				
				We define $\boldsymbol{x}_I^*$ to be the input vector such that for all $a_i \in I$, the value $x_{a_i}$ for $a_i$ gets its maximal value (if $S(a_i)=1$ or $0$) or $a_i$ gets its minimal value (if $S(a_i)=-1$). Similarly, we define $\boldsymbol{x}_I^\#$ to be the input vector such that for all $a_i \in I$, the value $x_{a_i}$ for $a_i$ gets its maximal value (if $S(a_i)=-1$ or $0$) or $a_i$ gets its minimal value (if $S(a_i)=1$).
				
				
				
				\begin{lemma} \label{lem:reach_max_2}
					
					Assume $b$ is the output node or a node in a hidden layer that $S(b)=1$. Then
					$\max_{\boldsymbol{x}} (\Val_{\boldsymbol{x}}(b)) =\max_{\{\boldsymbol{x} \mid \boldsymbol{x}_I=\boldsymbol{x}^*_I\}} (\Val_{\boldsymbol{x}}(b))$.
					
					Further,  for any valuation $\boldsymbol{x}^0_J$, 
					we have $$\max_{\{\boldsymbol{x} \mid \boldsymbol{x}_J=\boldsymbol{x}^0_J\}} (\Val_{\boldsymbol{x}}(b)) =  \Val_{\boldsymbol{x}^0_J,\boldsymbol{x}_I^*}(b).$$
					
					If $S(b)=-1$, then the similar results hold, replacing $\boldsymbol{x}^*_I$ by $\boldsymbol{x}^\#_I$. 
				\end{lemma}
				
				\begin{proof}
					For the first statement, we use 
					$$\max_{\boldsymbol{x}_K} (\Val_{\boldsymbol{x}_K}(z)) =\max_{\boldsymbol{x}_J} \max_{\boldsymbol{x}_I} (\Val_{\boldsymbol{x}_I,\boldsymbol{x}_J}(z))$$
					
					Now, for any fixed input vector $\boldsymbol{x}^0_J$, we can regard $\Val_{\boldsymbol{x}_I,\boldsymbol{x}^0_J}(z)$ as a DNN $D'$ with input nodes $a_i\in I$, with all $\boldsymbol{x}^0_J$ %and their propagation in hidden layers 
					as bias. In the simplified $D'$, there is no compensating path because of the definition of $I$. Therefore we can apply Theorem \ref{th1} to get that $z(\cdot,\boldsymbol{x}^0_J)$ reaches its maximal value for $\boldsymbol{x}_I=\boldsymbol{x}_I^*$, and we are done.
				\end{proof}
				
				
				\subsection{MILP abstraction}
				
				Consider now an MILP abstraction using the abstraction from \cite{MILP}, 
				where each variable $\alpha_b$ for the ReLU from $b$ to $\hat{b}$ is:
				\begin{itemize}
					\item linear  (i.e. $\alpha_b \in [0,1]$) for $b \notin B_{O}$
					\item binary/integer (i.e. $\alpha_b \in \{0,1\}$) for $b \in B_{O}$
				\end{itemize}
				
				Now consider the MILP framework for all hidden and output nodes layer by layer (to compute their bounds, and use the bounds of previous layers to build the MILP models of current layer), with ReLUs being either linear or binary depending on whether $b \in B_{O}$ or not. We call this standard framework.
				
				For a node $b$ in a hidden layer or the output layer, we denote by $\UB(b)$ the maximal bound on $b$ and $\LB(b)$ the minimal bound on $b$ considering the MILP constraints above.
				
				
				\begin{lemma}
					In the standard framework, both $N_I$ and $N_J$ are accurate in the sense that for any $b$ in $N_I$ or $N_J$, $\UB(b)=\max\Val(b)$ and $\LB(b)=\min\Val(b)$.
				\end{lemma}
				
				\begin{proof}
					For $N_I$, it is because of no compensating as proved in the first section. For $N_J$, it is because all nodes are either opened or constants.
				\end{proof}
				
				
				%\begin{definition} Assume $\boldsymbol{x}^0_I\oplus \boldsymbol{x}^0_J$ is a input vector, and $b$ is node in hidden layers or output layer.
				%
				%	1. For a given vector $\boldsymbol{x}$ , $\B_{\boldsymbol{x}^0_I, \boldsymbol{x}^0_J, \boldsymbol{x}}(b)$ is the value of $b$ in the MILP model for values $\boldsymbol{x}^0_I, \boldsymbol{x}^0_J, \boldsymbol{x}$.
				%\end{definition}
				
				
				For input vectors $\boldsymbol{x}^0_I,\boldsymbol{x}^0_J$, let $\UB_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b)$ be the upper bound in the above fixed formulation and framework while the input $\boldsymbol{x}^0_I,\boldsymbol{x}^0_J$ is fixed: that is, besides the constraints in above MILP models, we add new constraints $\boldsymbol{x}_I,\boldsymbol{x}_J=\boldsymbol{x}^0_I,\boldsymbol{x}^0_J$. We  define $\LB_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b)$ similarly for $\LB$ lower bound. For them, we have the following lemma:
				
				\begin{lemma} In an MILP formulation, for a node $c$ in a hidden layer or output layer:
					
					1. $\UB(b)=\max_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}\UB_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(c)$. 
					
					2. $\LB(b)=\min_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}\LB_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(c)$. 
				\end{lemma}
				
				\begin{proof}
					This is by the definition of MILP formulation and $\UB_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}, \LB_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}$.
				\end{proof}
				
				%\begin{lemma}\label{lem:pure_node}
				%	For a node $c\in B_{pure}$, $\UB_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}$ (and $\LB$ the same) only depends on $\boldsymbol{x}^0_I$.
				%\end{lemma}
				%
				%\begin{proof}
				%	We prove this lemma by induction. For the first hidden layer, this is trivial.
				%	
				%	Suppose for layer up to $l_i$, this lemma is true, we show it for layer $l_{i+1}$. For a node $c$ in layer $l^{i+1}$, we have that: \begin{align*}
					%		\UB_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(c) \leq B_c + \sum_{b\in B_{open}\cap l_i} w_{b,c}\ReLU(\B_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b))\\
					%		 + \sum_{b\in B_{pure}\cap l_{i}} w_{b,c} \mathrm{AppB}(\B_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b))\\
					%		 +\sum_{b\in B_{zero}\cap l_{i}} w_{b,c} \mathrm{AppB}(\B_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b)).
					%	\end{align*} Here, $\B$ means one of $\UB$ or $\LB$ depending on the weight/coefficient.
				%	
				%	
				%	But since $c\in B_{pure}$, for any $b\in B_{open}$, we must have $w_{b,c}=0$. And similarly, for any $b\in B_{zero}$, $w_{bc}=0$. Therefore, we will have:
				%	
				%	\begin{align*}
					%		\UB_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(c) \leq B_c + \sum_{b\in B_{pure}\cap l_{i}} w_{b,c} \mathrm{AppB}(\B_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b)).
					%	\end{align*} By induction, for any $b\in B_{pure}\cap l^i$, it only depends on $\boldsymbol{x}^0_I$.  Therefore, we will have:
				%	
				%	\begin{align*}
					%		\UB_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(c) = B_c + \sum_{b\in B_{pure}\cap l_{i}} w_{b,c} \mathrm{AppB}(B_{\boldsymbol{x}^0_I}(b)).
					%	\end{align*} So, $UB_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(c)$ only depends on $\boldsymbol{x}_I^0$.
				%\end{proof}
				
				
				We want to show that $\mathrm{UB}(z) = \max \Val (z)$ and $\mathrm{LB}(z) = \min \Val(z)$ for the unique output nodes and we will prove this for all $c\notin B_{zero}$ . As the MILP abstraction is a sound overapproximation, 
				it suffices to show that $\mathrm{UB}\leq \max \Val(c)$ and $\mathrm{LB} \geq \min \Val(c)$. We will prove this by induction on layers from the first hidden layer to the output layer. More specifically, we will prove the following lemmas:
				
				
				\begin{lemma}
					For the $i$-th layer $l^i$ ($1$-th layer is the first hidden layer), for a node $c\in l^i$, if $c\notin B_{zero}$, then:
					
					\vspace*{1ex}
					
					1. for any fixed $\boldsymbol{x}^0_J$ and for $\boldsymbol{x}^*_I$, the value for $c$ in the MILP model is a fixed number, and naturally $\UB_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(c)=\LB_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(c)$. The same is true for $\boldsymbol{x}^\#_I$.
					
					\vspace*{1ex}
					
					2. for any fixed $\boldsymbol{x}^0_J$, if $S(c)=1$,  $\max_{\boldsymbol{x}_I} \UB_{\boldsymbol{x}_I,\boldsymbol{x}^0_J}(c)=\UB_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(c)= \Val_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(c) = \max_{\boldsymbol{x}_I} \Val_{\boldsymbol{x}_I,\boldsymbol{x}^0_J}(c)$. The similar for $S(c)=-1$, $\langle \LB,\min\rangle$, $x^\#_I$: we can change two of them at the same time.
					
					\vspace*{1ex}
					
					3. Therefore, $\UB(c)=\max\Val(c)$ and $\LB(c)=\min\Val(c)$.
					
					
				\end{lemma}
				
				\begin{proof}
					We prove this lemma by induction on layers. For $l_1$, it is obvious. Suppose we have proved part 1,2,3 up to layer $l_i$, then for layer $l_{i+1}$, suppose $c$ is a node in this layer. 
					
					(1)	First, if $S$ is undefined on $c$, then by previous discussion, $c$ is in a sub-network from $J$ with all nodes opened and if $\boldsymbol{x}_J=\boldsymbol{x}_J^0$ is fixed, then in the MILP model, the value for $c$ is also fixed. If $S(c)$ is defined, then consider all $b$ such that $W_{bc}\neq 0$. First, $b$ cannot in $B_{zero}$; so by induction hypothesis, the value of $b$ is fixed under $\boldsymbol{x}^*_I,\boldsymbol{x}^0_J$ in MILP model. 
					
					
					
					If $b\in B_{open}$, the term is $W_{bc}\ReLU(b)$. We know then by induction hypothesis, the value for $b$ in MILP is a fixed value. Then the term $W_{bc}\ReLU(b)$ is also a fixed number.  
					
					If $b\in B_{pure}$, the term is $W_{bc}\mathrm{AppB}(b)$ ($\mathrm{AppB}$ is the LP approximation domain of $\ReLU$) and either $b$ is a constant in the network and so is in the MILP model (trivial case), or $S(b)$ is defined. 
					
					We know then by induction hypothesis part 1, 2 and 3,  we have the value of $b$ in the model is fixed (part 1), and it is equal to $\max_{\boldsymbol{x}_I} \Val(\boldsymbol{x}_I,\boldsymbol{x}^0_J)(c)$ or $\min_{\boldsymbol{x}_I} \Val(\boldsymbol{x}_I,\boldsymbol{x}^0_J)(c)$ (by part 2, depending on $S(b)$). Since $b$ only depends on $I$, this value also equal to $\max_{\boldsymbol{x}_I} \Val(\boldsymbol{x}_I)(c)$ or $\min_{\boldsymbol{x}_I} \Val(\boldsymbol{x}_I)(c)$, and therefor equal to $\UB(b)$ or $\LB(b)$. For either of them, $\mathrm{AppB}$ will be fixed value. Hence, in this case, the term $W_{bc}\mathrm{AppB}(b)$ is also a fixed number.
					
					Therefore, in the MILP model, $c$ is a sum of terms that are all fixed numbers. Hence $c$ is a fixed number.
					
					(2) If $S(c)=1$, then we have the following:	\begin{align*}
						\mathrm{UB}_{\boldsymbol{x}^0_J,\boldsymbol{x}^0_I}(c) \leq constant + \sum_{b\in B_{open}\cap l_i} W_{b,c}\ReLU(\B_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b)) + \sum_{b\in B_{pure}\cap l_{i}} W_{b,c} \mathrm{AppB}(\B_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b)),
					\end{align*}where $\B$ means one of $\UB$ or $\LB$ depending on the sign of $W_{bc}$. Now by induction hypothesis part 2, combining with $S(b)$ and both $\ReLU$ and $\mathrm{AppB}$ are non-decreasing functions, we have:\begin{align*}
						\mathrm{UB}_{\boldsymbol{x}^0_J,\boldsymbol{x}^0_I}(c)\leq 
						&constant + \sum_{b\in B_{open}\cap l_i} W_{b,c}\ReLU(\B_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(b)) + \sum_{b\in B_{pure}\cap l_{i}} W_{b,c} \mathrm{AppB}(\B_{\boldsymbol{x}^*_I}(b))\\
						= & \mathrm{UB}_{\boldsymbol{x}^0_J,\boldsymbol{x}^*_I}(c). 
					\end{align*}  This is the first equal.
					
					The second is simply because $\UB_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(c)\geq \Val_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(c)\geq\LB_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(c)$.	The third equal is proved in last subsection, Lemma \ref{lem:reach_max_2}. $S(c)=-1$ is similar. So we have proved part 2.
					
					(3) Once we have part 2, then we have that for any fixed $\boldsymbol{x}^0_J$, $\max_{\boldsymbol{x}_I} \UB_{\boldsymbol{x}_I,\boldsymbol{x}^0_J}(c)= \max_{\boldsymbol{x}_I} \Val(\boldsymbol{x}_I,\boldsymbol{x}^0_J)(c)$. Therefore, $\max_{\boldsymbol{x}_I,\boldsymbol{x}_J} \UB_{\boldsymbol{x}_I,\boldsymbol{x}_J}(c)= \max_{\boldsymbol{x}_I,\boldsymbol{x}_J} \Val(\boldsymbol{x}_I,\boldsymbol{x}_J)(c)$ and this is to say $\UB(c)=\max(c)$. This is what we want to show, and the same result holds for lower bound and minimal value. This is for part 3.
					
					
					
					
				\end{proof}
				
				
				
				%\begin{lemma}
				%	For the $i$-th layer $l^i$ ($1$-th layer is the first hidden layer), for a node $c\in l^i$, if $c\notin B_{zero}$ and $S(c)=1$, then:
				%	
				%	 1. if , then for any fixed $\boldsymbol{x}^0_J$, $\max_{\boldsymbol{x}_I} \UB_{\boldsymbol{x}_I,\boldsymbol{x}^0_J}(c)=\UB_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(c)= \Val(\boldsymbol{x}^*_I,\boldsymbol{x}^0_J)(c) = \max_{\boldsymbol{x}_I} \Val(\boldsymbol{x}_I,\boldsymbol{x}^0_J)(c)$, where $\boldsymbol{x}^*_I$ is described in previous section. More important, when $\boldsymbol{x}_I=\boldsymbol{x}^*_I$, for all node $c$ in the same layer, the approximation of $c$ is a constant number rather than an interval, and it is equal to $\UB_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(c)$ or $\LB_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(c)$.
				%	
				%The similar is true for lower bound $\LB$.
				%\end{lemma}
				
				
				
				%\vspace*{20ex}
				%\begin{proof}
				%	
				%	For $i= 1$, we have proved in previous section.
				%	
				%	Suppose we have proved this lemma up to layer $i$, and we are going to prove it for $i+1$-th layer. Let $c\notin B_{zero}$ be a node on $i+1$-th layer.
				%	
				%	By definition, we have that for any input vector $\boldsymbol{x}^0_I,\boldsymbol{x}^0_J$:
				%	
				%	\begin{align*}
					%		\mathrm{UB}_{\boldsymbol{x}^0_J,\boldsymbol{x}^0_I}(c) \leq B_c + \sum_{b\in B_{open}\cap l_i} w_{b,c}\ReLU(\B_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b)) + \sum_{b\in B_{pure}\cap l_{i}} w_{b,c} \mathrm{AppB}(\B_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b)).
					%	\end{align*} where $\mathrm{AppB}$ is the same as in previous section  whose definition is independent with $\boldsymbol{x}^0_I,\boldsymbol{x}^0_J$. $\B$ is one of $\UB,\LB$  that, if $w_{b,c}>0$, then $\B$ is $\UB$ and otherwise $\B$ is $\LB$.
				%	
				%	We can simplify above equation to \begin{align*}
					%			\mathrm{UB}_{\boldsymbol{x}^0_J,\boldsymbol{x}^0_I}(c) \leq B_c + \sum_{b\in B_{open}\cap l_i} w_{b,c}\ReLU(\B_{\boldsymbol{x}^0_I,\boldsymbol{x}^0_J}(b)) + \sum_{b\in B_{pure}\cap l_{i}} w_{b,c} \mathrm{AppB}(\B_{\boldsymbol{x}^0_I}(b)).
					%	\end{align*}
				%	
				%	By induction hypothesis, since both $\ReLU$ and $\mathrm{AppB}$ are non-decreasing functions, we have that \begin{align*}
					%		\mathrm{UB}_{\boldsymbol{x}^0_J,\boldsymbol{x}^0_I}(c)\leq 
					%		 &B_c + \sum_{b\in B_{open}\cap l_i} w_{b,c}\ReLU(\B_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(b)) + \sum_{b\in B_{pure}\cap l_{i}\wedge w_{b,c}\neq 0} w_{b,c} \mathrm{AppB}(\B_{\boldsymbol{x}^*_I}(b))\\
					%		= & \mathrm{UB}_{\boldsymbol{x}^0_J,\boldsymbol{x}^*_I}(c) \\
					%	\end{align*} The equal is because by induction hypothesis, when $\boldsymbol{x}_I=\boldsymbol{x}_I^*$, the approximation is a constant and is equal to $\B_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(b)$.
				%	
				%Now, since when $x$ reaches the max or min of $\mathrm{AppB}$'s domain, $\mathrm{AppB}(x)=\ReLU(x)$, and by induction hypothesis, for any $b\in B_{pure}\cap l_i$, $\mathrm{AppB}(B_{\boldsymbol{x}^*_I}(b))=\ReLU(B_{\boldsymbol{x}^*_I})$. Hence, by induction hypothesis, we will have \begin{align*}
					%	\mathrm{UB}_{\boldsymbol{x}^0_J,\boldsymbol{x}^0_I}(c)\leq & \ \mathrm{UB}_{\boldsymbol{x}^0_J,\boldsymbol{x}^*_I}(c) \\
					%	= &B_c + \sum_{b\in B_{open}\cap l_i} w_{b,c}\ReLU(B_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(b)) + \sum_{b\in B_{pure}\cap l_{i}\wedge w_{b,c}\neq 0} w_{b,c} \mathrm{AppB}(B_{\boldsymbol{x}^*_I}(b))\\
					%	= &B_c + \sum_{b\in B_{open}\cap l_i} w_{b,c}\ReLU(weight_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(b)) + \sum_{b\in B_{pure}\cap l_{i}} w_{b,c} \mathrm{ReLU}(weight_{\boldsymbol{x}^*_I}(b))\\
					%	= & \Val_{\boldsymbol{x}^*_I,\boldsymbol{x}^0_J}(c).
					%\end{align*} This finishes the proof.
					%	\end{proof}
				%
				%
				%Now, we have that for any fixed $\boldsymbol{x}^0_J$, $\max_{\boldsymbol{x}_I} \UB_{\boldsymbol{x}_I,\boldsymbol{x}^0_J}(c)= \max_{\boldsymbol{x}_I} \Val(\boldsymbol{x}_I,\boldsymbol{x}^0_J)(c)$. Therefore, $\max_{\boldsymbol{x}_I,\boldsymbol{x}_J} \UB_{\boldsymbol{x}_I,\boldsymbol{x}_J}(c)= \max_{\boldsymbol{x}_I,\boldsymbol{x}_J} \Val(\boldsymbol{x}_I,\boldsymbol{x}_J)(c)$ and this is to say $\UB(c)=\max(c)$. This is what we want to show, and the same result holds for lower bound and minimal value.
				

			
			
			
			
			
			
			
			
			
			
			
			
			
			
 