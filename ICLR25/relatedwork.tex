\subsection{Related Work} 

We compare {\CMP} with major verification tools for DNNs to clarify our methodology and its distinction from the existing state-of-the-art. CMP scales while preserving good accuracy, through targeting a limited number of binary variables based on compensating strength, stricking a good balance between exact encoding of a DNN using MILP~\cite{MILP} (too slow) and LP relaxation (too inaccurate). MIPplanet~\cite{MIPplanet} opts for a different selection of binary variables, and execute one large MILP encoding instead of CMP's many small encodings, which significantly reduce the number of binary variables necessary for each encoding. In \cite{9211410}, small encodings are also considered, however with a straightforward choice of binary nodes based on the weight of outgoing edges, which need much more integer variables (thus runtime) to reach the same accuracy as our utility function (see Table \ref{tab:example1}), and restrict integer variables to the previous layer, which limits the accuracy reachable.

Hybrid MILP can be seen as a refinement of $\alpha,\beta$-Crown~\cite{crown}, though its refined accurate path is vastly different than the base Branch and Bound technique used in 
$\alpha,\beta$ CROWN, BaBSR \cite{BaB} and MN-BaB \cite{ferrari2022complete}, which call BaB once per output neuron. In the worst case, this involves considering all possible ReLU configurations, though branch and bound typically circumvents most possibilities. In simple networks, like those trained robustly, branch and bound is highly efficient, focusing on branches crucial for verifying the actual property. However, branch and bound hits a complexity barrier when verifying harder instances, due to an overwhelming number of branches, as displayed in Table \ref{table_beta}. This is not the csae of Hybrid MILP, see Table \ref{table_hybrid}, which is much more accurate than $\alpha,\beta$-Crown.
That shortcoming for hard instances was witnessed in \cite{crown}, and a very specific solution using the full MILP encoding for the first few layers of a DNN was drafted, following similar proposal \cite{MILP2}. There are two key issues with this techniques: it is slow, as every neurons up to the layer are encoded using an integer variable, and it cannot scale to DNN with many neurons, making it not that accurate for intermediate networks (e.g. 9x100,9x200, Table \ref{table_hybrid}), and not usable for larger DNNs (6x500, CNN-B-Adv), whereas Hybrid MILP does scale.


Last, ERAN-DeepPoly \cite{deeppoly} computes bounds on values very quickly, by abstracting the weight of every node using two functions: an upper function and a lower function. While the upper function is fixed, the lower function offers two choices.
It relates to the LP encoding through the following new (to our knowledge) insight:  Proposition \ref{LP} state that the LP relaxation precisely matches the intersection of these two choices. Consequently, LP is more accurate (but slower) than DeepPoly, and Hybrid MILP is considerably more precise. Regarding PRIMA \cite{prima}, the approach involves explicitly maintaining dependencies between neurons, calculating bounds layer by layer.


%{\color{red} MN-BaB \cite{ferrari2022complete} is another state-of-the-art verifier which can be regarded as a development of PRIMA. As indicated by its name, it uses a combination technique of multi-neuron constraints and Branch and Bound. According to their experiments results, MN-BaB has similar speed and accuracy as $\alpha$,$\beta$ CROWN. Therefore, we do not compare our experiments results to MN-BaB separately.}

%For verification-agnostic DNNs that are not too large,  $\beta$-CROWN (and PRIMA) resorts to a {\em refined} path \cite{MILP2}, where the bounds {\em on the first few layers} are refined using an exact MILP encoding. In {\CMP}, we do not use an exact encoding but a partial one with the most important ReLU nodes obtained by considering the compensation strength. As it is more efficient, we compute bounds for neurons in {\em all} the layers. This would be infeasible without the selection based on the compensation. The refined version of $\beta$-CROWN is particularly accurate on small DNNs. As the depth grows, the more work is left to BaB and \CMP is more accurate (Table \ref{tab:example}).

Finally, methods such as Reluplex / Marabou \cite{Reluplex,katz2019marabou}  abstract the network: they diverge significantly from those abstracting values such as PRIMA, $\alpha,\beta$-CROWN)\cite{prima,crown}, Hybrid MILP. 
These network-abstraction algorithms are designed to be {\em complete} but completeness comes at the price of significant scalability challenges, and in practice they time-out on hard instances as shown in Table \ref{table_complete}.