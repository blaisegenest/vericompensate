\section*{Appendix}

\subsection*{Setting for Hybrid MILP}

Hybird MILP first call $\alpha,\beta$-Crown with short time-out (TO), then call partial MILP on those inputs which was neither certified nor falsified by this run of $\alpha,\beta$-Crown. We are using two settings of TO, for smaller DNNs we use T0$=10s$, and for the two larger ones, we use TO$=30s$.

The setting for partial MILP for fully-connected DNNs is about how many neurons need to be opened (once set, the selection is automatic). The runtime depending crucially upon the number of open ReLU neurons, we set it quite tightly, only allowing few neuron deviation to accommodate to a particularly accurate/inaccurate bound computation (measure by the weight of the remaining Utility function). As complexity increases with the layer considered, as the size of the MILP model grows, we lower this number with the depth, only committing to an intermediate number for the output neuron (the number of output neurons  is smaller than hidden layer, and this is the most important computation). We experimentally set this number so that each computing the bounds in each hidden layer takes around the same time. Remember that in layer 1, partial MILP is not necessary and propagating bounds using interval arithmetic is already exact. We open [48,48] to compute bounds for hidden layer 2, [21,24] for layer 3, [11,14] for layer 4, [6,9] for layer 5, [3,6] for layer 6, [2,5] for layer 7, [1,4] for hidden layer 8 (if any), and we open [14,17] for the output layer.

For convolutional CNNs, the strategy is adapted, as there is much more neurons, but in a shallower architecture and not fully connected. 
The second layer is computed accurately, opening 200 neurons, which is manageable as there is only one ReLU layer to consider, and accuracy here is crucial.
We do not open any nodes in the third layer (the first fully connected layer) if the output layer is the next one (which is the case for CNN-B-Adv), and instead rely on the choice of important nodes for the output layer. Otherwise, we open 20 neurons.
In the output layer, we open at least 45 neurons (there is less output neurons than nodes in the previous layer), and enlarge the number of open neurons (up to 300) till we find an upper bound, that is a best current MILP solution, of around +0.1 (this 0.1 was experimentally set as target, a good balance between accuracy and efficiency), and compute a guaranteed lower bound (the goal is to guarantee the bound is $>0$).

In Table \ref{table20}, we sum-up the TO and minimum open numbers for each DNN considered.
\begin{table}[h!]
	\centering
	\begin{tabular}{||l||c|c||}
		\hline \hline
		Network & TO for $\alpha,\beta$-Crown  & Minimum number of Open neurons  \\ 		  
		\hline
		MNIST $5 \times 100$ & 10s  & 48,21,11,6,14  \\ \hline
		MNIST $5 \times 200$ & 10s & 48,21,11,6,14  \\ \hline
		MNIST $8 \times 100$ & 10s  & 48,21,11,6,3,2,1,14  \\ \hline
		MNIST $8 \times 200$ & 10s & 48,21,11,6,3,2,1,14  \\ \hline
		MNIST $6 \times 500$ & 30s & 48,21,11,6,3,14 \\ \hline
		CIFAR CNN-B-adv & 30s & 200, 0, 45 \\ \hline \hline
	\end{tabular}
	\caption{Settings of Hybrid MILP for the different {\em hard} instances}
	\label{table20}
	\end{table}


$\alpha,\beta$-Crown uses massively parallel ($>4096$ threads) GPU, while Partial MILP uses 20 CPU-threads.

Notice that a different balance between accuracy and runtime could be set. For instance, we set up the numbers of open neurons to have similar runtime as Refined $\beta$-Crown for the first 4 DNNs ($50s-100s$). We could easily target better accuracy (e.g. for $8 \times 100$ with a relatively high $15\%$ undecided images) by increasing the number of open neurons, with a trade-off on runtime (current runtime is at $61s$).
By comparison, the sweet spot for $\alpha,\beta$-Crown seems to be around TO$=30s$, enlarging the time-out having very little impact on accuracy but large impact on runtime
(see Table \ref{table_beta}).

\medskip
In the following, we provide results comparing $\alpha,\beta$-Crown to other verifiers, to justify our use of $\alpha,\beta$-Crown as state of the art for efficient verifiers as main source of comparison to Hybrid MILP for hard DNN instance.

\newpage 

\subsection*{Comparison $\alpha,\beta$-Crown vs PRIMA}

PRIMA \cite{prima} is a major verifier in the ERAN toolkit. In Table \ref{table9}, we report the comparison between PRIMA and $\alpha,\beta$-Crown, mainly from \cite{crown}. The setting is mainly similar from ours, but numbers are not perfectly comparable as the images tested are not  exactly the same (1000 first or 200 first images for CNN-B-Adv), vs 100 first in Tables \ref{table_hybrid}, \ref{table_beta}. Also, time-out settings and hardware are slightly different. The overall picture is anyway the same.

\begin{table}[h!]
	\centering
	\begin{tabular}{||l||c|c||c||}
		\hline \hline
		Network & $\alpha,\beta$-Crown & Refined $\beta$-Crown & PRIMA \\ 		  
		\hline
		MNIST $5 \times 100$ & N/A  & 14.3\% (102s) & 33.2\% (159s)\\ \hline
		MNIST $5 \times 200$ & N/A & 13.7\% (86s) & 21.1\% (224s) \\ \hline
		MNIST $8 \times 100$ & N/A  & 20.0\% (103s) & 39.2\% (301s)   \\ \hline
		MNIST $8 \times 200$ & N/A & 17.6\% (95s) & 28.7\% (395s)  \\ \hline
		MNIST $6 \times 500$ & 51\% (16s) & $-$ & 64\% (117s) \\ \hline
		CIFAR CNN-B-adv & 18.5\% (32s) & $-$ & 27\% (344s)\\ \hline \hline
		CIFAR ResNet & 0\% (2s) & $-$ & 0\% (2s) \\ \hline \hline
	\end{tabular}
	\caption{Undecided images ($\%$, {\em lower is better}), as computed by $\alpha,\beta$-Crown, Refined $\beta$-Crown, and PRIMA, as reported in \cite{crown}, except for $6 \times 500$ that we run ourselves. N/A means that \cite{crown} did not report the numbers, while $-$ means that Refined $\beta$-Crown cannot be run on these DNNs.}
	\label{table9}
	%\begin{tablenotes}
	%	\footnotesize
%		\item Most data is directly from \cite{crown}. N/A means no data either in \cite{crown} or by our running.
%		\item  $^*$ The data in this row is from our own running on first 100 images of the MNIST dataset.
%		\item  $^{**}$ The data is from \cite{crown} on first 200 images of the CIFAR10 dataset.
%	\end{tablenotes}
	\end{table}

Analysis: On the 4 smallest MNIST networks, PRIMA uses a refined path comparable with Refined $\beta$-Crown. However, it is slower and less accurate than Refined $\beta$-Crown.
On larger {\em hard} networks, PRIMA has also more undecided images than $\alpha,\beta$-Crown, while the runtime is $>5$ times larger.
Hence, Hybrid MILP is more accurate than Prima with similar runtime or faster.

Notice that kPoly \cite{kpoly}, OptC2V \cite{optC2V}, SDP-FO \cite{SDPFI} numbers were also reported in \cite{crown} on these networks, with even more unfavorable results.

\subsection*{Comparison $\alpha,\beta$-Crown vs MN-BaB}

MN-BaB \cite{ferrari2022complete} is an improvement built over PRIMA, using a similar Branch and Bound technique as used in $\alpha,\beta$-Crown. Results in \cite{ferrari2022complete}
are close to those of $\alpha,\beta$-Crown. However, none of the {\em hard} networks from \cite{crown} that we consider have been tested. We thus tested three representative {\em hard} DNNs (first 100 images) to understand how MN-BaB fairs on such hard instances, and report the numbers in Table \ref{table10}. Results are directly comparable with Table \ref{table_hybrid}.


\begin{table}[h!]
	\centering
	\begin{tabular}{||l||c|c||c|c||}
		\hline \hline
		 & $\alpha,\beta$-Crown & $\alpha,\beta$-Crown & MN-BaB & MN-BaB \\ 
		 Network & TO=30s & TO=2000s &  TO=30s & TO=2000s \\ 
		\hline
		MNIST $5 \times 100$ & 55\% (19s) & 50\%(1026s) & 60\% (19s) & 50\% (1027s) \\ \hline
		MNIST $6 \times 500$ & 51\% (16s) & 50\% (1002s) & 58\% (18s) & 55\% (1036s) \\ \hline
		CIFAR CNN-B-adv & 22\% (8.7s) & 20\% (373s) & 43\% (14s) & 24\% (576s) \\ \hline 
	\end{tabular}
	\caption{Undecided images ($\%$, {\em lower is better}), as computed by $\alpha,\beta$-Crown, and MN-BaB}
	\label{table10}
\end{table}

Analysis: results reveal that MN-BaB is slightly slower and slightly less accurate than $\alpha,\beta$-Crown. Notice the specially high number of undecided images for CNN-B-Adv with TO=30s, probably meaning that 30s is too small for MN-BaB on this large DNN.
Hence, Hybrid MILP is more accurate than MN-BaB with similar runtime or faster.

\newpage

	\subsection*{Comparison $\alpha,\beta$-Crown vs NNenum}

NNenum \cite{nnenum} is a complete verifier with good performance according to VNNcomp.
It was the only complete verifier tested in Table \ref{table_complete} to verify more images than $\alpha,\beta$-Crown. The experiments section in \cite{nnenum} does not report
the {\em hard} DNNs we are considering. We tried to experiment it on the same MNIST 
$6 \times 500$ and CIFAR CNN-B-adv as we did in Table \ref{table10} for MN-BaB. Unfortunately, on $6 \times 500$, buffer overflow were reported.
We report in Table \ref{table11} experiments with the same 2000s Time-out (it was $10 000s$ in Table \ref{table_complete})  for a fair comparison with $\alpha,\beta$-Crown, on both 
MNIST $5 \times 100$ and CIFAR CNN-B-Adv. 
On MNIST $5 \times 100$, NNenum is slightly more accurate than $\alpha,\beta$-Crown, but far from the accuracy Hybrid MILP.
On CIFAR CNN-B-adv, NNenum was much less accurate than $\alpha,\beta$-CROWN, and thus of Hybrid MILP. In both test, the runtime of NNenum was also much longer than for Hybrid MILP.


\begin{table}[h!]
	\centering
	\begin{tabular}{||l||c||c||c||c||}
		\hline \hline
		 & $\alpha,\beta$-Crown & NNenum & Hybrid\\ 
		 Network & TO=2000s &  TO=2000s & MILP\\ 
		\hline
		MNIST $5 \times 100$ & 50\%(1026s) & 44\% (1046s) & \bf 13\% (46s)\\ \hline
		CIFAR CNN-B-adv & 20\% (373s) & 40\% (1020s) & \bf 11\% (417s)\\ \hline 
	\end{tabular}
	\caption{Undecided images ($\%$, {\em lower is better}), as computed by $\alpha,\beta$-Crown and NNenum with 2000s time-out, and Hybrid MILP}.
	\label{table11}
\end{table}


\subsubsection*{Comparison for layer 7}	

\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|}
	\hline
		Time & Uncertainty\\ 
	\hline	14 & 3.233021901\\
\hline	15.2 & 3.140309921\\
\hline	17.21 & 3.059083103\\
\hline	17.4 & 2.986166762\\
\hline	19.2 & 2.856229765\\
\hline	20.9 & 2.799248232\\
\hline	23.7 & 2.746167245\\
\hline	26.6 & 2.69485246\\	
	\hline
	
	
	\end{tabular}
	\caption{pMILP}
	\label{table12}
	\end{table}
	

	
	\begin{table}[h!]
		\centering
		\begin{tabular}{|c|c|}
			\hline
			Time & Uncertainty\\ 
			\hline	21.1 & 3.348236261\\
			\hline	27.6 & 3.24604282\\
			\hline	38.2 & 3.196640184\\
			\hline	47.1 & 3.164298172\\
			\hline	56.7 & 3.146913614\\
			\hline	106.7 & 3.108035223\\
			\hline	156.3 & 2.900438725\\
			\hline	205.8 & 2.84864842617\\	
			\hline	406.7 & 2.80026826460361 \\	
			\hline	606.1 & 2.737064255595137\\	
			\hline
			
			
		\end{tabular}
		\caption{full MILP}
		\label{table13}
	\end{table}
	
	
	
	
	
	
	
	\begin{figure}[h]\hspace*{-0.8cm}
			\includegraphics[scale=0.4]{Layer7_comparison.png}.
			\caption{Comparison of full MILP and pMILP on layer7}
			\label{fig3}
	\end{figure}
	
,
8,
,
,
,
,
,

	
	
\subsubsection*{Comparison for layer 3}	

\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		Time & Uncertainty\\ 
		\hline	5.9 & 1.702986873\\
		\hline	9.0 & 1.65469034\\
		\hline	10.6 & 1.612137282\\
		\hline	11.6 & 1.571001109\\
		\hline	12.92 & 1.531925404\\
		\hline	12.98 & 1.49535638\\
		\hline	13.6 & 1.46189314\\
		\hline	14.4 & 1.429953534\\
		\hline
		
		
	\end{tabular}
	\caption{When the first layer is MILP}
	\label{table14}
\end{table}






\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		Time & Uncertainty\\ 
		\hline	9.3 & 3.247378853\\
		\hline	10.6 & 3.022145891\\
		\hline	11.9 & 2.823833569\\
		\hline	13.1 & 2.638627159\\
		\hline	16.0 & 2.47324218\\
		\hline	28.3 & 2.327935601\\
		\hline	48.1 & 2.195061671\\
		\hline	89.4 & 2.071075926\\	
		\hline

	\end{tabular}
	\caption{When the first layer is LP}
	\label{table15}
\end{table}
	
	
		\begin{figure}[h]\hspace*{-1cm}
		\includegraphics[scale=0.6]{Layer3_comparison.png}.
		\caption{Comparison of layer3 when layer 1 is MILP or LP}
		\label{fig4}
	\end{figure}
