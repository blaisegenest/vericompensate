\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\pagestyle{plain}


%\usepackage[latin9]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tabularx}
\captionsetup{compatibility=false}
% \usepackage{esint}
\usepackage{array}
\usepackage{epstopdf}
\usepackage{placeins}
\usepackage{pgfplots}
\usepackage{url}
\usepackage{tikz}
\usepackage{calc}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usetikzlibrary{positioning, arrows.meta,calc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\vW}{\boldsymbol{W}}

\newcommand{\val}{{\textrm{value}}}
\newcommand{\Val}{{\textrm{value}}}
\newcommand{\MILP}{{\textrm{MILP}}}
\newcommand{\LP}{{\textrm{LP}}}

\newcommand{\UB}{\mathrm{UB}}
\newcommand{\LB}{\mathrm{LB}}
\newcommand{\ub}{\mathrm{ub}}
\newcommand{\lb}{\mathrm{lb}}
\newcommand{\B}{\mathrm{B}}

\newcommand{\CMP}{{\textrm{CMP}}\ }


\newcommand{\toolname}{\CMP}






\usepackage{amsmath, amsthm, amssymb, amsfonts}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}



\newcommand{\ReLU}{\mathrm{ReLU}}

\title{Partial MILP to improve $\alpha,\beta$-CROWN accuracy \\ for hard DNN verification instances}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
$\alpha,\beta$-CROWN has won the last 4 VNNcomp(etitions), as the DNN verifier with the best 
trade-off between accuracy vs computational time. VNNcomp however is focusing on relatively easy verification instances (network, inputs (images)), 
with few {\em unstable nodes}. In this paper, we consider harder verification instances, namely the usual local robustness problem, and 
compare how different solutions out there fare. On such instances, $\alpha,\beta$-CROWN displays a large gap ($\approx 40\%$) between instances that can be verified, and instances with an explicit attack. Enabling much larger time-outs for $\alpha,\beta$-CROWN only improves verification rate by few percents, leaving a large gap of undecided instances while already taking a considerable amount of time. Resorting to other techniques, such as complete verifiers, does not fare better even with very large time-outs (10000s per instance): They would theoretically be able to close the gap, but with an untractable runtime on all but small instances, because the instances considered are hard. 

In this paper, we study the complete MILP encoding of ReLU-DNNs, provide new deep insights in the LP relaxation, which allows us to carefully craft a {\em partial MILP} solution which only considers few neurons as integer variables, the rest using the LP relaxation. Compared with previous attempts, we can reduce the number of integer variables by around 4 times while maintaining the same level of accuracy. This gives rise to a very efficient yet accurate algorithm. Implemented as $\alpha,\beta$-pMILP, calling first $\alpha,\beta$-CROWN with a short time-out (10s) to solve easier instances, and then partial MILP for those for which $\alpha,\beta$-CROWN fails, produces a very accurate yet efficient verifier, reducing tremendously the gap of undecided instances to $\approx 10\%$, while keeping a reasonable runtime ($<500s$ on average per instance).
\end{abstract}






\section*{Tables}

The following table are the results of running $\alpha,\beta$-Crown pure BaB mode with different timeout for different networks. The numbers shown in the table is the rate of images in percentage.

\begin{tabular}{||c|c|c||c|c|c||}
	\hline
	Network & Acc & Upper  & $\alpha,\beta$-Crown& $\alpha,\beta$-Crown & $\alpha,\beta$-Crown \\ 
	Perturbation &   & Bound & TO=10s & TO=30s & TO=2000s\\ \hline

	MNIST 6$\times$100 & 96\% & 90\% & 33\% & 35\% & 40\%   \\
	$\epsilon = 0.026$ &  &  & ? &  18s &  1600s  \\  \hline
    MNIST 6$\times$200 & 97\%  & 96\%  & 46\%  & 49\%  & 50\%   \\ 
	$\epsilon = 0.015$ & &  & ? &  17s &  ?s  \\  \hline
	MNIST 9$\times$100 & 95\%  & 86\%  & 23\%  & 28\%  & 28\%   \\
	$\epsilon = 0.026$ &  &  & ? &  20s &  ?s  \\  \hline
	MNIST 9$\times$200 & 95\%  & 91\%  & 35\%  & 36\%  & 37\%   \\ 
	$\epsilon = 0.015$ & &  & 7s &  s &  ?s  \\  \hline
	MNIST 6$\times$500 &  ? & 94\%  & 41\%  & 43\%  & 44\%   \\ 
	$\epsilon = 0.035$ & &  & 6s &  15s &  1002s  \\  \hline
	CIFAR CNN-B-adv & ?  & 62\%  &  ? & 40\%  & 42\%   \\
	 &  &  & s & s & s  \\ \hline \hline
	CIFAR ResNet & 29\%  & 25\%  & 25\%  & 25\%  & 25\%   \\
	Wong &  &  & 2s & 2s & 2s  \\ \hline
	

\end{tabular}

\vspace*{4ex}
\begin{tabular}{|c|c|c|}
	\hline
	Verifier & Rate & Average time \\ \hline
	Marabou & 0 & 0 \\ \hline
	$\alpha,\beta$-Crown & 0 & 0 \\ \hline
	Full MILP & 0 & 0 \\ \hline
	0 & 0 & 0 \\ \hline
\end{tabular}



\vspace*{4ex}

The table of undecided images among first 100 images in database by different methods for different network.

\begin{tabular}{|c|c|c|c|c|c|}
	\hline
	Network & $\alpha,\beta$-Crown, 10s & $\alpha,\beta$-Crown, 30s & $\alpha,\beta$-Crown, 2000s & $\alpha,\beta$-Crown-refine & Our method \\ \hline
	& Rate(\%) \hfill Time(s) & Rate(\%) \hfill Time(s) & Rate(\%) \hfill Time(s) & Rate(\%) \hfill Time(s) & Rate(\%) \hfill Time(s)\\ \hline
	MNIST 6$\times$100 & 57 \hfill 6.9 & 55 \hfill 18.9 & 50 \hfill 1026.1 & 13 \hfill 91.7 & Rate \hfill Time \\ \hline
	MNIST 6$\times$200 & 50 \hfill 6.5 & 47 \hfill 16.6 & 46 \hfill 930.3 & 9 \hfill 79.8 & Rate \hfill Time \\ \hline
	MNIST 9$\times$100 & 63 \hfill 7.2 & 58 \hfill 20.1 & 58 \hfill 1162.9 & 21 \hfill 102.2 & Rate \hfill Time \\ \hline
	MNIST 9$\times$200 & 56 \hfill 6.8 & 55 \hfill 18.2 & 54 \hfill 1083.1 & 16 \hfill 83.2 & Rate \hfill Time \\ \hline
	0 & 0 & 0 & 0 & 0 & 0 \\ \hline
\end{tabular}


	\section{Introduction}

\input{introduction}
%
\input{relatedwork}

\section{Notations and Preliminaries}

In this paper, we will use lower case latin $a$ for scalars, bold $\boldsymbol{z}$ for vectors, 
capitalized bold $\boldsymbol{W}$ for matrices, similar to notations in.
To simplify the notations, we restrict the presentation to feed-forward, 
fully connected ReLU Deep Neural Networks (DNN for short), where the ReLU function is $ReLU : \mathbb{R} \rightarrow \mathbb{R}$ with
$ReLU(x)=x$ for $x \geq 0$ and $ReLU(x)=0$ for $x \leq 0$, which we extend componentwise on vectors.

%In this paper, we will not use tensors with a dimension higher than matrices: those will be flattened.

%\subsection{Neural Network and Verification}


% testtesttesttest
An $\ell$-layer DNN is provided by $\ell$ weight matrices 
$\boldsymbol{W}^i \in \mathbb{R}^{d_i\times d_{i-1}}$
and $\ell$ bias vectors $\vb^i \in \mathbb{R}^{d_i}$, for $i=1, \ldots, \ell$.
We call $d_i$ the number of neurons of hidden layer $i \in \{1, \ldots, \ell-1\}$,
$d_0$ the input dimension, and $d_\ell$ the output dimension.

Given an input vector $\boldsymbol{z}^0 \in \mathbb{R}^{d_0}$, 
denoting $\hat{\boldsymbol{z}}^{0}={\boldsymbol{z}}^0$, we define inductively the value vectors $\boldsymbol{z}^i,\hat{\vz}^i$ at layer $1 \leq i \leq \ell$ with
\begin{align*}
	\boldsymbol{z}^{i} = \boldsymbol{W}^i\cdot \hat{\boldsymbol{z}}^{i-1}+ \vb^i \qquad \, \qquad
	\hat{\boldsymbol{z}}^{i} = ReLU({\boldsymbol{z}}^i).
\end{align*} 

The vector $\hat{\boldsymbol{z}}$ is called post-activation values, 
$\boldsymbol{z}$ is called pre-activation values, 
and $\boldsymbol{z}^{i}_j$ is used to call the $j$-th neuron in the $i$-th layer. 
For $\boldsymbol{x}=\vz^0$ the (vector of) input, we denote by $f(\boldsymbol{x})=\vz^\ell$ the output. Finally, pre- and post-activation neurons are called \emph{nodes}, and when we refer to a specific node/neuron, we use $a,b,c,d,n$ to denote them, and $W_{a,b} \in \mathbb{R}$ to denote the weight from neuron $a$ to $b$. Similarly, for input $\boldsymbol{x}$, we denote by $\val_{\boldsymbol{x}}(a)$ the value of neuron $a$ when the input is $\boldsymbol{x}$. A path $\pi$ is a sequence $\pi=(a_i)_{k \leq  i \leq k'}$ of neurons in consecutive layers, and the weight of $\pi$ is 
$weight(\pi)=W_{a_k,a_{k+1}} \times \cdots \times  W_{a_{k'-1},a_{k'}}$.



\iffalse
and the $i$-th hidden layer is a vector in $\mathbb{R}^{d_i}$, 
and the output layer is a vector in $\mathbb{R}^{d'}$ or a scale. 
The weights, bias and activation functions decide propagate the from previous to the next layer. In formula, from layer $l_{i-1}$ to layer $l_{i}$, the weight 
$\boldsymbol{W}^i$ is matrix of $d_i\times d_{i-1}$, 
the bias is a vector $\vb^i$ in $\mathbb{R}^{d_i}$, and the activation function 
is $\sigma$, then  if the $i-1$-th layer is $\hat{\boldsymbol{z}}^{(i-1)}$, 
then the value of $i$-th layer is computed by: 
\begin{align*}
	{\boldsymbol{z}}^{i} &= \boldsymbol{W}^i\cdot \hat{\boldsymbol{z}}^{(i-1)}+ \vb^i\\
	\hat{\boldsymbol{z}}^{i}(n) &= \sigma({\boldsymbol{z}}^i(n)).
\end{align*} The vector $\hat{\boldsymbol{z}}$ is called post-activation values, and $\boldsymbol{z}$ is called pre-activation values, and $\boldsymbol{z}^{(i)}_j$ is used to call the $j$-th neuron in the $i$-th layer. In our style, we also call neurons \emph{nodes} and use $a,b,c,d$ to denote them. We use $W_{ab}$ to denote the weight from neuron $b$ to $a$. We use $\boldsymbol{x}$ to denote the vector of input and  $f(\boldsymbol{x})$ to denote the output.
\fi

\medskip

Concerning the verification problem, we focus on the well studied local-robustness question. Local robustness asks to determine whether the output of a neural network will be affected under small perturbations to the input. 
Formally, for an input $\vx$ perturbed by $\varepsilon >0$ under distance $d$, then the DNN is locally $\varepsilon$-robust in $\vx$ whenever:
\begin{align*}
	\forall \boldsymbol{x'} \text{ s.t. } d(\vx,\vx')\leq \varepsilon, \text{ we have }  
	argmax_i (f(\boldsymbol{x'})[i]) = argmax_i(f(\boldsymbol{x})[i])
\end{align*} 

\iffalse
In some cases, the output is a vector but the aim to get the label of dimension with the minimal value. In this case, the problem can be written as:\begin{align*}
	\forall \boldsymbol{x} \in\mathcal{D} \  \min f(\boldsymbol{x}) = \min f(\boldsymbol{x}_0)
\end{align*}

If so, the question of verification can turn to the following optimization question: \begin{align*}
	\min f(\boldsymbol{x}) \ s.t. {\boldsymbol{z}}^{i} &= \boldsymbol{W}^i\cdot \hat{\boldsymbol{z}}^{(i-1)}+ b^i\\
	\hat{\boldsymbol{z}}^{i}(n) &= \sigma({\boldsymbol{z}}^i(n)), \boldsymbol{x}\in\mathcal{D}.
\end{align*}

In this paper, we only consider $\ReLU$ function as the activation function: $\sigma(a)=\ReLU(a)=\max(0,a)$. 

In this paper, we consider $L^{\infty}$ norm the max value of distance of each dimension, that is $d(\vx,\boldsymbol{x}_0)=\max |\boldsymbol{x}(n)-\boldsymbol{x}_0(n)|$. 
\fi



\input{valabstraction}

\input{comppaths}


\input{experiments}


\section{Conclusion}

In this paper, we introduced the notion of {\em compensating pairs of paths}, with rationale why such a phenomenon creates inaccuracies hard to handle when verifying DNNs. We proved that this phenomenon is actually explaining entirely the inaccuracies, as in their absence, even the simplest Box abstraction (interval arithmetic) suffices to verify accurately DNNs. This is experimentally confirmed by the fact that DNNs harder to verify (verification-agnostic) also exhibit paths with larger compensating strength than  robustly-trained DNNs.

Based on this idea of compensating pairs of paths, we proposed the $\MILP_{Z}$ abstraction considering a subset $Z$ of the set of unstable ReLU nodes, that we proved to be fully accurate if {\em all} the compensating paths are covered by $Z$. Our empirical studies revealed that CMP, selecting $Z$ to cover only {\em a few} paths with the most significant compensation, can yield highly accurate results, up to 20\% more accurate than SOTA within the same runtime, again validating the focus on compensating strength. 
This underscores the potential of compensating strength as an innovative and promising avenue for enhancing DNN verification. We finally proposed several ways to use the compensating strength to optimize current tools in different directions, and leave that as future work.
\newpage

\bibliography{references}
\bibliographystyle{iclr2025_conference}

\newpage

\appendix

\input{proofs}

\end{document}

