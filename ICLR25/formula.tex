\section{Important neurons for accuracy and Utility function.}

In this section, we evaluate how each neuron would impact the accuracy of encoding this neuron with an integer (actually binary) or a linear variable. We will say we open a neuron when this neuron is selected with a binary variable. We denote by $X$ a choice of set of open neurons, and by $\mathcal{M}_X$ the MILP model where variables from $X$ are encoded with binary variables, and other variables are using the LP linear relaxation.

Let $z$ be a neuron of the DNN we want to evaluate, for instance computing an upper bound on the value it can take. We denote by $n < z$ the fact that a neuron $n$appears on a layer before the layer of $z$, and $n \leq z$ if it can also be $z$.
Let us denote ($\Sol\_\max_X^z(n))_{n \leq z}$ a solution of $\mathcal{M}_X$ 
maximizing $z$. In particular, $\Sol\_\max_X^z(z)$ is the maximum of $z$ under $\mathcal{M}_X$.

Consider ($\Sol\_\max_\emptyset^z(n))_{n \leq z}$ (this can be obtained very efficiently by {\em one} call to an LP solver). An accurate utility function is to evaluate 
$\Improve\_\max^z(a)=$ $\Sol\_\max_\emptyset^z(z) -$ $\Sol\_\max_{\{a\}}^z(z)$, 
representing how much opening neuron $a < z$ reduces the maximum computed for $z$
compared with using only LP. 
We have $\Improve\_\max^z(a)\geq 0$ as $\Sol\_\max_{\{a\}}^z$ fullfils all the constraints of 
$\mathcal{M}_\emptyset$, so $\Sol\_\max_{\{a\}}^z(z) \leq$ $\Sol\_\max_\emptyset^z(z)$.
Similarly, we define ($\Sol\_\min_\emptyset^z(n))_{n \leq z}$ and 
$\Improve\_\min^z(a)$. Calling MILP on $\mathcal{M}_{\{a\}}$ for each neuron $a \leq z$
would however be very time consuming when the number of neurons $a$ to evaluate is large.
Instead, we focus on the following upper bounding of $\Improve\_\max^z(a)$.



%\subsubsection*{Observation}

%The key of our formula is based on the following observation:



%Our observation (by experiments) is that \begin{align}
%	I_X \approx \sum_{b\in X} I_b.
%\end{align} Especially, if all neurons in $X$ are from one layer before $a$, then in %experiments, we observe that 

\iffalse
\begin{align*}
	|(I_X - \sum_{b\in X} I_b)/I_X| < 1\%. \ (\text{in experiments})
\end{align*} Even $X$ contains neurons from 3 layers before the target layer, in experiments, $I_X$ is still close to $\sum_{b\in X} I_b$.

Therefore, based on this observation, the question to choose $X$ is converted to compute $I_b$ for neurons $b$ in layers before the target layer. Our formula is to estimate the improvement of different individual neurons in different layers. For different layers, the formula will be different.  However, neither the observation in this subsection nor the formula in the next subsection has solid theoretical proof to show that they are very accurate. They are all based on experiments. 


In our algorithm, we will open neurons at most 3 layer3 before the target layer. So the formula will consists of three parts.


\subsubsection*{Compute the improvement of a single neuron}

\subsection*{One Layer before $z$}

\fi

For all neurons $n$, let $\sol(n)=$$\Sol\_\max_\emptyset^z(n)$ be the value of neuron $a$
in the solution of the LP instance $\Sol\_\max_\emptyset^z$ to maximize $z$.
%For one layer before the target layer, the formula is simple and most accurate. 
To estimate $\Improve\_\max^z(a)$, we define $\Utility\_\max^z(a)$, first for neurons $a$ one layer before $z$, by computing by how much the value of $z$ will change if $a$ is opened
and other values remain the same - in particular, $value(\hat{a})=\ReLU(sol(a))$. We define:
%we first need to run $M^a_{\emptyset}$ to compute the upper bound of $a$  to obtain the solution data. Especially, we will read the values of $b$, before $\ReLU$ function and after $\ReLU$ function.

	\begin{align}
		\Utility\_\max\nolimits^z(a) = W_{az} \times (\sol(\hat{a})- \ReLU(\sol(a))).
	\end{align}
	
	%In particular, if $\sol(\hat{a})=\ReLU(\sol(a))$, then we will have 
	%\begin{align*}
%		Utility^z(a) = 0.
%	\end{align*}

In the case $W_{az}<0$, to maximize $z$, the LP engine sets $\sol(\hat{a})$ to its minimal value, which is $\sol(\hat{a})=\ReLU(a)$ thanks to Proposition \ref{LP}. In this case, we have $\Utility\_\max^z(a)=0$.
Thus, we have $\Utility\_\max^z(a) \geq 0$ in all cases
as $\sol(\hat{a}) \geq \ReLU(a)$. 
We will show with a more general definition that $0 \leq \Improve\_\max^z(a) \leq \Utility\_\max^z(a)$ in Prop.~\ref{prop2}. Thus, $\Utility\_\max^z(a)$ can be used to approximate $\Improve\_\max^z(a)$. In particular, for all nodes $a$ with $W_{az}< 0$,
this node will have the smallest $\Utility\_\max\nolimits^z(a)=0$ (thus will not get picked in the open nodes $X$), and indeed it is not having any impact on  $\Sol\_\max_{\{a\}}^z(z)$. This is one stricking difference (but not the only one) with choosing utility based on 
$|W_{az}|$ \cite{DivideAndSlide}.

	
%	
%	Similarly, let $\sol(b)$ be the value of $b$ in the LP solution of lower bound of $a$, and $\sol(\hat{b})$ be the value of $\hat{b}$. Then the formula to estimate improvement of lower bound of $b$ is: \begin{align*}
%		Improve\_min^z(b) \approx -W_{ba}(\sol(\hat{b})-\ReLU(\sol(b))).
%	\end{align*}
	
%To explain the formula, we use upper bound and the case that $W_{ba} > 0$ as an example. To compute the upper bound of $a$, $\hat{b}$ should be as large as possible. In the LP model, for fixed $\sol(b)$, the upper bound of $\hat{b}$ may be larger than $\ReLU(\sol(b))$. This is because in LP model, the upper bound of $\sol(\hat{b})$ is decided by the linear approximation rather than $\ReLU$ function. So, when neuron $b$ is open, if $\sol(b)$ do not change, then the upper bound of $a$ will be improved because the value of $\sol(\hat{b})$ will be lower to $\ReLU(\sol(b))$.
% 			
%Of course changing other variables may also effect the upper bound, but our experiments show that, the change from $\sol(\hat{b})$ to $\ReLU(\sol(b))$ is the major part of improvement. 

\medskip

Now, consider neurons $a$ two layers before $z$. We use $b$ to denote neurons in the layer $\ell$ between $a$ and $z$. 
If we open $a$, then the change $\Delta(a)$ in the weight of $a$ is 
$\Delta(a)=\ReLU(\sol(a)) - \sol(\hat{a}) \leq 0$ as above. Its impact on $z$ is no more direct with $W_{az}$, but it is through all the $b \in \ell$. 
We let $\Delta(b) = W_{ab}\Delta(\hat{a})$.
Based on Proposition \ref{LP}, we can evaluate the impact 
$\Delta(\hat{b})$ of opening $a$ on each value of $\hat{b}$, by using the upper and lower bound $\UB(b),\LB(b)$:

	\begin{align*}
		&\Delta(\hat{b}) =
		\begin{cases}
			\frac{\UB(b)}{\UB(b)-\LB(b)}\Delta(b),  &\text{if }W_{bz} > 0\\
			\max(\Delta(b),-\sol(b)),  &\text{if }  W_{bz} < 0 \text{ and } \sol(b)\geq0\\
			\max(\Delta(b)+\sol(b),0),  &\text{if }  W_{bz} < 0 \text{ and } \sol(b)<0		 
		\end{cases}
		\end{align*}


Indeed, if $W_{bz}>0$, then according to Proposition \ref{LP}, the LP solver
sets $\sol(\hat{b}) = \sol(b) \frac{\UB(b)}{\UB(b)-\LB(b)} +$ Cst to maximize $z$.
Changing $b$ by $\Delta(b)$ thus results in changing $\sol(\hat{b})$ by 
$\frac{\UB(b)}{\UB(b)-\LB(b)}\Delta(b)$.
If $W_{bz}\leq0$, then the LP solver sets $\sol(\hat{b})$ to the lowest possible value to maximize $z$, which happens to be $\ReLU(b)$ according to Proposition \ref{LP}.
If $\sol(b) < 0$, then we have $\sol(\hat{b})=\ReLU(b)=0$ and opening $a$ change the 0 value only if $\sol(b)+\Delta(b)>0$. If $\sol(b) > 0$, then 
$\sol(\hat{b})=\ReLU(\sol(b))=\sol(b)$, and the change to $\hat{b}$ will be 
the full $\Delta(b)$, unless $\Delta(b) < -\sol(b) < 0$ in which case it is 
$-\sol(b)$. We then set:

%We then define 
%\begin{align*}
%	Utility\_max^z(a) = (\sol(\hat{a})-\ReLU(\sol(a)))\sum_b k(b).
%\end{align*}

$$ \Utility\_\max\nolimits^z(a) = -\sum_{b \in \ell} W_{bz} \Delta(\hat{b})$$
 



\begin{proposition}
	\label{prop2}
		$0 \leq \Improve\_\max^z(a) \leq \Utility\_\max^z(a)$. 
	\end{proposition}
	
	\begin{proof}
    Consider $\sol'(n))_{n \leq z})$ with
	$\sol'(n)=\sol(n)$ for all $n \notin \{z,\hat{a}\} \cup \{b,\hat{b} \mid b \in \ell\}$. In particular,  $\sol'(a) = \sol(a)$.
	Now, define $\sol'(\hat{a}) = \ReLU(\sol(a))$. 
	That is, $\sol'(\hat{a})$ is the correct value for $\hat{a}$, obtained if we open neuron $a$, compared to the LP abstraction for $\sol(\hat{a})$.
	We also define $\sol'(b)=\sol(b)+\Delta(b)$ and 
	$\sol'(\hat{b})=\sol(\hat{b}) + \Delta(\hat{b})$.
	Last, $\sol'(z)=\sol(z) + \sum_{b \in \ell} W_{bz} \Delta(\hat{b})$.
	% = \sol(z) - \Utility\_\max\nolimits^z(a)$.
	
	It is easy to check that $(\sol'(n))_{n \leq z}$ satisfies the constraints in 
	$\mathcal{M}_{\{a\}}$, as opening $a$ changes the value of $\hat{a}$ from
	$\sol(\hat{a})$ to $\ReLU(\sol(a))$, and the contribution from $a$ to $b,\hat{b},z$ 
	are respected.
	As $\sol'(z)$ is a solution of $\mathcal{M}_{\{a\}}$, it is smaller or equal to the maximal solution: $\sol'(z) \leq$ $\Sol\_\max_{\{a\}}^z(z)$. That is, 
	$\sol(z)-\sol'(z) \geq \sol(z) -$ $\Sol\_\max_{\{a\}}^z(z)$, i.e. 
	$\Improve\_\max^z(a) \leq \Utility\_\max^z(a)$.
	In particular, we have that $\Utility\_\max^z(a) \geq 0$, which was not obvious from the definition.
	\end{proof}
	

We can proceed inductively in the same way to define $\Utility\_\max^z(a)$ for deeper neurons $a$.


\iffalse
\subsubsection*{Three Layer before  $z$} 

Suppose $a$ is a neuron in three layers before $z$, we use $b$ to denote neurons in two layer before $z$ and $c$ to denote neurons in one layer before $z$ and. 

This formula is based on previous subsection but more complex. In some network, running this formula may cost too much time. 

The key problem is how to compute the coefficient $k$ for neurons in two layers before the target layer. To do this, we may use the values in the solution of LP model as follows:

\begin{definition}\label{3layer}
Let $\UB$ and $\LB$ denote the precomputed upper bounds and lower bounds used in building MILP models. We define the following function $h$ for all neurons $b$ in two layers before the target neuron $z$ as follows:
	\begin{align}
		&v_0 = \sol(\hat{b}), v_1 = \ReLU(\sol(b)), v_2 = \frac{\UB(b)\sol(b)-\UB(b)\LB(b)}{\UB(b)-\LB(b)}\\
		&h(b) =
		\begin{cases}
			\frac{v_0-v_1}{v_2-v_1}, & \text{if } v_2-v_1 > 0\\
			0.5, & \text{otherwise.}
		\end{cases}
	\end{align} 
\end{definition} 

\begin{definition}
	Continue the assumption in Definition \ref{3layer}. We define function $D$ layer by layer.
	
	First, $\Delta(a) = \ReLU(\sol(a))-\sol(\hat{a})$.
	
To compute $\Delta(b)$ for neurons $b$ in two layer before $z$, we define \begin{align}
	&u_0 = \max(\LB(b),\min(\UB(b),  \sol(b)+\Delta(a)W_{ab}))\\
	&u_1 = \begin{cases}
		\ReLU(u_0)+h(b)(\frac{\UB(c)u_0-\UB(b)\LB(b)}{\UB(b)-\LB(b)}-\ReLU(u_0)), & \text{if }\LB(b) < 0\\
	u_0, & \text{if }  \LB(b) \geq 0
	\end{cases}\\
	&\Delta(b) = u_1-\sol(\hat{b})
\end{align}
	
	To compute $\Delta(c)$ for neurons $c$ in one layer before $z$, we define 
	\begin{align}
		&w_0 = \sum_b \Delta(b)W_{bc}\\
		&w_1 = \min(\UB(c),\sol(c)+w_0)\\		
		&\Delta(c) =
		\begin{cases}
			w_1-\sol({c}), & \text{if }W_{cz} > 0 \text{ and } \LB(c)\geq 0\\
		k(c)(w_1-\sol({c})), & \text{if }W_{cz} > 0 \text{ and } \LB(c)< 0\\
		\ReLU(w_1)-\sol(\hat{c})	, & \text{if }  W_{cz} < 0
		\end{cases}\\
		&\Delta(z) = \sum_c \Delta(c)W_{cz}\\
		&\Utility\_\max^z(a) = -\Delta(z)
	\end{align}
\end{definition}
		
\fi