\section{Important neurons for accuracy and Utility function.}

In this section, we evaluate how each neuron would impact the accuracy of encoding this neuron with an integer (actually binary) or a linear variable. We will say we open a neuron when this neuron is selected with a binary variable. We denote by $X$ a choice of set of open neurons, and by $\mathcal{M}_X$ the MILP model where variables from $X$ are encoded with binary variables, and other variables are using the LP linear relaxation.

Let $z$ be a neuron of the DNN we want to evaluate, for instance computing an upper bound on the value it can take. We denote by $n< z$ the fact that a neuron appears on a layer before the layer of $z$, and $n \leq z$ if it can also be $z$.
Let us denote (Sol\_max$_X^z(n))_{n \leq z}$ is a solution of $\mathcal{M}_X$ 
maximizing $z$. In particular, Sol\_max$_X^z(z)$ is the maximum of $z$ under $\mathcal{M}_X$.

Consider (Sol\_max$_\emptyset^z(n))_{n \leq z}$ (this is obtained by {\em one} call to an LP solver, so can be obtained very efficiently).
An accurate utility function is to evaluate 
Improve\_max$^z(a)=$ Sol\_max$_\emptyset^z(z) -$ Sol\_max$_{\{a\}}^z(z)$, 
representing how much opening neuron $a < z$ reduces the maximum computed for $z$
compared with using only LP. 
We have Improve\_max$^z(a)\geq 0$ as Sol\_max$_{\{a\}}^z$ fullfils all the constraints of 
$\mathcal{M}_\emptyset$, so Sol\_max$_{\{a\}}^z(z) \leq$ Sol\_max$_\emptyset^z(z)$.

Similarly, we define (Sol\_min$_\emptyset^z(n))_{n \leq z}$ and 
Improve\_min$_a^z$. Calling MILP on $\mathcal{M}_{\{a\}}$ for each neuron $a \leq z$
would however be very time consuming when the number of neurons $a$ to evaluate is large.
Instead, we focus on the following upper bounding of Improve\_max$_a^z$.



%\subsubsection*{Observation}

%The key of our formula is based on the following observation:



%Our observation (by experiments) is that \begin{align}
%	I_X \approx \sum_{b\in X} I_b.
%\end{align} Especially, if all neurons in $X$ are from one layer before $a$, then in %experiments, we observe that 

\iffalse
\begin{align*}
	|(I_X - \sum_{b\in X} I_b)/I_X| < 1\%. \ (\text{in experiments})
\end{align*} Even $X$ contains neurons from 3 layers before the target layer, in experiments, $I_X$ is still close to $\sum_{b\in X} I_b$.

Therefore, based on this observation, the question to choose $X$ is converted to compute $I_b$ for neurons $b$ in layers before the target layer. Our formula is to estimate the improvement of different individual neurons in different layers. For different layers, the formula will be different.  However, neither the observation in this subsection nor the formula in the next subsection has solid theoretical proof to show that they are very accurate. They are all based on experiments. 


In our algorithm, we will open neurons at most 3 layer3 before the target layer. So the formula will consists of three parts.


\subsubsection*{Compute the improvement of a single neuron}

\subsection*{One Layer before $z$}

\fi

For all neurons $n$, let $sol(n)=$Sol\_max$_\emptyset^z(n)$ be the value of neuron $a$
in the solution of the LP instance Sol\_max$_\emptyset^z$ to maximize $z$.


%For one layer before the target layer, the formula is simple and most accurate. 
To estimate Improve\_max$_a^z$, we define Utility\_max$^z(a)$, first for neurons $a$ one layer before $z$:
%we first need to run $M^a_{\emptyset}$ to compute the upper bound of $a$  to obtain the solution data. Especially, we will read the values of $b$, before $\ReLU$ function and after $\ReLU$ function.

	\begin{align}
		Utility\_max^z(a) = |W_{az} \times (sol(\hat{a})- \ReLU(sol(a)))|.
	\end{align}
	
	%In particular, if $sol(\hat{a})=\ReLU(sol(a))$, then we will have 
	%\begin{align*}
%		Utility^z(a) = 0.
%	\end{align*}

We have that :

\begin{proposition}
$0 \leq Improve\_max^z(a) \leq Utility\_\max^z(a)$. 
\end{proposition}

\begin{proof}
Consider $(sol'(n))_{n \leq z})_{n \leq z})$ with
$sol'(n)=sol(n)$ for all $n \notin \{z,\hat{a}\}$.
In particular,  $sol'(a) = sol(a)$.

Now, define $sol'(\hat{a}) = \ReLU(sol(a))$. 
That is,  
sol'$(\hat{a})$ is the correct value for $\hat{a}$ (obtained if we open neuron $a$), 
compared to the LP abstraction for $sol(\hat{a})$.

Last, we define $sol'(z)=sol(z) + W_{az} \times (\ReLU(sol(a)) - sol(\hat{a})) = sol(z) - Utility\_max_z(a)$.

Now, it is easy to check that $(sol'(n))_{n \leq z}$ satisfies the constraints in 
$\mathcal{M}_{\{a\}}$, as openning $a$ changes the value of $\hat{a}$ from
$sol(\hat{a})$ to $\ReLU(sol(a))$, and the contribution from $a$ to $z$ is 
$W_{az}$. As $sol'(z)$ is a solution of $\mathcal{M}_{\{a\}}$, it is smaller or equal to the maximal solution: $sol'(z) \leq$ Sol\_max$_{\{a\}}^z(z)$. That is, 
$sol(z)-sol'(z) \geq sol(z) -$ Sol\_max$_{\{a\}}^z(z)$, i.e. 
$Improve\_max^z(a) \leq Utility\_\max^z(a)$.
\end{proof}

Thus, Utility\_max$^z(a)$ can be used to approximate 
Improve\_max$^z(a)$. In particular, consider a very frequent case where $W_{az}<0$.
In this case, to maximize $z$, the LP engine sets $sol(\hat{a})$ to its minimal value\dots which happens to be $sol(\hat{a})=ReLU(a)$, thanks to Proposition \ref{LP}. 
In this frequent case, we have Utility\_max$^z(a)=0$, and thus Improve\_max$^z(a)=0$. 
In particular, this node will have the smallest utility (thus will not get picked in the open nodes $X$), and indeed it is not having any impact on  Sol\_max$_{\{a\}}^z(z)$. 
This is one stricking difference (but not the only one) with choosing utility based on 
$|W_{az}|$ \cite{DivideAndSlide}.

	
%	
%	Similarly, let $sol(b)$ be the value of $b$ in the LP solution of lower bound of $a$, and $sol(\hat{b})$ be the value of $\hat{b}$. Then the formula to estimate improvement of lower bound of $b$ is: \begin{align*}
%		Improve\_min^z(b) \approx -W_{ba}(sol(\hat{b})-\ReLU(sol(b))).
%	\end{align*}
	
%To explain the formula, we use upper bound and the case that $W_{ba} > 0$ as an example. To compute the upper bound of $a$, $\hat{b}$ should be as large as possible. In the LP model, for fixed $sol(b)$, the upper bound of $\hat{b}$ may be larger than $\ReLU(sol(b))$. This is because in LP model, the upper bound of $sol(\hat{b})$ is decided by the linear approximation rather than $\ReLU$ function. So, when neuron $b$ is open, if $sol(b)$ do not change, then the upper bound of $a$ will be improved because the value of $sol(\hat{b})$ will be lower to $\ReLU(sol(b))$.
% 			
%Of course changing other variables may also effect the upper bound, but our experiments show that, the change from $sol(\hat{b})$ to $\ReLU(sol(b))$ is the major part of improvement. 

\medskip

Now, consider neurons $a$ two layers before $z$. We use $b$ to denote neurons in the layer between $a$ and $z$. If we open $a$, then the change in the weight of $a$ will still be $sol(\hat{a})-\ReLU(sol(a))$ as above. Its impact on $z$ is no more direct with $W_{az}$, but it is through all the $b$ in the intermediate layer. 
Based on Proposition \ref{LP}, we can evaluate how opening $a$ impacts each value of 
$\hat{b}$, through coefficient $k(\hat{b})$, by using the upper and lower bound
$\UB(b),\LB(b)$:
\begin{itemize}
 \item If $W_{bz}\leq0$, then $k(\hat{b})=W_{ab}$ if $sol(b) > 0$, and $k(\hat{b})=0$ if $sol(b) < 0$
 \item If $W_{bz}>0$, then $k(\hat{b})=W_{ab} \frac{\UB(b)}{\UB(b)-\LB(b)}$
\end{itemize}
%$\sum_b W_{bz}W_{ab}$ to replace the coefficient $W_{az}$ in last subsection. 

%To estimate Improve\_max$_a^z$ , 
Indeed, if $W_{bz}\leq0$, then the LP solver sets $sol(\hat{b})$ to the lowest possible value to maximize $z$, which happens to be $ReLU(b)$ according to Proposition \ref{LP}.
If $sol(b) < 0$, then we have a clip at $sol(\hat{b})=ReLU(b)=0$ and opening $a$ does not help. If $sol(b) > 0$, then we have $sol(\hat{b})=ReLU(b)=b$ and every change from 
$a$ is reflected with a coefficient $W_{ab}$ on $b$. 
Last, if $W_{bz}>0$, then according to Proposition \ref{LP}, the LP solver
sets $sol(\hat{b}) = sol(b) \frac{\UB(b)}{\UB(b)-\LB(b)}$ to maximize $z$.

%We then define 
%\begin{align*}
%	Utility\_max^z(a) = (sol(\hat{a})-\ReLU(sol(a)))\sum_b k(b).
%\end{align*}

We define a distance $D$ to account for the fact that a neuron $b$ 
can not exceed $\LB(b),\UB(b)$:
	\begin{align*}
		&D(\hat{a}) = sol(\hat{a}) - \ReLU(sol(a)) \geq 0\\
			&D(\hat{b}) =
		\begin{cases}
			sol(\hat{b}) - \min(\UB(b), sol(\hat{b}) - D(\hat{a}) k(\hat{b})), & \text{if }  W_{bz} > 0 \\
			sol(\hat{b}) - \max(\LB(b), 0, sol(\hat{b}) -D(\hat{a}) k(\hat{b})), & \text{if }W_{bz} < 0\\
		\end{cases}
		\end{align*}

We can now set 

$$ Utility\_max^z(a) = \sum_b W_{bz} D(\hat{b})$$
 
We proceed inductively in the same way to define Utility$\_max^z(a)$ for deeper neuron $a$.

\iffalse
\subsubsection*{Three Layer before  $z$} 

This formula is based on previous subsection but more complex. In some network, running this formula may cost too much time. 

The key problem is how to compute the coefficient $k$ for neurons in two layers before the target layer. To do this, we may use the values in the solution of LP model as follows:

\begin{definition}\label{3layer}
	Suppose we have a fixed target neuron $a$ and a fixed source neuron $b$ in three layers before $a$. Let $\UB$ and $\LB$ denote the precomputed upper bounds and lower bounds used in building MILP models. To compute the improvement of of upper bound of $a$ by $b$, let $v$ be the function of solutions of LP model; then we define the following function $h$ for all neurons $c$ in 2 layers before the target neuron $a$ as follows:
	\begin{align}
		&v_0 = sol(\hat{c}), v_1 = \ReLU(sol(c)), v_2 = \frac{\UB(c)sol(c)-\UB(c)\LB(c)}{\UB(c)-\LB(c)}\\
		&h(c) =
		\begin{cases}
			\frac{v_0-v_1}{v_2-v_1}, & \text{if } v_2-v_1 > 0\\
			0.5, & \text{otherwise.}
		\end{cases}
	\end{align} 
\end{definition} 

\begin{definition}
	Continue the assumption in Definition \ref{3layer}. To compute the improvement of of upper bound of $a$ by $b$, we define function $D$ layer by layer.
	
	First, $D(b) = \ReLU(sol(b))-sol(\hat{b})$.
	
To compute $D(c)$ for neurons $c$ in two layer before $a$, we define \begin{align}
	&u_0 = \max(\LB(c),\min(\UB(c),  sol(c)+D(b)W_{bc}))\\
	&u_1 = \begin{cases}
		\ReLU(u_0)+h(c)(\frac{\UB(c)u_0-\UB(c)\LB(c)}{\UB(c)-\LB(c)}-\ReLU(u_0)), & \text{if }\LB(c) < 0\\
	u_0, & \text{if }  \LB(c) \geq 0
	\end{cases}\\
	&D(c) = u_1-sol(\hat{c})
\end{align}
	
	To compute $D(d)$ for neurons $d$ in one layer before $a$, we define 
	\begin{align}
		&w_0 = \sum_c D(c)W_{cd}\\
		&w_1 = \min(\UB(d),sol(d)+w_0)\\		
		&D(d) =
		\begin{cases}
			w_1-sol({d}), & \text{if }W_{da} > 0 \text{ and } \LB(d)\geq 0\\
		k(d)(w_1-sol({d})), & \text{if }W_{da} > 0 \text{ and } \LB(d)< 0\\
		\ReLU(w_1)-sol(\hat{d})	, & \text{if }  W_{da} < 0
		\end{cases}\\
		&D(a) = \sum_d D(d)W_{da}
	\end{align}
\end{definition} $-D(a)$ is the estimation of improvement of upper bound of $a$ by opening neuron $b$.  For the lower bound, we can compute $D(a)$ by the same formula with the function $v$ from lower bound LP solution and negative $-W_{da}$  instead.
		
\fi