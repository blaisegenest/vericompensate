\documentclass[]{article}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newcommand{\ReLU}{\mathrm{ReLU}}



\title{Proof part 1: 3 layers}
\date{}

\begin{document}

\maketitle

All DNNs $N$ considered only use ReLU activation function, and are fully connected (some wheight between neurons can be 0 though).
A path $\pi$ in $N$ is a sequence of neurons $(a_i)_{i=0}^n$ such that for all $i < n$, layer($a_{i+1}$) = 1 + layer($a_{i}$).

\begin{definition}
	A pair of paths $(\pi,\pi')$
	is called {\em compensating} if they start in the same neuron $a$ and ends in the same neuron $z$, and the product of weights over $\pi$ is strictly positive and the product of weights over $\pi'$ is strictly negative.
\end{definition}

Intuitively, compensating paths will partially cancel out each other as they conribute the same weight $w(a)$ to the weight of the same neuron $w(z)$, but with opposite sign. 
It is not simple to take this compensation into account because of ReLUs: the particular compensation will depend upon the weight of intermediate nodes seen along $\pi$ and $\pi'$, 
as when one of this node gets negative input, it will clip it to 0.


\begin{theorem}
	If for all $(a,c)$ there is no compensation paths $(\pi,\pi')$ 
	in the network from $a$ to $c$, then the LP approximation is 
	$100\%$ accurate (or DeepPoly with the $f(x) \geq 0$ abstraction only, never using the 
	$f(x) \geq x$ abstraction). 
\end{theorem}


\begin{proof}

	First we consider the case when there are only three layers: input layer, one hidden layer, and output layer. 


%We use $c_i$ to denote nodes in the input layer, use $b_j$ to denote nodes in the hidden layer and use $x$ to denote the output node. 
%We use $W$ to denote the weights and $W_{bc}$ and $W_{xb}$ to denote the components. We use capital letter $B$ to denote bias, although it is not important here.



\begin{itemize}
    \item We will denote by $a_i$ nodes in the input layer, and by $b_i$ nodes in the hidden layer.

	\item We use $x^-$ to denote a node before applying the ReLU activation function (but after the linear weighted sum) and $x^+$ to denote the node after applying the ReLU: $x^+ = \ReLU(x^-)$.
	
	\item We denote by $\bar{f}(x)$ the upper bound approximation function of DeepPoly (using the ReLU$(x) \geq 0$ abstraction only) for node $x$, 
	and $\underline{f}(x)$ its lower bound approximation function.
\end{itemize}

We want to show that for any $c$ in the output layer (3rd layer), there exist 
input $\bar{Y}$ such that $W_{\bar{Y}}(c) = \bar{f}(c)(\bar{Y}) = \min_Y \bar{f}(c)(Y)$, and 
 $\underline{Y}$ such that $W_{\underline{Y}}(c) = \underline{f}(c)(\underline{Y}) = \min_Y \underline{f}(c)(Y)$.

Let $c$ be any output node.


\paragraph{DeepPoly bounds} 

First, we compute the bounds obtained by DeepPoly.
The case of the lower bound is symetrical (because the abstraction ReLU$(x)\geq 0$ is used, ensuring that the lower bound of $\underline{f}(x)$ is reached at 0). 

The DeepPoly function for $c$ can be expressed by the following steps:

First, we partition the set of indices $i$ for $b_i$ as $I,I_0,I^+,I^-$ with:
\begin{itemize}
	\item $i \in I$ if the sign of $b_i^-=\sum_{j} \alpha_{a_j b_i} a_j + B_{b_i}$ is always positive irrespective of $Y=(a_j)_j$, in which case $\bar{f}(b_i^+)(Y)=\sum_{j} \alpha_{a_j b_i} a_j + B_{b_i}$ and
	$\underline{f}(b_i^+)=\underline{f}(b_i^-)$, and else:
	\item $i \in I_0$ if the sign of $b_i^-=\sum_{j} \alpha_{a_j b_i} a_j + B_{b_i}$ is always negative irrespective of $Y=(a_j)_j$, in which case $\bar{f}(b_i^+)=\underline{f}(b_i^+)=0$,
	and else:
	\item $i \in I^+$ if $\alpha_{b_i c}>0$, and:
	\item $i \in I^-$ if $\alpha_{b_i c}<0$.
	\end{itemize}
	
Remark that for all $i \notin I$, we have $\underline{f_i}(b_i^+)(Y)=0$.
	

Then we have:
\begin{align*}
	&\bar{f}(c)(Y) = 
	\sum_{i \in I} \sum_{j} \alpha_{a_j b_i} a_j + B_{b_i}
	+ \sum_{i \in I^+} \alpha_{b_i c} \cdot \bar{f_i}(ReLU(\sum_{j} \alpha_{a_j b_i} a_j + B_{b_i}))(Y)+ B_c 
	\end{align*}
	
Now,


 \begin{align}
 &= C+\sum_{W_{xb_i}<0}W_{xb_i}\cdot\underline{k_i}\cdot(\sum_{c_j}W_{b_ic_j}c_j+B_i)+\sum_{W_{xb_i}>0}W_{xb_i}\cdot\bar{k_i}\cdot(\sum_{c_j}W_{b_ic_j}c_j+B_i)\\
 &= C+\sum_{c_j}c_j\cdot\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i\\
 \rightarrow& C+\sum_{\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i<0}\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i\cdot l(c_j)\\
 &+\sum_{\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i>0}\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i\cdot u(c_j)
\end{align} 

Here, $\bar{k_i}$ is the coefficient of $\bar{f_i}$, which is a linear function, and $\underline{k_i}$ is the coefficient of $\underline{f_i}$, which is also a linear function. $C$ is the sum of all constants that occurs in above computations. $K_i$ is the coefficient $\bar{k_i}$ or $\underline{k_i}$ depending on $i$. Notice that by the definition of DeepPoly, all $K_i$ are non negative. 

Among these formulas, (1) to (4) are abstract formula with formal variables $b_i,c_j$. The result of (5)(6) is the upper bound of DeepPoly for node $x$. 

For for each fixed $c_j$, because we assumed no Diamond, so all (for different $b_i$) $W_{xb_i}W_{b_ic_j}$ must have the same sign, or be $0$, and so is $\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i$. So we assign each node $c_j$ a sign of $+,-,0$,  if at least one of $W_{xb_i}W_{b_ic_j}$ is positive, at least one of $W_{xb_i}W_{b_ic_j}$ is negative, or all $W_{xb_i}W_{b_ic_j}$ are 0. Then we use $s(c_j)$ be $u(c_j)$ or $l(c_j)$ or $0$ if $c_j$ has sign of $+$ or $-$ or $0$.



Then (5)(6) can be simplified as \begin{align}
	=C+\sum_{c_j}s(c_j)\cdot\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i
\end{align} This is the DeepPoly upper bound of $x$.

\subsection*{Two} Second, we show that when each $c_j$ of $+,-$ sign gets $s(c_j)$ value, the actual value of $x$ is the same as its DeepPoly upper bound, hence the DeepPoly upper bound is the actual upper bound.

Using the chain from (4) to (2), (7) is equal to: \begin{align*}
&\sum_{W_{xb_i}<0}W_{xb_i}\underline{f_i}(\sum_{c_j}W_{b_ic_j}s(c_j)+B_i)\\
+&\sum_{W_{xb_i}>0}W_{xb_i}\bar{f_i}(\sum_{c_j}W_{b_ic_j}s(c_j)+B_i)+B_x
\end{align*} 

We only need to show when all $c_j$ of $+,-$ sign get $s(c_j)$ value, each $\bar{f_i}(\cdots)$ in above formula (or $\underline{f_i}$) is equal to the actual value of $\hat{b}_i$. 

\subsubsection*{positive} We fix a node $b_i$ that $W_{xb_i}>0$. If so, every $W_{b_ic_j}$ has the same sign as $W_{x_bi}W_{b_ic_j}$, and hence has the same sign as $c_j$ or is $0$ (and if $c_j$ has sign $0$, then $W_{b_ic_j}=0$, too). Consider all nonzero $W_{b_ic_j}$: because \begin{align}
	b_i = \sum_{c_j} W_{b_ic_j}c_j+B_i,
\end{align}  when all $c_j$ of $+,-$ sign get $s(c_j)$ value, $b_i$ also gets its upper bound. Because $b_i$ is in the second layer, this upper bound is equal to the DeepPoly upper bound of $b_i$. 


Moreover, by the algorithm of DeepPoly, for any case of $\bar{f_i}$, if $b_i$ get its DeepPoly upper bound, then $\bar{f_i}(b_i)=\ReLU(b_i)$. 

Put all together, when all $c_j$ of sign $+,-$ gets values $s(c_j)$, $b_i=\sum_{c_j}W_{b_ic_j}s(c_j)+B_i$ gets its actual upper bounds and DeepPoly upper bounds, and then  $\bar{f_i}(b_i)=\ReLU(b_i)$, the actual value of $\hat{b}_i$. This is what we want to show.
\end{proof}



\subsubsection*{negative}Now we consider a node $b_i$ that $W_{xb_i}<0$. All are similar. Now, each $W_{b_ic_j}$ has the opposite sign of $c_j$. So when all $c_j$ of $+,-$ sign get $s(c_j)$ value, $b_i$ also gets its lower bound and DeepPoly lower bound. By the assumption, when $b_i$ gets its DeepPoly lower bound, $\underline{f_i}(b_i)=\ReLU(b_i)$. This is what we want to show.




\end{document}
