\subsection{Related Work} 

We compare Hybrid MILP with major verification tools for DNNs to clarify our methodology and its distinction from the existing state-of-the-art. It scales while preserving good accuracy, through targeting a limited number of binary variables, stricking a good balance between exact encoding of a DNN using MILP~\cite{MILP} (too slow) and LP relaxation (too inaccurate). MIPplanet~\cite{MIPplanet} opts for a different selection of binary variables, and execute one large MILP encoding instead of Hybrid MILP's many small encodings, which significantly reduce the number of binary variables necessary for each encoding. In \cite{DivideAndSlide}, small encodings are also considered, however with a straightforward choice of binary nodes based on the weight of outgoing edges, which need much more {binary variables} (thus runtime) to reach the same accuracy.
% as our utility function (see Table \ref{tab:example1}), and restrict integer variables to the previous layer, which limits the accuracy reachable.

Compared with \cite{atva}, which uses pMILP in an abstraction refinement loop, 
they iteratively call pMILP to obtain bounds for the same output neuron, opening more and more ReLUs. This scales only to limited size DNN (500 neurons), because of the fact that many ReLUs need to be open (and then Gurobi takes a lot of time) and the iterative nature which cannot be parallelized, unlike our method which scales up to 20.000 neurons.

%Instead, we call pMILP on each hidden neuron *once*, layer per layer (divide and conquer), with a fixed limited number of open ReLUs thanks to our novel SAS selection.
%pMILP opens the most important ReLUs for a given neuron. 

The fact that pure BaB is not that efficient for e.g. verification-agnostic (even very small) DNNs has been witnessed before \cite{MILP2}. The workaround, e.g. in {\em refined} $\alpha,\beta$-CROWN, was to precompute very accurate bounds for the first few neurons of the DNN using a complete full MILP encoding, and then rely on a BaB call from that refined bounds (more complex calls to full MILP and BaB). Non-surprisingly, this very slow technique does not scale but to small DNNs (max 2000 ReLU activation functions). Hybrid MILP on the other hand relies only on small calls: it is much more efficient on small DNNs, and it can scale to larger DNNs as well: we demonstrated strong performance with at least one order of magnitude larger networks (CNN-B-Adv).

%Hybrid MILP can be seen as a refinement of $\alpha,\beta$-Crown~\cite{crown}, though its refined accurate path is vastly different than the base. This is not the case of Hybrid MILP, see Table \ref{table_hybrid}, which is much more accurate than $\alpha,\beta$-Crown.
%That shortcoming for hard instances was witnessed in \cite{crown}, and a very specific solution using the full MILP encoding for the first few layers of a DNN was drafted, following similar proposal \cite{MILP2}. The main issue is that it is slow
%and it cannot scale to DNNs with many neurons, as every neurons are encoded using an integer variable, making it not that accurate for intermediate networks (e.g. $9\times100$, $9\times200$, Table \ref{table_hybrid}), and not usable for larger DNNs ($6\times500$, CNN-B-Adv), whereas Hybrid MILP does scale.


Last, ERAN-DeepPoly \cite{deeppoly} computes bounds on values very quickly, by abstracting the weight of every node using two functions: an upper function and a lower function. While the upper function is fixed, the lower function offers two choices.
It relates to the LP encoding through Proposition \ref{LP} \cite{alessandro}: the LP relaxation precisely matches the intersection of these two choices. Consequently, LP is more accurate (but slower) than DeepPoly, and Hybrid MILP is considerably more precise. Regarding PRIMA \cite{prima}, the approach involves explicitly maintaining dependencies between neurons.


%{\color{red} MN-BaB \cite{ferrari2022complete} is another state-of-the-art verifier which can be regarded as a development of PRIMA. As indicated by its name, it uses a combination technique of multi-neuron constraints and Branch and Bound. According to their experiments results, MN-BaB has similar speed and accuracy as $\alpha$,$\beta$ CROWN. Therefore, we do not compare our experiments results to MN-BaB separately.}

%For verification-agnostic DNNs that are not too large,  $\beta$-CROWN (and PRIMA) resorts to a {\em refined} path \cite{MILP2}, where the bounds {\em on the first few layers} are refined using an exact MILP encoding. In {\CMP}, we do not use an exact encoding but a partial one with the most important ReLU nodes obtained by considering the compensation strength. As it is more efficient, we compute bounds for neurons in {\em all} the layers. This would be infeasible without the selection based on the compensation. The refined version of $\beta$-CROWN is particularly accurate on small DNNs. As the depth grows, the more work is left to BaB and \CMP is more accurate (Table \ref{tab:example}).

Finally, methods such as Reluplex / Marabou \cite{Reluplex,Marabou}  abstract the network: they diverge significantly from those abstracting values such as PRIMA, $\alpha,\beta$-CROWN)\cite{prima,crown}, Hybrid MILP. These network-abstraction algorithms are designed to be {\em intrinsically complete} (rather than asymptotically complete as BaB), but this comes at the price of significant scalability challenges, and in practice they time-out on complex instances as shown in Table \ref{table_complete}.