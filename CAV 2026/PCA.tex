\section{PCA Model Order Reduction for Bound Computation}

%Some words on PCA, learn of 2 Matrix full-dim to reduced-dim and back.
%Then, how we use it (pictures and text).


As explained in the previous section, global robustness is a very complex problem. On top of efficient MILP encodings, we propose to leverage an orthogonal technique, stemming from Engineering Science: Model Order Reduction.
The idea is to limit the search space to a given dimensions. This will have several benefits: the search is faster, it produces better upper bounds, 
and the lower bounds produced are much more natural wrt the dataset.
The simplest Model Order Reduction uses Primary Component Analysis (PCA) \cite{Paco}: PCA  is a linear technique to reduce the dimension of a dataset while keeping its main information. From a dataset, the most important linear components of this dataset are computed.
Projecting over the first few main components is a powerful model order reduction technique ({\em encoding} / {\em reducing from the full dimension}). 
It is easy to go from the reduced dimension back to the full dimension
by making the product between the reduced vector and the first few eigenvectors ({\em decoding}). The pipeline to reduce the dimensions is as follows (see Fig.~\ref{fig.MNIST}):

\begin{enumerate}
\item {\em Training} the PCA encoding and decoding from the Training DataSet (e.g. MNIST, FMNIST, CIFAR10). The DNN has been trained from the same dataset (on full dimension). 

\item {\em In Exploitation}, we call the DNN after an encoding /decoding of images through PCA. The input image is minimally changed by such a process. 
Although the direct images input of the DNN are full size, they live in a reduced space. We check the accuracy of this pipeline, that is the $\%$ of images where the class predicted matches the ground truth. We set the reduced dimension as small as possible (more efficient, 20 for MNIST) such that the loss of accuracy is minimal ($ \leq 1 \%$) wrt the original DNN without PCA.
% enccode/decode.

\item {\em In Bound Computation}, we consider as variables the original images, perturbed imaged and the original bound $\varepsilon$ on the perturbation, 
then the reduced PCA dimensions ($\times 2$: input and perturbed input'),
then the transformed image and perturbation through PCA encode/decode.
All these variables are linear, and the associated constraints, which come from PCA encode/decode, are also linear. Next, we have the exact same MILP encoding (e.g. {\em Diff} encoding) for the DNN($\times 2$). Hence, the number of binary variable is the same as without PCA. Finally, the goal is to optimize e.g. $\beta^\varepsilon_{6,8}$, the value of $o_6 -o_8+ o'_8 - o'_6$,  where $o_C$ is the output neuron of the DNN corresponding to class $C \in \{0,\ldots, 9\}$.
\end{enumerate}

\begin{figure*}[t!]
    \centering
    \includegraphics[scale=0.9]{MNIST.pdf} \hspace{1.5cm}
    \caption{Training, exploitation, and bound computation on the MNIST dataset.}
    \label{fig.MNIST}
\end{figure*}	


%
%It was also used to obtain the PCA encoding and decoding, transforming the images into a reduced basis. The number of PCA dimensions was chosen such that the exploitation accuracy remains constant. That is, encoding into a PCA basis and decoding it results in the same accuracy, as illustrated in ``Exploitation''. 
%
%Last, for the ``bound computation'', we determine an $L_1$-perturbation $\varepsilon$. The search space for bound computation is 20-dimensional. However, the bound itself is computed on the full space. This is possible since PCA is a \emph{linear operation} based on the eigenvectors of the covariance matrix and its transpose for the PCA decoding (inverse).
