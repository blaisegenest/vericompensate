While deep neural networks (DNNs) have demonstrated remarkable capabilities, achieving human-like or even superior performance across a wide range of tasks, their robustness is often compromised by their susceptibility to input perturbations \cite{szegedy}. This vulnerability has catalyzed the formal verification community to develop various methodologies, each presenting a unique trade-off between completeness and computational efficiency~\cite{Marabou,Reluplex,deeppoly}. 
This surge in innovation has also led to the inception of competitions such as VNNComp~\cite{VNNcomp}, which aims to systematically evaluate the performance of neural network verification tools. Notable examples include NNenum~\cite{nnenum}, Marabou~\cite{Marabou,Marabou2}, and PyRAT~\cite{pyrat}, as well as frameworks such as MnBAB~\cite{ferrari2022complete} (which builds upon ERAN~\cite{deeppoly} and PRIMA~\cite{prima}) and $\alpha,\beta$-CROWN~\cite{crown,xu2020fast}, based on branch-and-bound methodology \cite{cutting,BaB}.

These tools focus on {\em local} robustness: given a DNN, an image, and a small neighborhood around this image, they verify whether all images in the neighborhood are assigned to the same classification. This neighborhood is provided by a maximal perturbation of the input image, often an $L_\infty$-norm constraint. Under this metric, every subpixel\todo{what is a subpixel here? pixel component like each R/G/B?} of the input image can vary in a very small range, typically $\frac{2}{255}$ (that is, 2 levels of grey/blue/red/green). 
Although it is not necessarily the most semantically meaningful perturbation, $L_\infty$ is the usual choice because it is easier to verify since it is perfectly linear and specifies subpixel perturbations independently.

%- to line break
Crucially, however, these verification tools for local robustness are too compu-tationally-intensive for real-time decision-making pipelines. For instance, consider an autonomous vehicle processing a live video feed: images of the feed cannot be certified robust in a few ms on embedded hardware. \eca{This prevents the development of safety filters that could skip non-robust images and only consider certified robust images.}

\smallskip

\todo{shouldn't we unify notation w/ section4?}
In this paper, we consider {\em global} robustness, that is we do not restrict the certification process to the neighborhood of a specific, fixed input. We follow a two-step procedure. 
The first step, performed offline, computes global bounds on the \eca{maximum possible} shift between output values of different decision classes due to a perturbation, following an approach similar to VHAGaR~\cite{vhagar}. 
%
Specifically, for any two decision classes $C$ and $D$ and perturbation $\varepsilon$, we compute an 
upper bound 
$\bar{\beta}^\varepsilon_{C,D}$ of $\max_{I,I', |I-I'| \leq \epsilon}(value_{I}(C) - value_{I}(D) + value_{I'}(D) - value_{I'}(C))$
, where $value_{J}(X)$ is the output value of class $X \in \{C,D\}$ for input image $J \in \{I,I'\}$. 
%
Unlike local robustness methods, which require $k$ separate calls, computationally expensive calls to certify $k$ images, the global bound $\bar{\beta}^\varepsilon_{C,D}$ is computed offline once per network and remains valid across the entire input space. 

The second step is real-time, being performed with the DNN inference of the image $I$ considered: it suffices to consider the class $C$ with the highest output value $value_{I}(C)$, and check whether for every other class $D \neq C$, 
$value_{I}(C) - value_{I}(D) > \bar{\beta}^\varepsilon_{C,D}$. 
%
If this is the case, then we are certified that image $I$ is robust for perturbation $\varepsilon$, because $\varepsilon$-perturbed image $I'$ could at most get  $value_{I'}(D) \leq \bar{\beta}^\varepsilon_{C,D}  - (value_{I}(C) - value_{I}(D))  + value_{I'}(C) < value_{I'}(C)$, hence $C$ is also the predicted class for image $I'$. 
%
This check typically requires only a few dozen CPU instructions, which can be completed under 1 ms on a single CPU core. When an image fails this certification, one could either skip it (e.g., in a video stream) or revert to a safer, degraded mode until a trustworthy, robust image is received.

Our main contributions address the challenges of computing the {\em global bounds} $\bar{\beta}^\varepsilon_{C,D}$, for $C,D$ output neurons:
\begin{enumerate}
	\item We develop a novel {\em Diff MILP encoding} for the global robustness problem, 
	where the variables are the values of the perturbed neurons, as well as the difference between the original and the perturbed neuron values (called the {\em diff variables}, introduced in \cite{diff}). We study and encode how the {\em diff variables} evolve after passing through a ReLU (Prop.~\ref{Prop2}), see Section \ref{s.diff}. 
	Compared with the {\em classical MILP model} \cite{MILP} employed in 
	\cite{vhagar,lipshitz,ITNE}, which considers the input and the perturbation but {\em diff variable} \todo{and the diff variable? I don't get? 3 vars for them?}, we keep the same number of 2 binary variables per ReLU.
	However, our {\em Diff MILP model} is more efficient, one reason being the accuracy of its linear relaxation.

	\item Further, from the {\em Diff MILP model}, which is exact, 
	we develop two abstract MILP models, which are more efficient but also asymptotically less accurate than {\em Diff}. 
	Namely, the "2b+1" model, accurate on the {\em diff variables} but abstract on the perturbed variables; while the "1b" model has a unique binary variable per ReLU, only considering the {\em diff variables}.

   \item  In terms of perturbations, we consider conjunctions of $L_\infty$- and $L_1$-norms, which allow to accurately describe perturbations. For instance, "each subpixel is perturbed by at most $\frac{50}{255}$ ($L_\infty$) and the sum of the absolute value of perturbations over all subpixels is at most $1$" ($L_1$-perturbation). While $L_1$ perturbations are not linear (because of the absolute values), and thus they are seldom used, we show in Section~\ref{s.L1} how to use them as a perturbation in the MILP model without incurring any expensive binary variables (only cheap linear variables are necessary). 

    \item Bounds obtained on the full input space are particularly pessimistic, as all inputs, including {\em Out of Distribution (OOD \cite{OOD})} inputs far away from the training dataset, need to be accounted for. To address this, we consider model order reduction techniques from engineering science \cite{Paco}. Specifically, we use principal component analysis (PCA) to faithfully represent common inputs from the dataset. OOD inputs may be represented unfaithfully, which is reasonable as the DNN is unlikely to provide a reasonable answer on such inputs anyway. For instance, using just $20$ out of 784 dimensions suffices to represent MNIST inputs faithfully, without losing accuracy for the DNN on the MNIST dataset.

    \item Experimentally, the {\em Diff MILP model} computes upper bounds
    $\bar{\beta}^{\varepsilon}_{C,D}$ which reduces the gap to the lower bound compared with the optimized version of the classical MILP encoding; namely, reducing the number of binary variables (VHAGaR \cite{vhagar}); or adding linear constraints from the {\em diff variables} (ITNE \cite{ITNE}), see Table \ref{table.classical}.
    The abstractions "2b+1" and "1b" variant offer different trade-offs, reaching better bounds than the {\em Diff MILP} model when the instance is very complex, or when runtime is limited (Table \ref{table.L1}).  
    Further, using PCA reduces the upper bound $\beta_{C,D}$ by $3$ to $20$ times.
    Overall, using these different techniques ({\em Diff} MILP model, abstraction, and PCA) enables the real-time certification of $\geq 70\%$ of fresh images for an $L1$-perturbation of $1,4,2$ over the MNIST, Fashion-MNIST, and CIFAR-10 datasets, respectively, see Table \ref{table.cert}. The online process only adds 0.5ms of latency per image, and 2000 images/second can be treated per CPU core.
\end{enumerate}