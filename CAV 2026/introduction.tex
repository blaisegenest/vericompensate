Deep neural networks (DNNs for short) have demonstrated remarkable capabilities, achieving human-like or even superior performance across a wide range of tasks. However, their robustness is often compromised by their susceptibility to input perturbations \cite{szegedy}. This vulnerability has catalyzed the verification community to develop various methodologies, each presenting a unique balance between completeness and computational efficiency \cite{Marabou,Reluplex,deeppoly}. This surge in innovation has also led to the inception of competitions such as VNNComp \cite{VNNcomp}, which aim to systematically evaluate the performance of neural network verification tools. Among them, NNenum \cite{nnenum}, Marabou \cite{Marabou,Marabou2}, and PyRAT \cite{pyrat} MnBAB \cite{ferrari2022complete}, built upon ERAN \cite{deeppoly} and PRIMA \cite{prima}; and $\alpha,\beta$-CROWN \cite{crown,xu2020fast}, 
%the winner of the last 4 VNNcomp, benefiting from 
based on branch-and-bound based methodology \cite{cutting,BaB}.

These tools %benchmarks usually 
focus on {\em local} robustness, i.e. given a DNN, an image and a small neighborhood around this image, is it the case that all the images in the neighborhood are classified in the same way by the DNN? The neighborhood is provided by a maximal perturbation of the input image, often an 
$L_\infty$-perturbation, i.e. every subpixel of the input image can vary in a very small range, typically $\frac{2}{255}$ (that is 2 levels of grey/blue/red/green). 
Although it is not necessarily the most meaningful perturbation,
$L_\infty$ is the usual choice because it is perfectly linear and specifies 
subpixel perturbations independantly, which is easier to verify. 
Importantly, these verification tools for local robustness are too computationally-intensive to be used in a real-time decision making pipeline: considering an autonomous car with a video feed from the dashboard, 
images of the feed cannot be certified robust in few ms on embedded hardware to e.g. skip non-robust images and only consider certified robust images.
% for the decision-making process.

\smallskip

In this paper, we consider {\em global} robustness, that is we do not restrict the certification process to the neighborhood of a fix input. We follow a two steps procedure. 
The first step, performed offline once, computes global bounds on the shift between 
output values of different decision classes due to the perturbation, close to \cite{vhagar}. That is, considering decision classes $C$ and $D$ and perturbation $\varepsilon$, compute an 
upper bound $\bar{\beta}^\varepsilon_{C,D}$ of $\max_{I,I', |I-I'| \leq \epsilon}(value_{I}(C) - value_{I}(D) + value_{I'}(D) - value_{I'}(C))$, where $value_{J}(X)$ is the output value of class $X \in \{C,D\}$ for input image $J \in \{I,I'\}$. %Notice that as $I,I',C,D$ have symetrical roles, we can choose $\beta^\varepsilon_{D,C} = \beta^\varepsilon_{C,D}$. Also, as $I,I'$ have symetrcal role, the minimum value is exactly $-$ the maximum value. 
%Hence, if we have $n$ decision classes, we only have to compute $n (n-1)/2$ bounds. 
Bound $\bar{\beta}^\varepsilon_{C,D}$ is computed offline once for a DNN, 
and it is valid over the whole input space; compared with $k$ calls to 
{\em local} robustness, once for each of the $k$ input images, with results only valid for these $k$ images.

The second step is real-time, being performed with the DNN inference of the image $I$ considered: it suffices to consider the class $C$ with the highest output value $value_{I}(C)$, and check whether for every other class $D \neq C$, 
$value_{I}(C) - value_{I}(D) > \bar{\beta}^\varepsilon_{C,D}$. 
If this is the case, then we are certified that image $I$ is robust for perturbation $\varepsilon$, because $\varepsilon$-perturbed image $I'$ could at most get  $value_{I'}(D) \leq \bar{\beta}^\varepsilon_{C,D}  - (value_{I}(C) - value_{I}(D))  + value_{I'}(C) < value_{I'}(C)$, hence $C$ is also the predicted class for image $I'$. This typically needs a couple of tens CPU instructions, which can be performed under 1ms (mili-second) on a single CPU core. If the image is not certified robust, one could either skip image $I$ (in a video feed), or use safer degraded mode in the decision making process till a trustable robust image is received.


Our main contributions address the challenges to compute the {\em global bounds} $\bar{\beta}^\varepsilon_{C,D}$, for $C,D$ output neurons:
% for standard ReLU DNNs (e.g. \cite{vhagar}). Our findings can be extended to other activation functions, following similar extention by \cite{DivideAndSlide}:
%, with updated MILP models e.g. for maxpool:
\begin{enumerate}
	%\item  Our first contribution studies the {\em LP relaxation} of the exact MILP encoding of ReLUs. {\color{blue} We establish in Proposition \ref{LP} its equivalence with the so-called "triangular abstraction"}.
	
	\item We develop a novel {\em Diff MILP encoding} for the global robustness problem, %called the {\em "2v" model}, 
	where the variables are the values of the perturbed neurons, as well as the difference between the original and the perturbed neuron values (called the {\em diff variables}, introduced in \cite{diff}). We study and encode how the {\em diff variables} evolve after passing through a ReLU (Prop.~\ref{Prop2}), see Section \ref{s.diff}. 
	Compared with the {\em classical MILP model} \cite{MILP} employed in 
	\cite{vhagar,lipshitz,ITNE}, which considers the input and the perturbation but {\em diff variable}, we keep the same number of 2 binary variables per ReLU.
	%the results are more accurate for the same runtime.
	%the linear relaxation is much more accurate, as each {\em diff variable} can be bounded after a ReLU as a function of the value of the {\em diff variables} before the ReLU, whereas the linear relaxations of the classical model is extremely inaccurate, as the variables are independent of each other. This was observed in \cite{lipshitz,ITNE}, and constraints encoding
	%a part of the linear relaxation of our "2v" model were added explicitly. 
	%Experimentally, 
	However, our {\em Diff MILP model} is more efficient, one reason being 
	the accuracy of its linear relaxation.
	%The number of variables a priori doubles compared with local robustness, from each neuron value in the perturbed image to each neuron value in the perturbed image {\em and} in the original image, as the original image is no more fixed. 
	%Recall that the worst case complexity of MILP is exponential in the number of binary variables \cite{DivideAndSlide}. 
	%A straightforward MILP model would be to use the  for each of these variables, as in \cite{vhagar,lipshitz}. The main issue with the classical model is that its linear relaxations is extremely inaccurate, as the variables are independent of each other. Instead, we develop another exact MILP model, 

	\item Further, from the {\em Diff MILP model}, which is exact, 
	we develop two abstract MILP models, which are more efficient but also asymptotically less accurate than {\em Diff}. 
	Namely, the "2b+1" model, accurate on the {\em diff variables} but abstract on the perturbed variables; while the "1b" model has a unique binary variable per ReLU, only considering the {\em diff variables}.
	%\item We adapt the Solution Aware Scoring from \cite{ATVA25} to our novel MILP models, in order to select the most important ReLUs to be treated using complex binary variables, while less important variables are treated using linear relaxation. The chosen number of binary variables depends upon the complexity of the DNN as well as the targeted runtime.

   \item  In terms of perturbations, we consider conjunctions of $L_\infty$- and $L_1$-norms, which allow to accurately describe perturbations. For instance, "each subpixel is perturbed by at most $\frac{50}{255}$ ($L_\infty$) and the sum of the absolute value of perturbations over all subpixels is at most $1$" ($L_1$-perturbation). While $L_1$-perturbations are not linear (because of the absolute values), reason for which it is seldom used, we show in Section~\ref{s.L1} how to use it as a perturbation in the MILP model without incurring any expansive binary variables (only cheap linear variables are necessary). 



\item Bounds obtained on the full input space are particularly pessimistic, as all inputs, including {\em Out of Distribution (OOD \cite{OOD})} inputs far away from the training dataset, need to be accounted for. 
%As a result, the runtime to obtain the bound is particularly long. 
	%Finally, when computing worst-case 
	%pairs (image, perturbation), improbable images are generated, hence these  are not meaningful. 
To address this, we consider model order reduction techniques from engineering science \cite{Paco}. Specifically, we use Principal Component Analysis (PCA) to represent faithfully common inputs from the dataset. OOD	inputs may be represented unfaithfully, which is reasonable as the DNN is unlikely to provide reasonable answer on such inputs anyway.
	%	focus on reduce the space to a linear input space. We choose the number of dimensions of the space to equal the accuracy of the DNN on the reduced space. 
	%(using a projection to the reduced space then the inverse projection to obtain a very similar image understandable by the DNN). 
For instance, using just $20$ out of 784 dimensions suffices to represent faithfully MNIST inputs, without losing accuracy for the DNN on the MNIST dataset.
	%On the MNIST benchmark, this means 20 linear dimensions to match the $97\%$ accuracy of the DNN we considered, instead of the 784 dimensions of the full image space. 
	%Using PCA, which is linear, makes it easy to specify perturbations on the actual image (where it is meaningful) rather than on the reduced space.

\item Experimentally, the {\em Diff MILP model} computes upper bounds
$\bar{\beta}^{\varepsilon}_{C,D}$ which reduces the gap to the lower bound compared with optimized version of the classical MILP encoding; namely reducing the number of binary variables (Vhagar \cite{vhagar}); or adding linear constraints from the {\em diff variables} (ITNE \cite{ITNE}), see Table \ref{table.classical}.
The abstractions  "2b+1" and "1b" variant offer different trade-offs, 
reaching better bounds than the {\em Diff MILP} model when the instance is very complex, or when runtime is limited (Table \ref{table.L1}).  
Further, using PCA reduces the upper bound $\beta_{C,D}$ by $3$ to $20$ times.
Overall, using these different techniques ({\em Diff} MILP model, abstraction and PCA) enables the real-time certification of $> 67\%$ of fresh images for an $L1$-perturbation of $1,4,2$ over the MNIST, Fashion MNIST and CIFAR-10 datasets respectively, see Table \ref{table.cert}. The online process adds only 0.5ms of latency per image, and 2000 images/second can be treated per CPU core.
\end{enumerate}


%\newpage

%   
% 
%
%In this context, application of DNNs in safety critical applications is cautiously envisioned. For that to happen at a large scale, hard guarantees should be provided \cite{certification}, through e.g. incremental verification \cite{incremental}, so that to avoid dramatic consequences. It is the reason for the development of (hard) verification tools since 2016, with now many tools with different trade-offs from exact computation but slow (e.g. Marabou \cite{katz2019marabou}/Reluplex\cite{Reluplex}), up to very efficient but also incomplete (e.g. ERAN-DeepPoly \cite{deeppoly}). To benchmark these tools, a competition has been run since 2019, namely VNNcomp \cite{VNNcomp}. The current overall better performing verifier is $\alpha$-$\beta$-CROWN \cite{crown}, a fairly sophisticatedly engineered tool based mainly on "branch and bound" (BaB) \cite{BaB}, and which can scale all the way from complete on smaller DNNs \cite{xu2020fast} up to very efficient on larger DNNs, constantly upgraded, e.g. \cite{cutting}. 
%
%While the verification engines are generic, the benchmarks usually focus on local robustness, i.e. given a DNN, an image and a small neighbourhood around this image, 
%is it the case that all the images in the neighbourhood are classified in the same way.
%While some quite large DNNs (e.g. ResNet with tens of thousands of neurons) can be verified very efficiently (tens of seconds per input) \cite{crown}, with all inputs either certified robust or an attack on robustness is found; some smaller DNNs (with hundreds of neurons, only using the simpler ReLU activation function) cannot be analysed fully, with $12-20\%$ of inputs where neither of the decisions can be reached (\cite{crown} and Table \ref{tab:example}). Actually, DNNs which are trained to be robust (using DiffAI \cite{DiffAI} or PGD \cite{PGD}) are easier to verify, while the DNNs trained in a "natural" way are harder to verify.
%
%
%In this paper, we focus on DNNs trained in a "natural" way,
%%uncovering what makes the DNNs trained in a natural way so hard to verify (
%because for "easier" DNNs, adequate methods already exist. 
%To do so, we analyse the abstraction mechanisms at the heart of several efficient algorithms, namely Eran-DeepPoly \cite{deeppoly}, the Linear Programming approximation \cite{MILP}, PRIMA \cite{prima}, and different versions of ($\alpha$)($\beta$)-CROWN \cite{crown}. All these algorithms compute lower or/and upper bounds for the values of neurons (abstraction on values) for inputs in the considered input region, and conclude based on such bounds. For instance, if for all image $I'$ in the neighbourhood of image $I$, we have $weight_{I'}(n'-n) < 0$ for $n$ the output neuron corresponding to the expected class, then we know that the DNN is robust in the neighbourhood of image $I$. We restrict the formal study to DNNs using only the standard ReLU activation function, although nothing specific prevents the results to be extended to more general architectures. We uncover that {\em compensations} 
%(see next paragraph) is the phenomenon creating inaccuracies. We verified experimentally that a DNN trained in a natural way has heavier compensating pairs than DNNs trained in a robust way.
%
%Formally, a compensating pair is a pair of paths $(\pi,\pi')$ between a pair of neurons $(a,b)$, such that we have $w < 0 < w'$, for $w,w'$ the products of weight seen along $\pi$ and $\pi'$. Ignoring the (ReLU) activation functions, the weight of $b$ is loaded with $w \cdot weight(a)$ by $\pi$, while it is loaded with $w' \cdot weight(a)$ by $\pi'$. That is, it is loaded by $(w+w') weight(a)$. As $w,w'$ have opposite sign, they will compensate (partly) each other. The compensation is only partial due to the ReLU activation seen along the way of $\pi$ which can "clip" a part of $w \cdot weight(a)$, and similarly for $\pi'$. However, it is very hard to evaluate by how much without explicitly considering both phases of the ReLUs, which all the efficient tools try to avoid because it is very expansive (could be exponential in the number of such ReLU nodes opened).

%Our first main contribution is to formally show, in Theorem \ref{th1}, that compensation is the sole reason for the inaccuracies as (most) efficient algorithms will compute exact bounds for all neurons if there is no compensating pair of paths at all.
%While this theorem is theoretically interesting, it is not usable in practice as (almost) all networks have some compensating pairs. However, this notion of compensating pairs opens a first interesting idea concerning an exact abstraction of the network using a Mixed Integer Linear Program \cite{MILP}, where the weight of each neuron is a linear variable, and ReLU node may be associated with binary variables (exact encoding) or linear variables (overapproximation). While LP tools can scale to thousands of linear variables, MILP encoding can only be solved for a limited number of binary variables. This suggests that a simpler encoding could be used for those ReLUs that are not on compensating pairs, as their precise outcome may not be necessary.

%Our second main contribution is to show formally in Theorem \ref{th2}, that 
%encoding all ReLU nodes on a pair of compensating paths with a binary variable,
%and using linear relaxation for the other ReLU nodes, will lead to exact bounds for (most) of the algorithms considered. This theorem allows to restrict the number of integer variables, and thus to obtain encodings that are faster to solve. Practically, however, (almost) all ReLU nodes are on some compensating path, and using this exact restricted MILP encoding will be too time consuming.

%Our third main contribution is more practical, proposing Algorithm \ref{algo1} based on this knowledge that compensating pair of paths are the reason for inaccuracy. The idea is thus to use this information to rank the ReLU nodes in terms of importance, and only keep the most important ones as binary variables, and use linear relaxation for the least important ones.
%%More precisely, the algorithm will, as DeepPoly, consider layers one by one and neurons $b$ %on this layer one by one, selecting the heaviest pairs of compensating paths ending in $b$
%%and associating these nodes with a binary variable. Then an MILP tool such as Gurobi is used %to compute the lower and upper bound for node $b$. 
%Overall, the worst case complexity of algorithm \ref{algo1} is lower than $O(N 2^K LP(N))$, where $N$ is the number of nodes of the DNN, $K$ the number of ReLU nodes selected as binary variable, and $LP(N)$ is the (polynomial time) complexity of solving a linear program representing a DNN with $N$ nodes. This complexity is an upper bound, as e.g. Gurobi is fairly efficient and never need to consider all of the $2^K$ ReLU configurations to compute the bounds. Keeping $K$ reasonably low thus provides an efficient algorithm. 
%By design, it will never run into a complexity wall (unlike the full MILP encoding), although it can take a while on large networks because of the linear factor $N$ in the number of nodes. An additional interesting point is that it is extremely easy to parallelize, as all the nodes in the same layer can be run in parallel. We verify experimentally that the algorithm offers interesting trade-offs, by testing on local robustness for DNNs trained "naturally" (and thus difficult to verify).


%KSM: I suggest we move this to experimental evaluation
%This paper does not focus on producing the most efficient tool, and we did not spend engineering efforts to optimize it. The focus is instead on the novel notion of compensation, the associated methodology and its evaluation. For instance, our implementation is fully in Python, with uncompetitive runtime for our DeepPoly implementation ($\approx 100$ slower than in CROWN). Still, evaluation of the methodology versus even the most efficient tools reveals a lot of potential for the notion of compensation, opening up several opportunities for applying it in different contexts of DNN verification (see Section \ref{Discussion}). 

