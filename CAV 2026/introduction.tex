While deep neural networks (DNNs) have demonstrated remarkable capabilities, achieving human-like or even superior performance across a wide range of tasks, their robustness is often compromised by their susceptibility to input perturbations \cite{szegedy}. This vulnerability has catalyzed the formal verification community to develop various methodologies, each presenting a unique trade-off between completeness and computational efficiency~\cite{Marabou,Reluplex,deeppoly}. 
This surge in innovation has also led to the inception of competitions such as VNNComp~\cite{VNNcomp}, which aims to systematically evaluate the performance of neural network verification tools. Notable examples include NNenum~\cite{nnenum}, Marabou~\cite{Marabou,Marabou2}, and PyRAT~\cite{pyrat}, as well as frameworks such as MnBAB~\cite{ferrari2022complete} (which builds upon ERAN~\cite{deeppoly} and PRIMA~\cite{prima}) and $\alpha,\beta$-CROWN~\cite{crown,xu2020fast}, the winner of the last 4 VNNComp, based on branch-and-bound methodology \cite{cutting,BaB}.

These tools focus on {\em local} robustness: given a DNN, an image, and a small neighborhood around this image, they verify whether all images in the neighborhood are assigned to the same classification. This neighborhood is provided by a maximal perturbation of the input image, often an $L_\infty$-norm constraint. Under this metric, every subpixel of the input image can vary in a very small range, typically $\frac{2}{255}$ (that is, 2 levels of gray/blue/red/green). 
Although it is not necessarily the most meaningful perturbation,
$L_\infty$ is the usual choice because it is perfectly linear  
(subpixel perturbations are independent), which is easier to verify.

%- to line break
Crucially, however, these verification tools for local robustness are too computationally-intensive for real-time decision-making pipelines. 
For instance, consider an autonomous vehicle processing a live video feed: images of the feed cannot be certified robust in a few ms on embedded hardware. This prevents the development of safety filters that could skip non-robust images and only consider certified robust images.

\smallskip

In this paper, we consider {\em global} robustness, that is we do not restrict the certification process to the neighborhood of a specific, fixed input. We follow a two-step procedure. 
The first step, performed offline, computes global bounds on the maximum possible shift between output values of different decision classes due to a perturbation, following an approach similar to VHAGaR~\cite{vhagar}. 
%
Specifically, for any two decision classes $C$ and $D$ and perturbation $\varepsilon$, we compute an 
upper bound 
$\bar{\beta}^\varepsilon_{C,D}$ of $\max_{I,I', |I-I'| \leq \epsilon}(
x_C - x'_C + x'_D - x_D)$, where $x_{C,D},x'_{C,D}$ are the output value of class $C,D$ from image $I$ and $I'$. 
%
Unlike local robustness methods, which require $k$ separate calls, computationally expensive calls to certify $k$ images, the global bound $\bar{\beta}^\varepsilon_{C,D}$ is computed offline once per network and remains valid across the entire input space. 

The second step is real-time, being performed with the DNN inference of the image $I$ considered: it suffices to consider the class $C$ with the highest output value $x_C$, and check whether for every other class $D \neq C$, 
$x_C - x_D > \bar{\beta}^\varepsilon_{C,D}$. 
%
If this is the case, then we are certified that image $I$ is robust for perturbation $\varepsilon$, because $\varepsilon$-perturbed image $I'$ could at most get 
$x'_D \leq \bar{\beta}^\varepsilon_{C,D}  - (x_C - x_D)  + x'_C < x'_C$, hence $C$ is also the predicted class for image $I'$. 
%
This check typically requires only a few dozen CPU instructions, which can be completed under 1 ms on a single CPU core. When an image fails this certification, one could either skip it (e.g., in a video stream) or revert to a safer, degraded mode until a trustworthy, robust image is received.

Our main contributions address the challenges of computing the {\em global bounds} $\bar{\beta}^\varepsilon_{C,D}$ for all pair of classes $(C,D)$:
\begin{enumerate}
	\item We develop a novel {\em Diff MILP encoding} for the global robustness problem, 
	where the variables are the values of the perturbed neurons, as well as the difference between the original and the perturbed neuron values (called the {\em diff variables}, introduced in \cite{diff}). We study and encode how the {\em diff variables} evolve after passing through a ReLU (Prop.~\ref{Prop2}), see Section \ref{s.diff}. 
	Compared with the {\em classical MILP model} \cite{MILP} employed in 
	\cite{vhagar,lipshitz,ITNE}, which considers the input and the perturbation instead of the {\em diff variable}, we keep the same number (2) of binary variables per ReLU.
	%the results are more accurate for the same runtime.
	%the linear relaxation is much more accurate, as each {\em diff variable} can be bounded after a ReLU as a function of the value of the {\em diff variables} before the ReLU, whereas the linear relaxations of the classical model is extremely inaccurate, as the variables are independent of each other. This was observed in \cite{lipshitz,ITNE}, and constraints encoding
	%a part of the linear relaxation of our "2v" model were added explicitly. 
	%Experimentally, 
	However, our {\em Diff MILP model} is more efficient, one reason being 
	the accuracy of its linear relaxation.
	%The number of variables a priori doubles compared with local robustness, from each neuron value in the perturbed image to each neuron value in the perturbed image {\em and} in the original image, as the original image is no more fixed. 
	%Recall that the worst case complexity of MILP is exponential in the number of binary variables \cite{DivideAndSlide}. 
	%A straightforward MILP model would be to use the  for each of these variables, as in \cite{vhagar,lipshitz}. The main issue with the classical model is that its linear relaxations is extremely inaccurate, as the variables are independent of each other. Instead, we develop another exact MILP model, 

	\item Further, from the {\em Diff MILP model}, which is exact, 
	we develop two abstract MILP models, which are more efficient but also asymptotically less accurate than {\em Diff}. 
	Namely, the {\em 2b+1} model, accurate on the {\em diff variables} but abstract on the perturbed variables; while the {\em 1b} model has a unique binary variable per ReLU, only considering the {\em diff variables}.

   \item  In terms of perturbations, we consider conjunctions of $L_\infty$- and $L_1$-norms, which allow to accurately describe perturbations. For instance, "each subpixel is perturbed by at most $\frac{50}{255}$ ($L_\infty$) and the sum of the absolute value of perturbations over all subpixels is at most $1$" ($L_1$-perturbation). While $L_1$ perturbations are not linear (because of the absolute values), and thus they are seldom used, we show in Section~\ref{s.L1} how to use them as a perturbation in the MILP model without incurring any expensive binary variables (only cheap linear variables are necessary). 

    \item Bounds obtained on the full input space are particularly pessimistic, as all inputs, including {\em Out of Distribution (OOD \cite{OOD})} inputs far away from the training dataset, need to be accounted for. To address this, we consider 
	principal component analysis (PCA)
	 from engineering science \cite{Paco}. 
	 Specifically, we use PCA to faithfully represent common inputs from the dataset. OOD inputs may be represented unfaithfully, which is reasonable as the DNN is unlikely to provide a reasonable answer on OOD inputs anyway. For instance, $20$ linear components represent MNIST inputs faithfully: the PCA-DNN does not loose accuracy on the MNIST dataset.

\item Experimentally, the {\em Diff MILP model} computes upper bounds
$\bar{\beta}^{\varepsilon}_{C,D}$ which reduces the gap to the lower bound compared with the optimized version of the classical MILP encoding (see Table \ref{table.classical}); namely reducing the number of binary variables (VHAGaR \cite{vhagar}); or adding linear constraints from the {\em diff variables} (ITNE \cite{ITNE}).
The abstractions  {\em 2b+1} and {\em 1b} variant offer different trade-offs, 
reaching better bounds than the {\em Diff MILP} model when the instance is very complex, or when runtime is limited (Table \ref{table.L1}).  
Further, using PCA reduces the upper bound $\bar{\beta}^{\varepsilon}_{C,D}$ by $3$ to $20$ times.
Overall, using these different techniques ({\em Diff} MILP model, abstraction, and PCA) enables the real-time certification of $\geq 70\%$ of fresh images for an $L1$-perturbation of $1,4,2$ over the MNIST, Fashion-MNIST, and CIFAR-10 datasets respectively, see Table \ref{table.cert}. The online process only adds 0.5 ms of latency per image, and 2000 images/second can be treated per CPU core.
\end{enumerate}