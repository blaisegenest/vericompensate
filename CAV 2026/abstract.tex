Deep neural networks (DNNs) are often brittle to small perturbations, which has led to extensive research to verify their robustness. 
Most existing methods focus on {\em local} robustness, i.e., verification in the neighborhood of fixed specific inputs. Local robustness techniques are impractical to guarantee robustness of {\em real-time} inputs on embedded systems, due to excessive latency and computational intensivity.

In this paper, we consider {\em global} robustness, which is significantly more complex than local robustness, as the number of variables doubles (from the deviation image to the image and its deviation). 
We derive {\em bounds} on the variation of output values across different decision classes of a DNN, given $L_\infty$- {\em and} $L_1$-perturbations. We develop novel {\em Diff} MILP models encoding the evolution of {\em diff}erential variables between the %image and its perturbation.
original and perturbed inputs. This encoding is more efficient than the standard MILP encodings that treat variables independently. In addition, tto obtain better bounds, we apply principal component analysis (PCA) which allow us to reduce the considered space and focus on {\em realistic} inputs. 
Our approach achieves {\em real-time} robustness certification for $\geq 70\%$ 
of incoming images with an $L_1$-perturbation of $1,4,2$ on the MNIST, Fashion MNIST and CIFAR-10 datasets, respectively, while adding only 0.5 ms of latency on a single CPU core.