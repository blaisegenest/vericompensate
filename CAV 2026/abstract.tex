Deep neural networks (DNNs) are often brittle to small perturbations, which has led to extensive research to verify their robustness. While most existing methods focus on {\em local} robustness, i.e., verification in the neighborhood of fixed specific inputs, these techniques are often impractical for guaranteeing robustness to {\em real-time} inputs on embedded systems, due to excessive latency and computational cost.

\eca{To tackle real-time verification, we proceed in two steps. First, offline, we solve {\em global} robustness problems, which are significantly more complex than local robustness. 
We then use the results online to perform real-time verification.
%, as the number of variables doubles (from the deviation input to the input and its deviation). 
Offline, we compute} {\em bounds} on the difference of output values across different DNN decision classes under both $L_\infty$- {\em and} $L_1$-perturbations. 
We develop novel {\em Diff} MILP models, explicitly representing the small {\em diff}erential variables between the input and its perturbation, instead of implicitly representing them as differences between 2 larger values (standard MILP encoding). To further improve the bounds, we employ principal component analysis (PCA) to focus on {\em realistic} in-distribution inputs. 
\eca{Online,}
our method verifies in real-time over 70\% of incoming images across standard benchmarks (MNIST, Fashion-MNIST, and CIFAR-10 with $L_1$-perturbations of $1, 4, 2$, respectively), requiring only $.5 ms$ of latency on a single CPU core.