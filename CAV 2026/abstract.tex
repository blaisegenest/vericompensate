Deep neural networks (DNNs) are often brittle to small perturbations, which has led to extensive research to verify their robustness. 
Most existing methods focus on {\em local} robustness, i.e., verification in the neighborhood of fixed specific inputs. Local robustness 
%does {\em not} provide guarantees on whether new specific incoming inputs are robust, e.g., real-time images in a video stream. Moreover, most verification 
techniques are impractical to guarantee robustness of {\em real-time} inputs
on embedded systems, due to excessive latency and computational intensivity.

In this paper, we consider {\em global} robustness, which is significantly more complex than local robustness, as the number of variables doubles (from the deviation image to the image and its deviation). 
%Further, the values each neuron can take are no longer limited to a small neighborhood. 
%, that is, guarantees not restricted to a set of local images. 
We focus on deriving {\em bounds} on the variation of output values across different decision classes of a DNN, given $L_\infty$- {\em and} $L_1$-perturbations. We develop novel {\em Diff} MILP models encoding the evolution of {\em diff}erential variables between the image and its perturbation, more efficient than the classical MILP encoding of variables independently. To obtain better bounds, we reduce the space dimension by using principal component analysis (PCA), focusing on {\em realistic} inputs. These bounds enable the {\em real-time} certification of robustness for $\geq 67\%$ 
of incoming images for an $L_1$-perturbation of $1,4,2$
over the MNIST, Fashion MNIST and CIFAR-10 datasets respectively,
adding only $0.5ms$ of latency using only a single CPU core.