Deep neural networks (DNNs) are often brittle to small perturbations, leading to extensive research into methods to verify their robustness. Most existing methods focus on {\em local} robustness, i.e., the model's behavior in the neighborhood of a specific input. Local robustness does {\em not} provide guarantees on whether new specific incoming inputs are robust, e.g., real-time images in a video stream. Moreover, most verification techniques are computationally intensive, rendering them impractical for real-time deployment on embedded systems. 

In this paper, we consider {\em global} robustness, which is significantly more complex than local robustness, as %the number of variables doubles (from the deviation image to the image and its deviation). Further, 
the values each neuron can take are no longer limited to a small neighborhood. 
%, that is, guarantees not restricted to a set of local images. 
We focus on deriving {\em bounds} on the variation of output values across different decision classes of a DNN, given $L_\infty$- {\em and} $L_1$-perturbations. To obtain usable bounds, we develop {\em global}  MILP models for global robustness, more efficient than the classical encoding and its variants. Last, we reduce the space dimension, focusing on realistic images, by using principal component analysis (PCA). These bounds ensure the {\em real-time} certification of robustness for $69\%$ (resp. $86\%$) of incoming images for an $L_1$-perturbation of $1$ (resp. $.5$), adding only $0.0005s$ of latency using a single CPU core.