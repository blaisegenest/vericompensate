Deep neural networks (DNNs) are often brittle to small perturbations, which has led to extensive research to verify their robustness. While most existing methods focus on {\em local} robustness, i.e., verification in the neighborhood of fixed specific inputs, these techniques are often impractical for guaranteeing robustness to {\em real-time} inputs on embedded systems, due to excessive latency and computational cost.

In this paper, we consider {\em global} robustness, which is significantly more complex than local robustness, as the number of variables doubles (from the deviation input to the input and its deviation). We propose a method to derive {\em bounds} on the difference of output values across different DNN decision classes  under both $L_\infty$ {\em and} $L_1$ perturbations. 
We develop novel {\em Diff} MILP models, explicitly representing the small {\em diff}erential variables between the input and its perturbation, instead of implicitly representing them as differences between 2 larger values (standard MILP encoding). To further improve the bounds, we employ principal component analysis (PCA) to focus on {\em realistic} in-distribution inputs. Our method verifies over 70\% of incoming images across the standard benchmarks (MNIST, Fashion-MNIST, and CIFAR-10) with $L_1$-perturbations of $1, 4, 2$, respectively, while requiring only 0.5 ms of latency on a single CPU core.