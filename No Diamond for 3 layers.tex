\documentclass[]{article}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newcommand{\ReLU}{\mathrm{ReLU}}



\title{Proof part 1: 3 layers}
\date{}

\begin{document}

\maketitle

\begin{definition}
	A pair of paths $(\pi,\pi')$
	is called {\em compensating} if they start in the same neuron $a$ and ends in the same neuron $z$, and the product of weights over $\pi$ is strictly positive and the product of weights over $\pi'$ is strictly negative.
\end{definition}

Intuitively, compensating paths will partially cancel out each other as they conribute the same weight $w(a)$ to the weight of the same neuron $w(z)$, but with opposite sign. 
It is not simple to take this compensation into account because of ReLUs: the particular compensation will depend upon the weight of intermediate nodes seen along $\pi$ and $\pi'$, 
as when one of this node gets negative input, it will clip it to 0.


\begin{theorem}
	If for all $(a,d)$ there is no compensation paths $(\pi,\pi')$ 
	in the network from $a$ to $d$, then the LP approximation is 
	$100\%$ accurate (or deepoly with the $f(x) \geq 0$ abstraction only, never using the 
	$f(x) \geq x$ abstraction). 
\end{theorem}


\begin{proof}
First we consider the case when there are only three layers: input layer, one hidden layer, and output layer. For convenience, we assume that the output only has one node. Of course, we assume all active functions are ReLU function.

We use $c_i$ to denote nodes in the input layer, use $b_j$ to denote nodes in the hidden layer and use $x$ to denote the output node. We use $W$ to denote the weights and $W_{bc}$ and $W_{xb}$ to denote the components. We use capital letter $B$ to denote bias, although it is not important here.



\begin{definition}
	1. We use $b$ to denote the node before ReLU and $\hat{b}$ to denote the node after ReLU: $\hat{b} = \ReLU(b)$.
	
	2. $\bar{f}$ is the upper bound approximation function of DeepPoly, and $\underline{f}$ is the lower bound approximation function.
\end{definition}



\begin{theorem}
	If there following conditions holds, then the bounds of $x$ obtained by DeepPoly are equal to the actual bounds:
	
	1. All DeepPoly triangle are right-angled triangle: for any ReLU node $b$ that upper bound $u(b)>0$ and lower bound $l(b)<0$, the approximation of lower bound is the zero function $\underline{f}(b)=0$.
	
	2. No Diamond: for any input node $c$, any output node $x$, any two paths $p_1,p_2$ from $c$ to $x$ along the direction of forward propagation. If $p_1$ is $c,b_i,x$, $p_2$ is $c,b_j,x$, then for $W_{b_jc}W_{xb_j}$ and $W_{b_ic}W_{xb_i}$, either one of them is $0$, or they have the same sign: both positive or negative.
\end{theorem}


\subsection*{One} First, we compute the bounds obtained by DeepPoly. Without loss of generality, we only consider the upper bound of $x$.   By the algorithm, the DeepPoly upper bound of $x$ is computed by following steps: \begin{align}
 \rightarrow&\sum_{W_{xb_i}<0}W_{xb_i}\underline{f_i}(b_i)+\sum_{W_{xb_i}>0}W_{xb_i}\bar{f_i}(b_i)+B_x\\
 \rightarrow&\sum_{W_{xb_i}<0}W_{xb_i}\underline{f_i}(\sum_{c_j}W_{b_ic_j}c_j+B_i)+\sum_{W_{xb_i}>0}W_{xb_i}\bar{f_i}(\sum_{c_j}W_{b_ic_j}c_j+B_i)+B_x\\
 =& C+\sum_{W_{xb_i}<0}W_{xb_i}\cdot\underline{k_i}\cdot(\sum_{c_j}W_{b_ic_j}c_j+B_i)+\sum_{W_{xb_i}>0}W_{xb_i}\cdot\bar{k_i}\cdot(\sum_{c_j}W_{b_ic_j}c_j+B_i)\\
 =& C+\sum_{c_j}c_j\cdot\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i\\
 \rightarrow& C+\sum_{\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i<0}\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i\cdot l(c_j)\\
 &+\sum_{\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i>0}\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i\cdot u(c_j)
\end{align} Here, $\bar{k_i}$ is the coefficient of $\bar{f_i}$, which is a linear function, and $\underline{k_i}$ is the coefficient of $\underline{f_i}$, which is also a linear function. $C$ is the sum of all constants that occurs in above computations. $K_i$ is the coefficient $\bar{k_i}$ or $\underline{k_i}$ depending on $i$. Notice that by the definition of DeepPoly, all $K_i$ are non negative. 

Among these formulas, (1) to (4) are abstract formula with formal variables $b_i,c_j$. The result of (5)(6) is the upper bound of DeepPoly for node $x$. 

For for each fixed $c_j$, because we assumed no Diamond, so all (for different $b_i$) $W_{xb_i}W_{b_ic_j}$ must have the same sign, or be $0$, and so is $\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i$. So we assign each node $c_j$ a sign of $+,-,0$,  if at least one of $W_{xb_i}W_{b_ic_j}$ is positive, at least one of $W_{xb_i}W_{b_ic_j}$ is negative, or all $W_{xb_i}W_{b_ic_j}$ are 0. Then we use $s(c_j)$ be $u(c_j)$ or $l(c_j)$ or $0$ if $c_j$ has sign of $+$ or $-$ or $0$.



Then (5)(6) can be simplified as \begin{align}
	=C+\sum_{c_j}s(c_j)\cdot\sum_{b_i}W_{xb_i}W_{b_ic_j}\cdot K_i
\end{align} This is the DeepPoly upper bound of $x$.

\subsection*{Two} Second, we show that when each $c_j$ of $+,-$ sign gets $s(c_j)$ value, the actual value of $x$ is the same as its DeepPoly upper bound, hence the DeepPoly upper bound is the actual upper bound.

Using the chain from (4) to (2), (7) is equal to: \begin{align*}
&\sum_{W_{xb_i}<0}W_{xb_i}\underline{f_i}(\sum_{c_j}W_{b_ic_j}s(c_j)+B_i)\\
+&\sum_{W_{xb_i}>0}W_{xb_i}\bar{f_i}(\sum_{c_j}W_{b_ic_j}s(c_j)+B_i)+B_x
\end{align*} 

We only need to show when all $c_j$ of $+,-$ sign get $s(c_j)$ value, each $\bar{f_i}(\cdots)$ in above formula (or $\underline{f_i}$) is equal to the actual value of $\hat{b}_i$. 

\subsubsection*{positive} We fix a node $b_i$ that $W_{xb_i}>0$. If so, every $W_{b_ic_j}$ has the same sign as $W_{x_bi}W_{b_ic_j}$, and hence has the same sign as $c_j$ or is $0$ (and if $c_j$ has sign $0$, then $W_{b_ic_j}=0$, too). Consider all nonzero $W_{b_ic_j}$: because \begin{align}
	b_i = \sum_{c_j} W_{b_ic_j}c_j+B_i,
\end{align}  when all $c_j$ of $+,-$ sign get $s(c_j)$ value, $b_i$ also gets its upper bound. Because $b_i$ is in the second layer, this upper bound is equal to the DeepPoly upper bound of $b_i$. 


Moreover, by the algorithm of DeepPoly, for any case of $\bar{f_i}$, if $b_i$ get its DeepPoly upper bound, then $\bar{f_i}(b_i)=\ReLU(b_i)$. 

Put all together, when all $c_j$ of sign $+,-$ gets values $s(c_j)$, $b_i=\sum_{c_j}W_{b_ic_j}s(c_j)+B_i$ gets its actual upper bounds and DeepPoly upper bounds, and then  $\bar{f_i}(b_i)=\ReLU(b_i)$, the actual value of $\hat{b}_i$. This is what we want to show.
\end{proof}



\subsubsection*{negative}Now we consider a node $b_i$ that $W_{xb_i}<0$. All are similar. Now, each $W_{b_ic_j}$ has the opposite sign of $c_j$. So when all $c_j$ of $+,-$ sign get $s(c_j)$ value, $b_i$ also gets its lower bound and DeepPoly lower bound. By the assumption, when $b_i$ gets its DeepPoly lower bound, $\underline{f_i}(b_i)=\ReLU(b_i)$. This is what we want to show.




\end{document}
